diff -uNr linux-4.12.9/block/blk-core.c linux-4.12.9-pf/block/blk-core.c
--- linux-4.12.9/block/blk-core.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/blk-core.c	2019-01-02 21:24:25.705429751 +0900
@@ -2550,6 +2550,19 @@
 }
 EXPORT_SYMBOL(blk_fetch_request);
 
+//PARALFETCH                                                                   
+extern bool flashfetch_monitor_rq_complete;                                    
+extern unsigned long long int evaluator_lba;                                   
+                                                                               
+#ifdef CONFIG_ACPI_BATTERY                                                     
+extern int get_ff_battery(void);                                               
+void print_battery_status(struct work_struct *work)                            
+{                                                                              
+               printk("E-BATTERY[%d]\n", get_ff_battery());                    
+}                                                                              
+DECLARE_WORK(print_battery_work, print_battery_status);                        
+#endif
+
 /**
  * blk_update_request - Special helper function for request stacking drivers
  * @req:      the request being processed
@@ -2577,10 +2590,20 @@
 	int total_bytes;
 
 	trace_block_rq_complete(req, error, nr_bytes);
-
+	
 	if (!req->bio)
 		return false;
 
+	//PARALFETCH
+	if(evaluator_lba) {                                                     
+		if(req->bio->bi_bdev && (blk_rq_pos(req) == evaluator_lba)) {   
+			if(rq_data_dir(req) == READ) {                     
+				printk("E %llu %x %lu + %u\n", ktime_to_ns(ktime_get()), (int)req->bio->bi_bdev->bd_dev, blk_rq_pos(req), blk_rq_bytes(req));
+				//schedule_work(&print_battery_work);           
+			}                                                       
+		}                                                               
+	}
+	
 	if (error && !blk_rq_is_passthrough(req) &&
 	    !(req->rq_flags & RQF_QUIET)) {
 		char *error_type;
diff -uNr linux-4.12.9/block/fiops-iosched.c linux-4.12.9-pf/block/fiops-iosched.c
--- linux-4.12.9/block/fiops-iosched.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/block/fiops-iosched.c	2019-01-02 21:24:25.713429797 +0900
@@ -0,0 +1,762 @@
+/*
+ * IOPS based IO scheduler. Based on CFQ.
+ *  Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>
+ *  Shaohua Li <shli@kernel.org>
+ */
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/blkdev.h>
+#include <linux/elevator.h>
+#include <linux/jiffies.h>
+#include <linux/rbtree.h>
+#include <linux/ioprio.h>
+#include <linux/blktrace_api.h>
+#include "blk.h"
+
+#define VIOS_SCALE_SHIFT 10
+#define VIOS_SCALE (1 << VIOS_SCALE_SHIFT)
+
+#define VIOS_READ_SCALE (1)
+#define VIOS_WRITE_SCALE (1)
+#define VIOS_SYNC_SCALE (2)
+#define VIOS_ASYNC_SCALE (5)
+
+#define VIOS_PRIO_SCALE (5)
+
+struct fiops_rb_root {
+	struct rb_root rb;
+	struct rb_node *left;
+	unsigned count;
+
+	u64 min_vios;
+};
+#define FIOPS_RB_ROOT	(struct fiops_rb_root) { .rb = RB_ROOT}
+
+enum wl_prio_t {
+	IDLE_WORKLOAD = 0,
+	BE_WORKLOAD = 1,
+	RT_WORKLOAD = 2,
+	FIOPS_PRIO_NR,
+};
+
+struct fiops_data {
+	struct request_queue *queue;
+
+	struct fiops_rb_root service_tree[FIOPS_PRIO_NR];
+
+	unsigned int busy_queues;
+	unsigned int in_flight[2];
+
+	struct work_struct unplug_work;
+
+	unsigned int read_scale;
+	unsigned int write_scale;
+	unsigned int sync_scale;
+	unsigned int async_scale;
+};
+
+struct fiops_ioc {
+	struct io_cq icq;
+
+	unsigned int flags;
+	struct fiops_data *fiopsd;
+	struct rb_node rb_node;
+	u64 vios; /* key in service_tree */
+	struct fiops_rb_root *service_tree;
+
+	unsigned int in_flight;
+
+	struct rb_root sort_list;
+	struct list_head fifo;
+
+	pid_t pid;
+	unsigned short ioprio;
+	enum wl_prio_t wl_type;
+};
+
+#define ioc_service_tree(ioc) (&((ioc)->fiopsd->service_tree[(ioc)->wl_type]))
+#define RQ_CIC(rq)		icq_to_cic((rq)->elv.icq)
+
+enum ioc_state_flags {
+	FIOPS_IOC_FLAG_on_rr = 0,	/* on round-robin busy list */
+	FIOPS_IOC_FLAG_prio_changed,	/* task priority has changed */
+};
+
+#define FIOPS_IOC_FNS(name)						\
+static inline void fiops_mark_ioc_##name(struct fiops_ioc *ioc)	\
+{									\
+	ioc->flags |= (1 << FIOPS_IOC_FLAG_##name);			\
+}									\
+static inline void fiops_clear_ioc_##name(struct fiops_ioc *ioc)	\
+{									\
+	ioc->flags &= ~(1 << FIOPS_IOC_FLAG_##name);			\
+}									\
+static inline int fiops_ioc_##name(const struct fiops_ioc *ioc)	\
+{									\
+	return ((ioc)->flags & (1 << FIOPS_IOC_FLAG_##name)) != 0;	\
+}
+
+FIOPS_IOC_FNS(on_rr);
+FIOPS_IOC_FNS(prio_changed);
+#undef FIOPS_IOC_FNS
+
+#define fiops_log_ioc(fiopsd, ioc, fmt, args...)	\
+	blk_add_trace_msg((fiopsd)->queue, "ioc%d " fmt, (ioc)->pid, ##args)
+#define fiops_log(fiopsd, fmt, args...)	\
+	blk_add_trace_msg((fiopsd)->queue, "fiops " fmt, ##args)
+
+enum wl_prio_t fiops_wl_type(short prio_class)
+{
+	if (prio_class == IOPRIO_CLASS_RT)
+		return RT_WORKLOAD;
+	if (prio_class == IOPRIO_CLASS_BE)
+		return BE_WORKLOAD;
+	return IDLE_WORKLOAD;
+}
+
+static inline struct fiops_ioc *icq_to_cic(struct io_cq *icq)
+{
+	/* cic->icq is the first member, %NULL will convert to %NULL */
+	return container_of(icq, struct fiops_ioc, icq);
+}
+
+static inline struct fiops_ioc *fiops_cic_lookup(struct fiops_data *fiopsd,
+					       struct io_context *ioc)
+{
+	if (ioc)
+		return icq_to_cic(ioc_lookup_icq(ioc, fiopsd->queue));
+	return NULL;
+}
+
+/*
+ * The below is leftmost cache rbtree addon
+ */
+static struct fiops_ioc *fiops_rb_first(struct fiops_rb_root *root)
+{
+	/* Service tree is empty */
+	if (!root->count)
+		return NULL;
+
+	if (!root->left)
+		root->left = rb_first(&root->rb);
+
+	if (root->left)
+		return rb_entry(root->left, struct fiops_ioc, rb_node);
+
+	return NULL;
+}
+
+static void rb_erase_init(struct rb_node *n, struct rb_root *root)
+{
+	rb_erase(n, root);
+	RB_CLEAR_NODE(n);
+}
+
+static void fiops_rb_erase(struct rb_node *n, struct fiops_rb_root *root)
+{
+	if (root->left == n)
+		root->left = NULL;
+	rb_erase_init(n, &root->rb);
+	--root->count;
+}
+
+static inline u64 max_vios(u64 min_vios, u64 vios)
+{
+	s64 delta = (s64)(vios - min_vios);
+	if (delta > 0)
+		min_vios = vios;
+
+	return min_vios;
+}
+
+static void fiops_update_min_vios(struct fiops_rb_root *service_tree)
+{
+	struct fiops_ioc *ioc;
+
+	ioc = fiops_rb_first(service_tree);
+	if (!ioc)
+		return;
+	service_tree->min_vios = max_vios(service_tree->min_vios, ioc->vios);
+}
+
+/*
+ * The fiopsd->service_trees holds all pending fiops_ioc's that have
+ * requests waiting to be processed. It is sorted in the order that
+ * we will service the queues.
+ */
+static void fiops_service_tree_add(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	struct rb_node **p, *parent;
+	struct fiops_ioc *__ioc;
+	struct fiops_rb_root *service_tree = ioc_service_tree(ioc);
+	u64 vios;
+	int left;
+
+	/* New added IOC */
+	if (RB_EMPTY_NODE(&ioc->rb_node)) {
+		if (ioc->in_flight > 0)
+			vios = ioc->vios;
+		else
+			vios = max_vios(service_tree->min_vios, ioc->vios);
+	} else {
+		vios = ioc->vios;
+		/* ioc->service_tree might not equal to service_tree */
+		fiops_rb_erase(&ioc->rb_node, ioc->service_tree);
+		ioc->service_tree = NULL;
+	}
+
+	fiops_log_ioc(fiopsd, ioc, "service tree add, vios %lld", vios);
+
+	left = 1;
+	parent = NULL;
+	ioc->service_tree = service_tree;
+	p = &service_tree->rb.rb_node;
+	while (*p) {
+		struct rb_node **n;
+
+		parent = *p;
+		__ioc = rb_entry(parent, struct fiops_ioc, rb_node);
+
+		/*
+		 * sort by key, that represents service time.
+		 */
+		if (vios <  __ioc->vios)
+			n = &(*p)->rb_left;
+		else {
+			n = &(*p)->rb_right;
+			left = 0;
+		}
+
+		p = n;
+	}
+
+	if (left)
+		service_tree->left = &ioc->rb_node;
+
+	ioc->vios = vios;
+	rb_link_node(&ioc->rb_node, parent, p);
+	rb_insert_color(&ioc->rb_node, &service_tree->rb);
+	service_tree->count++;
+
+	fiops_update_min_vios(service_tree);
+}
+
+/*
+ * Update ioc's position in the service tree.
+ */
+static void fiops_resort_rr_list(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	/*
+	 * Resorting requires the ioc to be on the RR list already.
+	 */
+	if (fiops_ioc_on_rr(ioc))
+		fiops_service_tree_add(fiopsd, ioc);
+}
+
+/*
+ * add to busy list of queues for service, trying to be fair in ordering
+ * the pending list according to last request service
+ */
+static void fiops_add_ioc_rr(struct fiops_data *fiopsd, struct fiops_ioc *ioc)
+{
+	BUG_ON(fiops_ioc_on_rr(ioc));
+	fiops_mark_ioc_on_rr(ioc);
+
+	fiopsd->busy_queues++;
+
+	fiops_resort_rr_list(fiopsd, ioc);
+}
+
+/*
+ * Called when the ioc no longer has requests pending, remove it from
+ * the service tree.
+ */
+static void fiops_del_ioc_rr(struct fiops_data *fiopsd, struct fiops_ioc *ioc)
+{
+	BUG_ON(!fiops_ioc_on_rr(ioc));
+	fiops_clear_ioc_on_rr(ioc);
+
+	if (!RB_EMPTY_NODE(&ioc->rb_node)) {
+		fiops_rb_erase(&ioc->rb_node, ioc->service_tree);
+		ioc->service_tree = NULL;
+	}
+
+	BUG_ON(!fiopsd->busy_queues);
+	fiopsd->busy_queues--;
+}
+
+/*
+ * rb tree support functions
+ */
+static void fiops_del_rq_rb(struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	elv_rb_del(&ioc->sort_list, rq);
+}
+
+static void fiops_add_rq_rb(struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+	struct fiops_data *fiopsd = ioc->fiopsd;
+
+	elv_rb_add(&ioc->sort_list, rq);
+
+	if (!fiops_ioc_on_rr(ioc))
+		fiops_add_ioc_rr(fiopsd, ioc);
+}
+
+static void fiops_reposition_rq_rb(struct fiops_ioc *ioc, struct request *rq)
+{
+	elv_rb_del(&ioc->sort_list, rq);
+	fiops_add_rq_rb(rq);
+}
+
+static void fiops_remove_request(struct request *rq)
+{
+	list_del_init(&rq->queuelist);
+	fiops_del_rq_rb(rq);
+}
+
+static u64 fiops_scaled_vios(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc, struct request *rq)
+{
+	int vios = VIOS_SCALE;
+
+	if (rq_data_dir(rq) == WRITE)
+		vios = vios * fiopsd->write_scale / fiopsd->read_scale;
+
+	if (!rq_is_sync(rq))
+		vios = vios * fiopsd->async_scale / fiopsd->sync_scale;
+
+	vios +=  vios * (ioc->ioprio - IOPRIO_NORM) / VIOS_PRIO_SCALE;
+
+	return vios;
+}
+
+/* return vios dispatched */
+static u64 fiops_dispatch_request(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	struct request *rq;
+	struct request_queue *q = fiopsd->queue;
+
+	rq = rq_entry_fifo(ioc->fifo.next);
+
+	fiops_remove_request(rq);
+	elv_dispatch_add_tail(q, rq);
+
+	fiopsd->in_flight[rq_is_sync(rq)]++;
+	ioc->in_flight++;
+
+	return fiops_scaled_vios(fiopsd, ioc, rq);
+}
+
+static int fiops_forced_dispatch(struct fiops_data *fiopsd)
+{
+	struct fiops_ioc *ioc;
+	int dispatched = 0;
+	int i;
+
+	for (i = RT_WORKLOAD; i >= IDLE_WORKLOAD; i--) {
+		while (!RB_EMPTY_ROOT(&fiopsd->service_tree[i].rb)) {
+			ioc = fiops_rb_first(&fiopsd->service_tree[i]);
+
+			while (!list_empty(&ioc->fifo)) {
+				fiops_dispatch_request(fiopsd, ioc);
+				dispatched++;
+			}
+			if (fiops_ioc_on_rr(ioc))
+				fiops_del_ioc_rr(fiopsd, ioc);
+		}
+	}
+	return dispatched;
+}
+
+static struct fiops_ioc *fiops_select_ioc(struct fiops_data *fiopsd)
+{
+	struct fiops_ioc *ioc;
+	struct fiops_rb_root *service_tree = NULL;
+	int i;
+	struct request *rq;
+
+	for (i = RT_WORKLOAD; i >= IDLE_WORKLOAD; i--) {
+		if (!RB_EMPTY_ROOT(&fiopsd->service_tree[i].rb)) {
+			service_tree = &fiopsd->service_tree[i];
+			break;
+		}
+	}
+
+	if (!service_tree)
+		return NULL;
+
+	ioc = fiops_rb_first(service_tree);
+
+	rq = rq_entry_fifo(ioc->fifo.next);
+	/*
+	 * we are the only async task and sync requests are in flight, delay a
+	 * moment. If there are other tasks coming, sync tasks have no chance
+	 * to be starved, don't delay
+	 */
+	if (!rq_is_sync(rq) && fiopsd->in_flight[1] != 0 &&
+			service_tree->count == 1) {
+		fiops_log_ioc(fiopsd, ioc,
+				"postpone async, in_flight async %d sync %d",
+				fiopsd->in_flight[0], fiopsd->in_flight[1]);
+		return NULL;
+	}
+
+	return ioc;
+}
+
+static void fiops_charge_vios(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc, u64 vios)
+{
+	struct fiops_rb_root *service_tree = ioc->service_tree;
+	ioc->vios += vios;
+
+	fiops_log_ioc(fiopsd, ioc, "charge vios %lld, new vios %lld", vios, ioc->vios);
+
+	if (RB_EMPTY_ROOT(&ioc->sort_list))
+		fiops_del_ioc_rr(fiopsd, ioc);
+	else
+		fiops_resort_rr_list(fiopsd, ioc);
+
+	fiops_update_min_vios(service_tree);
+}
+
+static int fiops_dispatch_requests(struct request_queue *q, int force)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *ioc;
+	u64 vios;
+
+	if (unlikely(force))
+		return fiops_forced_dispatch(fiopsd);
+
+	ioc = fiops_select_ioc(fiopsd);
+	if (!ioc)
+		return 0;
+
+	vios = fiops_dispatch_request(fiopsd, ioc);
+
+	fiops_charge_vios(fiopsd, ioc, vios);
+	return 1;
+}
+
+static void fiops_init_prio_data(struct fiops_ioc *cic)
+{
+	struct task_struct *tsk = current;
+	struct io_context *ioc = cic->icq.ioc;
+	int ioprio_class;
+
+	if (!fiops_ioc_prio_changed(cic))
+		return;
+
+	ioprio_class = IOPRIO_PRIO_CLASS(ioc->ioprio);
+	switch (ioprio_class) {
+	default:
+		printk(KERN_ERR "fiops: bad prio %x\n", ioprio_class);
+	case IOPRIO_CLASS_NONE:
+		/*
+		 * no prio set, inherit CPU scheduling settings
+		 */
+		cic->ioprio = task_nice_ioprio(tsk);
+		cic->wl_type = fiops_wl_type(task_nice_ioclass(tsk));
+		break;
+	case IOPRIO_CLASS_RT:
+		cic->ioprio = IOPRIO_PRIO_DATA(ioc->ioprio);
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_RT);
+		break;
+	case IOPRIO_CLASS_BE:
+		cic->ioprio = IOPRIO_PRIO_DATA(ioc->ioprio);
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_BE);
+		break;
+	case IOPRIO_CLASS_IDLE:
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_IDLE);
+		cic->ioprio = 7;
+		break;
+	}
+
+	fiops_clear_ioc_prio_changed(cic);
+}
+
+static void fiops_insert_request(struct request_queue *q, struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	fiops_init_prio_data(ioc);
+
+	list_add_tail(&rq->queuelist, &ioc->fifo);
+
+	fiops_add_rq_rb(rq);
+}
+
+/*
+ * scheduler run of queue, if there are requests pending and no one in the
+ * driver that will restart queueing
+ */
+static inline void fiops_schedule_dispatch(struct fiops_data *fiopsd)
+{
+	if (fiopsd->busy_queues)
+		kblockd_schedule_work(&fiopsd->unplug_work);
+}
+
+static void fiops_completed_request(struct request_queue *q, struct request *rq)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	fiopsd->in_flight[rq_is_sync(rq)]--;
+	ioc->in_flight--;
+
+	fiops_log_ioc(fiopsd, ioc, "in_flight %d, busy queues %d",
+		ioc->in_flight, fiopsd->busy_queues);
+
+	if (fiopsd->in_flight[0] + fiopsd->in_flight[1] == 0)
+		fiops_schedule_dispatch(fiopsd);
+}
+
+static struct request *
+fiops_find_rq_fmerge(struct fiops_data *fiopsd, struct bio *bio)
+{
+	struct task_struct *tsk = current;
+	struct fiops_ioc *cic;
+
+	cic = fiops_cic_lookup(fiopsd, tsk->io_context);
+
+	if (cic) {
+		return elv_rb_find(&cic->sort_list, bio_end_sector(bio));
+	}
+
+	return NULL;
+}
+
+static enum elv_merge fiops_merge(struct request_queue *q, struct request **req,
+		     struct bio *bio)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct request *__rq;
+
+	__rq = fiops_find_rq_fmerge(fiopsd, bio);
+	if (__rq && elv_bio_merge_ok(__rq, bio)) {
+		*req = __rq;
+		return ELEVATOR_FRONT_MERGE;
+	}
+
+	return ELEVATOR_NO_MERGE;
+}
+
+static void fiops_merged_request(struct request_queue *q, struct request *req,
+			       enum elv_merge type)
+{
+	if (type == ELEVATOR_FRONT_MERGE) {
+		struct fiops_ioc *ioc = RQ_CIC(req);
+
+		fiops_reposition_rq_rb(ioc, req);
+	}
+}
+
+static void
+fiops_merged_requests(struct request_queue *q, struct request *rq,
+		    struct request *next)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+
+	fiops_remove_request(next);
+
+	ioc = RQ_CIC(next);
+	/*
+	 * all requests of this task are merged to other tasks, delete it
+	 * from the service tree.
+	 */
+	if (fiops_ioc_on_rr(ioc) && RB_EMPTY_ROOT(&ioc->sort_list))
+		fiops_del_ioc_rr(fiopsd, ioc);
+}
+
+static int fiops_allow_bio_merge(struct request_queue *q, struct request *rq,
+			   struct bio *bio)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *cic;
+
+	/*
+	 * Lookup the ioc that this bio will be queued with. Allow
+	 * merge only if rq is queued there.
+	 */
+	cic = fiops_cic_lookup(fiopsd, current->io_context);
+
+	return cic == RQ_CIC(rq);
+}
+
+static void fiops_exit_queue(struct elevator_queue *e)
+{
+	struct fiops_data *fiopsd = e->elevator_data;
+
+	cancel_work_sync(&fiopsd->unplug_work);
+
+	kfree(fiopsd);
+}
+
+static void fiops_kick_queue(struct work_struct *work)
+{
+	struct fiops_data *fiopsd =
+		container_of(work, struct fiops_data, unplug_work);
+	struct request_queue *q = fiopsd->queue;
+
+	spin_lock_irq(q->queue_lock);
+	__blk_run_queue(q);
+	spin_unlock_irq(q->queue_lock);
+}
+
+static int fiops_init_queue(struct request_queue *q, struct elevator_type *e)
+{
+	struct fiops_data *fiopsd;
+	int i;
+	struct elevator_queue *eq;
+
+	eq = elevator_alloc(q, e);
+	if (!eq)
+		return -ENOMEM;
+
+	fiopsd = kzalloc_node(sizeof(*fiopsd), GFP_KERNEL, q->node);
+	if (!fiopsd) {
+		kobject_put(&eq->kobj);
+		return -ENOMEM;
+	}
+	eq->elevator_data = fiopsd;
+
+	fiopsd->queue = q;
+	spin_lock_irq(q->queue_lock);
+	q->elevator = eq;
+	spin_unlock_irq(q->queue_lock);
+
+	for (i = IDLE_WORKLOAD; i <= RT_WORKLOAD; i++)
+		fiopsd->service_tree[i] = FIOPS_RB_ROOT;
+
+	INIT_WORK(&fiopsd->unplug_work, fiops_kick_queue);
+
+	fiopsd->read_scale = VIOS_READ_SCALE;
+	fiopsd->write_scale = VIOS_WRITE_SCALE;
+	fiopsd->sync_scale = VIOS_SYNC_SCALE;
+	fiopsd->async_scale = VIOS_ASYNC_SCALE;
+
+	return 0;
+}
+
+static void fiops_init_icq(struct io_cq *icq)
+{
+	struct fiops_data *fiopsd = icq->q->elevator->elevator_data;
+	struct fiops_ioc *ioc = icq_to_cic(icq);
+
+	RB_CLEAR_NODE(&ioc->rb_node);
+	INIT_LIST_HEAD(&ioc->fifo);
+	ioc->sort_list = RB_ROOT;
+
+	ioc->fiopsd = fiopsd;
+
+	ioc->pid = current->pid;
+	fiops_mark_ioc_prio_changed(ioc);
+}
+
+/*
+ * sysfs parts below -->
+ */
+static ssize_t
+fiops_var_show(unsigned int var, char *page)
+{
+	return sprintf(page, "%d\n", var);
+}
+
+static ssize_t
+fiops_var_store(unsigned int *var, const char *page, size_t count)
+{
+	char *p = (char *) page;
+
+	*var = simple_strtoul(p, &p, 10);
+	return count;
+}
+
+#define SHOW_FUNCTION(__FUNC, __VAR)					\
+static ssize_t __FUNC(struct elevator_queue *e, char *page)		\
+{									\
+	struct fiops_data *fiopsd = e->elevator_data;			\
+	return fiops_var_show(__VAR, (page));				\
+}
+SHOW_FUNCTION(fiops_read_scale_show, fiopsd->read_scale);
+SHOW_FUNCTION(fiops_write_scale_show, fiopsd->write_scale);
+SHOW_FUNCTION(fiops_sync_scale_show, fiopsd->sync_scale);
+SHOW_FUNCTION(fiops_async_scale_show, fiopsd->async_scale);
+#undef SHOW_FUNCTION
+
+#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)				\
+static ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)	\
+{									\
+	struct fiops_data *fiopsd = e->elevator_data;			\
+	unsigned int __data;						\
+	int ret = fiops_var_store(&__data, (page), count);		\
+	if (__data < (MIN))						\
+		__data = (MIN);						\
+	else if (__data > (MAX))					\
+		__data = (MAX);						\
+	*(__PTR) = __data;						\
+	return ret;							\
+}
+STORE_FUNCTION(fiops_read_scale_store, &fiopsd->read_scale, 1, 100);
+STORE_FUNCTION(fiops_write_scale_store, &fiopsd->write_scale, 1, 100);
+STORE_FUNCTION(fiops_sync_scale_store, &fiopsd->sync_scale, 1, 100);
+STORE_FUNCTION(fiops_async_scale_store, &fiopsd->async_scale, 1, 100);
+#undef STORE_FUNCTION
+
+#define FIOPS_ATTR(name) \
+	__ATTR(name, S_IRUGO|S_IWUSR, fiops_##name##_show, fiops_##name##_store)
+
+static struct elv_fs_entry fiops_attrs[] = {
+	FIOPS_ATTR(read_scale),
+	FIOPS_ATTR(write_scale),
+	FIOPS_ATTR(sync_scale),
+	FIOPS_ATTR(async_scale),
+	__ATTR_NULL
+};
+
+static struct elevator_type iosched_fiops = {
+	.ops.sq = {
+		.elevator_merge_fn =		fiops_merge,
+		.elevator_merged_fn =		fiops_merged_request,
+		.elevator_merge_req_fn =	fiops_merged_requests,
+		.elevator_allow_bio_merge_fn =	fiops_allow_bio_merge,
+		.elevator_dispatch_fn =		fiops_dispatch_requests,
+		.elevator_add_req_fn =		fiops_insert_request,
+		.elevator_completed_req_fn =	fiops_completed_request,
+		.elevator_former_req_fn =	elv_rb_former_request,
+		.elevator_latter_req_fn =	elv_rb_latter_request,
+		.elevator_init_icq_fn =		fiops_init_icq,
+		.elevator_init_fn =		fiops_init_queue,
+		.elevator_exit_fn =		fiops_exit_queue,
+	},
+	.icq_size	=	sizeof(struct fiops_ioc),
+	.icq_align	=	__alignof__(struct fiops_ioc),
+	.elevator_attrs =	fiops_attrs,
+	.elevator_name =	"fiops",
+	.elevator_owner =	THIS_MODULE,
+};
+
+static int __init fiops_init(void)
+{
+	return elv_register(&iosched_fiops);
+}
+
+static void __exit fiops_exit(void)
+{
+	elv_unregister(&iosched_fiops);
+}
+
+module_init(fiops_init);
+module_exit(fiops_exit);
+
+MODULE_AUTHOR("Jens Axboe, Shaohua Li <shli@kernel.org>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("IOPS based IO scheduler");
diff -uNr linux-4.12.9/block/Kconfig.iosched linux-4.12.9-pf/block/Kconfig.iosched
--- linux-4.12.9/block/Kconfig.iosched	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/Kconfig.iosched	2019-01-02 21:24:25.706429757 +0900
@@ -32,6 +32,14 @@
 
 	  This is the default I/O scheduler.
 
+config IOSCHED_FIOPS
+	tristate "IOPS based I/O scheduler"
+	default y
+	---help---
+	  This is an IOPS based I/O scheduler. It will try to distribute
+          IOPS equally among all processes in the system. It's mainly for
+          Flash based storage.
+
 config CFQ_GROUP_IOSCHED
 	bool "CFQ Group Scheduling support"
 	depends on IOSCHED_CFQ && BLK_CGROUP
diff -uNr linux-4.12.9/block/Makefile linux-4.12.9-pf/block/Makefile
--- linux-4.12.9/block/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/Makefile	2019-01-02 21:24:25.706429757 +0900
@@ -19,6 +19,7 @@
 obj-$(CONFIG_IOSCHED_NOOP)	+= noop-iosched.o
 obj-$(CONFIG_IOSCHED_DEADLINE)	+= deadline-iosched.o
 obj-$(CONFIG_IOSCHED_CFQ)	+= cfq-iosched.o
+obj-$(CONFIG_IOSCHED_FIOPS)     += fiops-iosched.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
diff -uNr linux-4.12.9/block/noop-iosched.c linux-4.12.9-pf/block/noop-iosched.c
--- linux-4.12.9/block/noop-iosched.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/noop-iosched.c	2019-01-02 21:24:25.707429762 +0900
@@ -18,6 +18,10 @@
 	list_del_init(&next->queuelist);
 }
 
+#ifdef CONFIG_PARALFETCH
+extern int prefetch_state;
+#endif
+
 static int noop_dispatch(struct request_queue *q, int force)
 {
 	struct noop_data *nd = q->elevator->elevator_data;
@@ -26,7 +30,16 @@
 	rq = list_first_entry_or_null(&nd->queue, struct request, queuelist);
 	if (rq) {
 		list_del_init(&rq->queuelist);
+
+#ifdef CONFIG_PARALFETCH
+		if(unlikely(prefetch_state == 0))
+			elv_dispatch_sort(q, rq);
+		else
+			elv_dispatch_add_tail(q, rq);
+#else
 		elv_dispatch_sort(q, rq);
+#endif
+
 		return 1;
 	}
 	return 0;
@@ -80,6 +93,7 @@
 	spin_lock_irq(q->queue_lock);
 	q->elevator = eq;
 	spin_unlock_irq(q->queue_lock);
+
 	return 0;
 }
 
diff -uNr linux-4.12.9/.cocciconfig linux-4.12.9-pf/.cocciconfig
--- linux-4.12.9/.cocciconfig	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/.cocciconfig	1970-01-01 09:00:00.000000000 +0900
@@ -1,3 +0,0 @@
-[spatch]
-	options = --timeout 200
-	options = --use-gitgrep
diff -uNr linux-4.12.9/drivers/acpi/battery.c linux-4.12.9-pf/drivers/acpi/battery.c
--- linux-4.12.9/drivers/acpi/battery.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/drivers/acpi/battery.c	2019-01-02 21:24:46.551547910 +0900
@@ -1208,6 +1208,9 @@
 	return ret;
 }
 
+//PARALFETCH
+struct acpi_battery *ff_battery[2]={NULL, NULL};
+
 static int acpi_battery_add(struct acpi_device *device)
 {
 	int result = 0;
@@ -1235,6 +1238,10 @@
 	if (result)
 		goto fail;
 
+	//PARALFETCH
+        if(ff_battery[0] == NULL) ff_battery[0] = battery;
+        else if(ff_battery[1] == NULL) ff_battery[1] = battery;
+
 #ifdef CONFIG_ACPI_PROCFS_POWER
 	result = acpi_battery_add_fs(device);
 #endif
@@ -1264,6 +1271,31 @@
 	return result;
 }
 
+//PARALFETCH
+int get_ff_battery(void)                                                        
+{                                                                               
+	int tot_bat = 0;                                                        
+	int result = 0;                                                         
+
+	if(ff_battery[0] != NULL) {                                             
+		result = acpi_battery_update(ff_battery[0], false);             
+		if (result)                                                     
+			goto fail;                                              
+		tot_bat += ff_battery[0]->capacity_now;                         
+	}                                                                       
+
+	if(ff_battery[1] != NULL) {                                             
+		result = acpi_battery_update(ff_battery[1], false);             
+		if (result)                                                     
+			goto fail;                                              
+		tot_bat += ff_battery[1]->capacity_now;                         
+	}                                                                       
+
+	return tot_bat;                                                         
+fail:                                                                           
+	return -ENODEV;                                                         
+}
+
 static int acpi_battery_remove(struct acpi_device *device)
 {
 	struct acpi_battery *battery = NULL;
diff -uNr linux-4.12.9/drivers/ata/libata-core.c linux-4.12.9-pf/drivers/ata/libata-core.c
--- linux-4.12.9/drivers/ata/libata-core.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/drivers/ata/libata-core.c	2019-01-02 21:24:52.766583139 +0900
@@ -176,6 +176,8 @@
 MODULE_LICENSE("GPL");
 MODULE_VERSION(DRV_VERSION);
 
+//PARALFETCH
+extern u64 dev_req_cnt;
 
 static bool ata_sstatus_online(u32 sstatus)
 {
@@ -5086,6 +5088,9 @@
 	qc->flags &= ~ATA_QCFLAG_ACTIVE;
 	ap->qc_active &= ~(1 << qc->tag);
 
+	//PARALFETCH
+	dev_req_cnt--;
+
 	/* call completion callback */
 	qc->complete_fn(qc);
 }
@@ -5332,6 +5337,15 @@
 
 	ap->ops->qc_prep(qc);
 	trace_ata_qc_issue(qc);
+
+	//PARALFETCH
+	dev_req_cnt++;
+	/*
+	if(dev_req_cnt >= 3) {
+		printk("*PF* dev_req_cnt: %llu\n", dev_req_cnt);
+	}
+	*/
+	
 	qc->err_mask |= ap->ops->qc_issue(qc);
 	if (unlikely(qc->err_mask))
 		goto err;
diff -uNr linux-4.12.9/fs/binfmt_elf.c linux-4.12.9-pf/fs/binfmt_elf.c
--- linux-4.12.9/fs/binfmt_elf.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/binfmt_elf.c	2019-01-02 21:24:54.305591862 +0900
@@ -44,6 +44,9 @@
 #include <asm/param.h>
 #include <asm/page.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+
 #ifndef user_long_t
 #define user_long_t long
 #endif
@@ -678,6 +681,19 @@
 #endif
 }
 
+//PARALFETCH
+#ifdef CONFIG_ACPI_BATTERY
+extern int get_ff_battery(void);
+#endif
+extern unsigned long long int evaluator_lba;
+extern u64 flashfetch_launch_ts;
+
+//PARALFETCH
+extern pid_t launch_pid;
+extern unsigned long long int flashfetch_prefetch_cnt;
+extern unsigned long long int flashfetch_prefetch_hit;
+extern unsigned long long int flashfetch_readahead_hit;
+
 static int load_elf_binary(struct linux_binprm *bprm)
 {
 	struct file *interpreter = NULL; /* to shut gcc up */
@@ -701,6 +717,10 @@
 	} *loc;
 	struct arch_elf_state arch_state = INIT_ARCH_ELF_STATE;
 
+	//PARALFETCH
+        char appname[NAME_LEN+1];
+        unsigned int launch_type;
+
 	loc = kmalloc(sizeof(*loc), GFP_KERNEL);
 	if (!loc) {
 		retval = -ENOMEM;
@@ -726,6 +746,64 @@
 	if (!elf_phdata)
 		goto out;
 
+	//PARALFETCH
+#if (EI_PAD < EI_NIDENT)                                                        
+        launch_type = loc->elf_ex.e_ident[EI_PAD];                              
+                                                                                
+        /*                                                                      
+        if(evaluator_lba != 0) {                                                
+                printk("TIME: %Lu, EXEC[%s],PF[%u]\n", ktime_to_ns(ktime_get()), bprm->filename, launch_type);
+                //printk("TIME:%Lu, EXEC[%s],PF[%u], BATTERY[%i]\n", ktime_to_ns(ktime_get()), bprm->filename, launch_type, get_ff_battery());
+                                                                                
+        } */ 
+
+	switch(launch_type)
+        {
+                case NO_PREFETCH:
+                        break;
+                case ASYNC_PREFETCH:
+                case SYNC_PREFETCH:
+			//PARALFETCH
+			flashfetch_prefetch_cnt = 0;
+			flashfetch_prefetch_hit = 0;
+			flashfetch_readahead_hit = 0;
+                case GEN_PREFETCH:
+			//PARALFETCH
+			launch_pid = current->pid;
+
+                        //printk("inode:%lu, argc:%d, p:%lu\n", bprm->file->f_dentry->d_inode->i_ino, bprm->argc, bprm->p);
+			flashfetch_launch_ts = ktime_to_ns(ktime_get());        
+                        if(bprm->filename) {                                    
+                                int i, len, offs;                               
+                                len = strlen(bprm->filename);                   
+                                offs = 0;                                       
+                                for(i=len-1; i>=0; i--) {                       
+                                        if(bprm->filename[i] == '/') {          
+                                                offs = i+1;                     
+                                                break;                          
+                                        }                                       
+                                }                                               
+                                strncpy(appname, &(bprm->filename[offs]), NAME_LEN);
+                                appname[len - offs] = '\0';                     
+                                                                                
+                                printk("EXEC[%s],PF[%u]\n", appname, launch_type);
+                        }                                                       
+                        if(launch_type == ASYNC_PREFETCH) {                     
+                                flashfetch_async(appname);                      
+                        } else if(launch_type == SYNC_PREFETCH) {               
+                                flashfetch_sync(appname);                       
+                        } else if(launch_type == GEN_PREFETCH) {                
+                                flashfetch_genpf(appname, bprm->filename);      
+                        }                                                       
+                        break;                                                  
+                case SEQ_REFINE1:                                               
+                case SEQ_REFINE2:                                               
+                        break;                                                  
+                default:                                                        
+                        break;                                                  
+        }                                                                       
+#endif
+
 	elf_ppnt = elf_phdata;
 	elf_bss = 0;
 	elf_brk = 0;
diff -uNr linux-4.12.9/fs/binfmt_script.c linux-4.12.9-pf/fs/binfmt_script.c
--- linux-4.12.9/fs/binfmt_script.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/binfmt_script.c	2019-01-02 21:24:54.310591891 +0900
@@ -21,6 +21,9 @@
 	struct file *file;
 	char interp[BINPRM_BUF_SIZE];
 	int retval;
+	//PARALFETCH
+	char appname[NAME_MAX+1];
+        unsigned int launch_type = 0;
 
 	if ((bprm->buf[0] != '#') || (bprm->buf[1] != '!'))
 		return -ENOEXEC;
@@ -54,7 +57,17 @@
 		else
 			break;
 	}
-	for (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\t'); cp++);
+	//PARALFETCH
+	if(bprm->buf[2] >= '0' && bprm->buf[2] <= '9' && bprm->buf[3] == '/') {
+		launch_type = (unsigned int)(bprm->buf[2] - '0');
+		printk("Paralfetch: script launch_type: %u, %s\n", launch_type,
+				bprm->filename ? bprm->filename : "<NULL>");
+		for (cp = bprm->buf+4; (*cp == ' ') || (*cp == '\t'); cp++);
+	} else { 
+		//ORG
+		for (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\t'); cp++);
+	}
+
 	if (*cp == '\0') 
 		return -ENOEXEC; /* No interpreter name found */
 	i_name = cp;
diff -uNr linux-4.12.9/fs/block_dev.c linux-4.12.9-pf/fs/block_dev.c
--- linux-4.12.9/fs/block_dev.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/block_dev.c	2019-01-02 21:24:54.299591828 +0900
@@ -1908,6 +1908,10 @@
 
 	size -= pos;
 	iov_iter_truncate(to, size);
+
+	//PARALFETCH2
+	printk("blkdev_read_iter\n");
+
 	return generic_file_read_iter(iocb, to);
 }
 EXPORT_SYMBOL_GPL(blkdev_read_iter);
diff -uNr linux-4.12.9/fs/buffer.c linux-4.12.9-pf/fs/buffer.c
--- linux-4.12.9/fs/buffer.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/buffer.c	2019-01-02 21:24:54.297591817 +0900
@@ -47,6 +47,10 @@
 #include <linux/pagevec.h>
 #include <trace/events/block.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
 			 struct writeback_control *wbc);
@@ -3090,11 +3094,19 @@
 	}
 }
 
+//PARALFETCH
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
 static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
 			 struct writeback_control *wbc)
 {
 	struct bio *bio;
 
+	//PARALFETCH
+	int ret = 0;
+        struct disk_log_entry *ff_log;
+
 	BUG_ON(!buffer_locked(bh));
 	BUG_ON(!buffer_mapped(bh));
 	BUG_ON(!bh->b_end_io);
@@ -3136,6 +3148,24 @@
 		op_flags |= REQ_PRIO;
 	bio_set_op_attrs(bio, op, op_flags);
 
+	//PARALFETCH
+        if(flashfetch_monitor_blkio && (op == REQ_OP_READ)/*((rw & RW_MASK) == READ)*/ && flashfetch_trace)
+        {
+                ret = get_log_entry(flashfetch_trace);
+                if(ret >= 0) {
+                        ff_log = &(flashfetch_trace->log[ret]);
+                        ff_log->ts = ktime_to_ns(ktime_get());
+                        ff_log->dev = bh->b_bdev->bd_dev;
+                        ff_log->ino = 0;
+                        ff_log->blk_num = (u64)bh->b_blocknr;
+                        ff_log->blk_len = (u32)bh->b_size;
+                        printk("[%d]:%lld dev:0x%x offs:%llu size:%u\n", ret, ff_log->ts, ff_log->dev, ff_log->blk_num, ff_log->blk_len);
+                        if(ff_log->dev == 0) printk("BBB[%d]:%lld dev:0x%x offs:%llu size:%u\n", ret, ff_log->ts, ff_log->dev, ff_log->blk_num, ff_log->blk_len);
+                }
+                //printk("log_ent:%d dev:0x%x offs:%lu size:%lu\n", get_log_entry(flashfetch_trace), bh->b_bdev->bd_dev, bh->b_blocknr, bh->b_size);
+                //ret = 0;
+        }
+
 	submit_bio(bio);
 	return 0;
 }
diff -uNr linux-4.12.9/fs/drop_caches.c linux-4.12.9-pf/fs/drop_caches.c
--- linux-4.12.9/fs/drop_caches.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/drop_caches.c	2019-01-02 21:24:54.304591857 +0900
@@ -13,7 +13,8 @@
 /* A global variable is a bit ugly, but it keeps the code simple */
 int sysctl_drop_caches;
 
-static void drop_pagecache_sb(struct super_block *sb, void *unused)
+//PARALFETCH
+/*static*/ void drop_pagecache_sb(struct super_block *sb, void *unused)
 {
 	struct inode *inode, *toput_inode = NULL;
 
diff -uNr linux-4.12.9/fs/exec.c linux-4.12.9-pf/fs/exec.c
--- linux-4.12.9/fs/exec.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/exec.c	2019-01-02 21:24:54.298591823 +0900
@@ -1676,6 +1676,9 @@
 	return ret;
 }
 
+//PARALFETCH
+extern unsigned long long int evaluator_lba;
+
 /*
  * sys_execve() executes a new program.
  */
@@ -1709,6 +1712,9 @@
 	 * further execve() calls fail. */
 	current->flags &= ~PF_NPROC_EXCEEDED;
 
+	//PARALFETCH
+        if(evaluator_lba) printk("X %llu %s\n", ktime_to_ns(ktime_get()), filename->name);
+
 	retval = unshare_files(&displaced);
 	if (retval)
 		goto out_ret;
diff -uNr linux-4.12.9/fs/ext4/extents.c linux-4.12.9-pf/fs/ext4/extents.c
--- linux-4.12.9/fs/ext4/extents.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/ext4/extents.c	2019-01-02 21:24:55.628599361 +0900
@@ -46,6 +46,11 @@
 
 #include <trace/events/ext4.h>
 
+//PARALFETCH
+#include <linux/flashfetch_scheduler.h>
+int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct dependency_entry **dep_ptr);
+void ext4_log_block_path(struct inode *inode, struct ext4_ext_path *path, struct dependency_entry **dep_ptr);
+
 /*
  * used by extent splitting.
  */
@@ -5947,3 +5952,260 @@
 	}
 	return replaced_count;
 }
+
+int ext4_ext_walk_range(struct inode *inode,
+                                    ext4_lblk_t block, ext4_lblk_t num,
+                                    struct dependency_entry **dep_ptr)
+{
+        struct ext4_ext_path *path = NULL;
+        struct ext4_ext_path *path_tmp = NULL; //new
+        struct ext4_extent *ex;
+        struct extent_status es;
+        ext4_lblk_t next, next_del, start = 0, end = 0;
+        ext4_lblk_t last = block + num;
+        int exists, depth = 0, err = 0;
+        unsigned int flags = 0;
+        //unsigned char blksize_bits = inode->i_sb->s_blocksize_bits;
+        struct ext4_iloc iloc; //new
+        int k, l; //new
+        struct dependency_entry **last_dep; //new
+        struct dependency_entry *new_dep; //new
+
+	//PARALFETCH
+	struct dependency_entry **head_dep = NULL;
+	struct dependency_entry *tmp_dep = NULL;
+
+        BUG_ON(inode == NULL);  //new
+        last_dep = dep_ptr; //new
+        k = ext4_get_inode_loc(inode, &iloc); //new
+
+        // new-start
+        if(!k) {
+                new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                if(!new_dep) {
+                        brelse(iloc.bh);
+                        return -ENOMEM;
+                }
+                new_dep->type = TYPE_META;
+                new_dep->pblk_num = iloc.bh->b_blocknr;
+		new_dep->blk_num = iloc.bh->b_blocknr;
+                new_dep->blk_len = EXT4_BLOCK_SIZE(inode->i_sb);
+                new_dep->next = NULL;
+                *last_dep = new_dep;
+                last_dep = &(new_dep->next);
+                brelse(iloc.bh);
+		//PARALFETCH
+		head_dep = &new_dep;
+        } else return -ENOENT;
+        // new-end
+
+        while (block < last && block != EXT_MAX_BLOCKS) {
+                num = last - block;
+                /* find extent for this block */
+                down_read(&EXT4_I(inode)->i_data_sem);
+
+                path = ext4_find_extent(inode, block, &path, 0);
+                if (IS_ERR(path)) {
+                        up_read(&EXT4_I(inode)->i_data_sem);
+                        err = PTR_ERR(path);
+                        path = NULL;
+                        break;
+                }
+
+                //new-start
+                l = path->p_depth;
+                path_tmp = path;
+                for(k=0; k<=l; k++, path_tmp++) {
+                        if(path_tmp->p_idx) {
+                                new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                                if(new_dep) {
+					new_dep->type = TYPE_META;
+                                        new_dep->pblk_num = ext4_idx_pblock(path_tmp->p_idx);
+                                        new_dep->blk_num = ext4_idx_pblock(path_tmp->p_idx);
+                                        new_dep->blk_len = EXT4_BLOCK_SIZE(inode->i_sb);
+                                        new_dep->next = NULL;
+                                        *last_dep = new_dep;
+                                        last_dep = &(new_dep->next);
+					//PARALFETCH
+					if(head_dep == NULL)
+						head_dep = &new_dep;
+                                }
+                        }
+                }
+                //new-end
+
+                depth = ext_depth(inode);
+                if (unlikely(path[depth].p_hdr == NULL)) {
+                        up_read(&EXT4_I(inode)->i_data_sem);
+                        EXT4_ERROR_INODE(inode, "path[%d].p_hdr == NULL", depth);
+                        err = -EIO;
+                        break;
+                }
+                ex = path[depth].p_ext;
+                next = ext4_ext_next_allocated_block(path);
+
+                flags = 0;
+                exists = 0;
+                if (!ex) {
+                        /* there is no extent yet, so try to allocate
+                         * all requested space */
+                        start = block;
+			end = block + num;
+                } else if (le32_to_cpu(ex->ee_block) > block) {
+                        /* need to allocate space before found extent */
+                        start = block;
+                        end = le32_to_cpu(ex->ee_block);
+                        if (block + num < end)
+                                end = block + num;
+                } else if (block >= le32_to_cpu(ex->ee_block)
+                                + ext4_ext_get_actual_len(ex)) {
+                        /* need to allocate space after found extent */
+                        start = block;
+                        end = block + num;
+                        if (end >= next)
+                                end = next;
+                } else if (block >= le32_to_cpu(ex->ee_block)) {
+                        /*
+                         * some part of requested space is covered
+                         * by found extent
+                         */
+                        start = block;
+                        end = le32_to_cpu(ex->ee_block)
+                                + ext4_ext_get_actual_len(ex);
+                        if (block + num < end)
+                                end = block + num;
+                        exists = 1;
+                } else {
+                        BUG();
+                }
+                BUG_ON(end <= start);
+
+		if (!exists) {
+                        es.es_lblk = start;
+                        es.es_len = end - start;
+                        es.es_pblk = 0;
+                } else {
+                        es.es_lblk = le32_to_cpu(ex->ee_block);
+                        es.es_len = ext4_ext_get_actual_len(ex);
+                        es.es_pblk = ext4_ext_pblock(ex);
+                        if (ext4_ext_is_unwritten(ex))
+                                flags |= FIEMAP_EXTENT_UNWRITTEN;
+
+                        //new-start
+                        new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                        if(new_dep) {
+                                new_dep->type = TYPE_REGFILE;
+                                new_dep->pblk_num = es.es_pblk + (block-es.es_lblk);
+                                new_dep->blk_num = block;
+                                if(es.es_lblk + es.es_len >= block + num) new_dep->blk_len = num;
+                                else new_dep->blk_len = es.es_len - (block - es.es_lblk);
+                                new_dep->next = NULL;
+                                *last_dep = new_dep;
+                                last_dep = &(new_dep->next);
+				//PARALFETCH
+				if(head_dep != NULL) {
+					tmp_dep = *head_dep;
+					while(tmp_dep != new_dep)
+					{
+						if(tmp_dep->pblk_num > new_dep->pblk_num)
+						{
+							printk("INVERSED METADATA: meta->pblk:%llu, data->pblk:%llu\n",
+									tmp_dep->pblk_num, new_dep->pblk_num);
+						}
+						tmp_dep = tmp_dep->next;
+					}
+				}
+                        }
+                        //new-end
+                }
+		/*
+                 * Find delayed extent and update es accordingly. We call
+                 * it even in !exists case to find out whether es is the
+                 * last existing extent or not.
+                 */
+                next_del = ext4_find_delayed_extent(inode, &es);
+                if (!exists && next_del) {
+                        exists = 1;
+                        flags |= (FIEMAP_EXTENT_DELALLOC |
+                                        FIEMAP_EXTENT_UNKNOWN);
+                }
+                up_read(&EXT4_I(inode)->i_data_sem);
+
+                if (unlikely(es.es_len == 0)) {
+                        EXT4_ERROR_INODE(inode, "es.es_len == 0");
+                        err = -EIO;
+                        break;
+                }
+
+                /*
+                 * This is possible iff next == next_del == EXT_MAX_BLOCKS.
+                 * we need to check next == EXT_MAX_BLOCKS because it is
+                 * possible that an extent is with unwritten and delayed
+                 * status due to when an extent is delayed allocated and
+                 * is allocated by fallocate status tree will track both of
+                 * them in a extent.
+                 *
+                 * So we could return a unwritten and delayed extent, and
+                 * its block is equal to 'next'.
+                 */
+		if (next == next_del && next == EXT_MAX_BLOCKS) {
+                        flags |= FIEMAP_EXTENT_LAST;
+                        if (unlikely(next_del != EXT_MAX_BLOCKS ||
+                                                next != EXT_MAX_BLOCKS)) {
+                                EXT4_ERROR_INODE(inode,
+                                                "next extent == %u, next "
+                                                "delalloc extent = %u",
+                                                next, next_del);
+                                err = -EIO;
+                                break;
+                        }
+                }
+
+                //new-start
+                /*
+                if (exists) {
+                        err = fiemap_fill_next_extent(fieinfo,
+                                        (__u64)es.es_lblk << blksize_bits,
+                                        (__u64)es.es_pblk << blksize_bits,
+                                        (__u64)es.es_len << blksize_bits,
+                                        flags);
+                        if (err < 0)
+                                break;
+                        if (err == 1) {
+                                err = 0;
+                                break;
+                        }
+                }
+                */
+                //new-end
+
+		block = es.es_lblk + es.es_len;
+
+		//PARALFETCH
+		head_dep = NULL;
+        }
+
+        ext4_ext_drop_refs(path);
+        kfree(path);
+        return err;
+}
+
+void ext4_log_block_path(struct inode *inode, struct ext4_ext_path *path, struct dependency_entry **dep_ptr)
+{
+        struct ext4_iloc iloc;
+        int k, l = path->p_depth;
+
+        k = ext4_get_inode_loc(inode, &iloc);
+
+        if(!k) printk("inum: 0x%x:%lu(loc:%lu) ", inode->i_sb->s_dev, inode->i_ino, iloc.bh->b_blocknr);
+        else printk("inum: 0x%x:%lu(loc:unknown) ", inode->i_sb->s_dev, inode->i_ino);
+
+        for(k=0; k<=l; k++, path++) {
+                if(path->p_idx) printk("I %d->%llu ", le32_to_cpu(path->p_idx->ei_block), ext4_idx_pblock(path->p_idx));
+                //else if(path->p_ext) printk("E %d+%d->%llu ", le32_to_cpu(path->p_ext->ee_block), ext4_ext_get_actual_len(path->p_ext), ext4_ext_pblock(path->p_ext));
+                //else printk("]");
+        }
+        printk("\n");
+
+        brelse(iloc.bh);
+}
diff -uNr linux-4.12.9/fs/ext4/inode.c linux-4.12.9-pf/fs/ext4/inode.c
--- linux-4.12.9/fs/ext4/inode.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/ext4/inode.c	2019-01-02 21:24:55.629599367 +0900
@@ -48,6 +48,10 @@
 
 #define MPAGE_DA_EXTENT_TAIL 0x01
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
 static __u32 ext4_inode_csum(struct inode *inode, struct ext4_inode *raw,
 			      struct ext4_inode_info *ei)
 {
@@ -3270,11 +3274,38 @@
 	return generic_block_bmap(mapping, block, ext4_get_block);
 }
 
+//PARALFETCH
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
 static int ext4_readpage(struct file *file, struct page *page)
 {
 	int ret = -EAGAIN;
 	struct inode *inode = page->mapping->host;
 
+	//PARALFETCH
+        sector_t block_in_file;
+        int idx;
+        struct disk_log_entry *ff_log;
+
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                if(inode) {
+                        block_in_file = (sector_t)page->index << (PAGE_SHIFT - inode->i_blkbits);
+                        if(inode->i_sb && inode->i_sb->s_dev) {
+                                idx = get_log_entry(flashfetch_trace);
+                                if(idx >= 0) {
+                                        ff_log = &(flashfetch_trace->log[idx]);
+                                        ff_log->ts = ktime_to_ns(ktime_get());
+                                        ff_log->dev = inode->i_sb->s_dev;
+                                        ff_log->ino = inode->i_ino;
+                                        ff_log->blk_num = (u64)block_in_file;
+                                        ff_log->blk_len = (u32)1 /*PAGE_SIZE*/;
+                                        /*if(ff_log->dev == 0)*/ printk("RRR [%d]:%lld dev:0x%x ino:%lu offs:%llu size:%u\n", idx, ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+                                }
+                        }
+                }
+        }
+
 	trace_ext4_readpage(page);
 
 	if (ext4_has_inline_data(inode))
@@ -3291,6 +3322,30 @@
 		struct list_head *pages, unsigned nr_pages)
 {
 	struct inode *inode = mapping->host;
+	//PARALFETCH
+        sector_t block_in_file;
+        struct page *page;
+        int idx;
+        struct disk_log_entry *ff_log;
+
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                if(inode) {
+                        page = list_entry(pages->prev, struct page, lru);
+                        block_in_file = (sector_t)page->index << (PAGE_SHIFT - inode->i_blkbits);
+                        if(inode->i_sb && inode->i_sb->s_dev) {
+                                idx = get_log_entry(flashfetch_trace);
+                                if(idx >= 0) {
+                                        ff_log = &(flashfetch_trace->log[idx]);
+                                        ff_log->ts = ktime_to_ns(ktime_get());
+                                        ff_log->dev = inode->i_sb->s_dev;
+                                        ff_log->ino = inode->i_ino;
+                                        ff_log->blk_num = (u64)block_in_file;
+                                        ff_log->blk_len = (u32)nr_pages/* * PAGE_SIZE */;
+                                        /*if(ff_log->dev == 0)*/ printk("RRRR [%d]:%lld dev:0x%x ino:%lu offs:%llu size:%u\n", idx, ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+                                }
+                        }
+                }
+        }
 
 	/* If the file has inline data, no need to do readpages. */
 	if (ext4_has_inline_data(inode))
diff -uNr linux-4.12.9/fs/flashfetch_core.c linux-4.12.9-pf/fs/flashfetch_core.c
--- linux-4.12.9/fs/flashfetch_core.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_core.c	2019-01-02 21:24:54.295591806 +0900
@@ -0,0 +1,688 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/writeback.h>
+#include <linux/sysctl.h>
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/flashfetch.h>
+#include <linux/jiffies.h>
+#include <linux/param.h>
+
+#include <linux/debugfs.h>
+#include <linux/kthread.h>
+
+//PARALFETCH
+#include <linux/mm_inline.h>
+
+#include <linux/flashfetch_trace.h>
+#include <linux/signal.h>
+
+unsigned int flashfetch_monitor_blkio = 0;
+EXPORT_SYMBOL(flashfetch_monitor_blkio);
+
+unsigned int flashfetch_monitor_rq_complete = 0;
+EXPORT_SYMBOL(flashfetch_monitor_rq_complete);
+
+unsigned int flashfetch_monitor_timeout = FFTIMEOUT;
+EXPORT_SYMBOL(flashfetch_monitor_timeout);
+
+struct disk_trace *flashfetch_trace = NULL;
+EXPORT_SYMBOL(flashfetch_trace);
+
+unsigned int flashfetch_flags = 0;
+EXPORT_SYMBOL(flashfetch_flags);
+
+u64 flashfetch_launch_ts = 0;
+EXPORT_SYMBOL(flashfetch_launch_ts);
+
+//PARALFETCH
+int pfault_trace = 0;
+int pfault_debug = 0;
+
+unsigned long long int flashfetch_readahead_hit = 0;
+EXPORT_SYMBOL(flashfetch_readahead_hit);
+
+unsigned long long int flashfetch_prefetch_cnt = 0;
+EXPORT_SYMBOL(flashfetch_prefetch_cnt);
+
+unsigned long long int flashfetch_prefetch_hit = 0;
+EXPORT_SYMBOL(flashfetch_prefetch_hit);
+
+
+//PARALFETCH
+pid_t launch_pid = 0;
+
+spinlock_t flashfetch_lock = __SPIN_LOCK_UNLOCKED(flashfetch_lock);
+
+int flashfetch_async(char *appname)
+{
+	int ret;
+	//printk("flashfetch_async: %s\n", appname);
+	ret = flashfetch_prefetch_async(appname);
+	return ret;
+}
+
+int flashfetch_sync(char *appname)
+{
+	int ret;
+	//printk("flashfetch_sync: %s\n", appname);
+	printk("jiffies before prefetch : %lu\n", jiffies);
+	ret = flashfetch_prefetch_sync(appname);
+	printk("jiffies after prefetch : %lu\n", jiffies);
+	return ret;
+}
+
+extern void drop_pagecache_sb(struct super_block *sb, void *unused);
+extern void drop_slab(void);
+
+extern int metadata_shift_onoff;
+extern int metadata_shift_way;
+extern int metadata_shift_value;
+extern int io_distance_merge_onoff;
+extern int io_distance_allowed;
+extern int io_infill_distance_merge_onoff;
+extern int io_infill_distance_allowed;
+extern int io_infill_hole_allowed_blks;
+extern unsigned long long int evaluator_lba;
+
+//PARALFETCH
+extern int rotational_value;
+extern pid_t launch_pid;
+
+int seqmon_thread(void *params)
+//void seqmon_thread(struct work_struct *work)
+{
+	struct seqmon_param *param;
+	struct disk_log_entry *logs;
+	int i, log_count;
+	replay_info *trace_buffer = NULL;
+	int trace_count;
+	//int j;
+	
+	//param = (struct seqmon_param *)work->data;
+	param = (struct seqmon_param *)params;
+
+	printk(KERN_WARNING "seqmon_thread appname:%s, filename:%s\n", param->appname, param->filename);
+	
+	//daemonize("flashfetch_seqmon");
+	allow_signal(SIGKILL);
+	printk("jiffies before timeout : %lu\n", jiffies);
+	schedule_timeout_uninterruptible(flashfetch_monitor_timeout * HZ);
+	printk("jiffies after timeout : %lu\n", jiffies);
+
+	//PARALFETCH
+	launch_pid = 0;
+
+	flashfetch_monitor_blkio = 0;
+	
+	logs = kmalloc(sizeof(struct disk_log_entry) * (flashfetch_trace->pos), GFP_KERNEL);
+	if(logs) {
+		memcpy((char *)logs, (char *)flashfetch_trace->log, sizeof(struct disk_log_entry) * (flashfetch_trace->pos));
+	}
+
+/*
+#define TIME_INTERVAL (unsigned long)1000000000
+#define MIN_COUNT 5
+
+	for(i=0, log_count=0; i<flashfetch_trace->pos; i++)
+	{
+		if(logs[i].ts - logs[log_count].ts < TIME_INTERVAL) continue;
+		for(j=log_count; j<i; j++) {
+			if(logs[i].ts - logs[j].ts > TIME_INTERVAL) log_count++;
+		}
+		if(i - log_count < MIN_COUNT) log_count = i;
+	}
+	*/
+
+	log_count = flashfetch_trace->pos;
+
+	reset_disk_trace(flashfetch_trace);
+	flashfetch_flags &= ~FF_INPROGRESS;
+
+	flashfetch_launch_ts = 0;
+
+	// process logs here
+	printk("S-ORIGINAL TRACE\n");
+	printk("log count: %d\n", log_count);
+	for(i=0; i<log_count; i++)
+	{
+		//if(logs[i].dev != 0) printk("%llu 0x%x %lu %llu %d\n", logs[i].ts, logs[i].dev, logs[i].ino, logs[i].blk_num, logs[i].blk_len);
+		//if(logs[i].dev == 0) printk("%d %llu 0x%x %lu %llu %d\n", i, logs[i].ts, logs[i].dev, logs[i].ino, logs[i].blk_num, logs[i].blk_len);
+	}
+	printk("E-ORIGINAL TRACE\n");
+	
+	if(logs) {
+		//PARALFETCH
+		if(rotational_value != 0) 
+			write_disk_trace_sched_hard(param->appname, logs, log_count, &trace_buffer, &trace_count);
+		else
+			write_disk_trace_sched_flash(param->appname, logs, log_count, &trace_buffer, &trace_count);
+
+		write_disk_replayinfo_raw(param->appname, trace_buffer, trace_count);
+		kfree(logs);
+	}
+	
+	//complete_and_exit(NULL, 0);
+	return 0;
+	//return;
+}
+
+extern void sync_filesystems_ff(int wait);
+
+//DECLARE_DELAYED_WORK(seqmon_work, seqmon_thread);
+
+int flashfetch_genpf(char *appname, const char *filename)
+{
+	struct seqmon_param *params;
+	struct task_struct *t;
+
+	spin_lock(&flashfetch_lock);
+	if(flashfetch_flags & FF_INPROGRESS) {
+		spin_unlock(&flashfetch_lock);
+		return -EBUSY;
+	} 
+
+	flashfetch_flags |= FF_INPROGRESS;
+	spin_unlock(&flashfetch_lock);
+
+	// sync
+	wakeup_flusher_threads(0, WB_REASON_SYNC);
+	//sync_filesystems_ff(0);
+	sync_filesystems_ff(1);
+	//if (unlikely(laptop_mode))
+	//laptop_sync_completion();
+
+	// drop_caches
+	iterate_supers(drop_pagecache_sb, NULL);
+    	drop_slab();
+
+	flashfetch_monitor_blkio = 1;
+
+	printk("flashfetch_genpf: %s, filename : %s\n", appname, filename);
+
+	params = kmalloc(sizeof(struct seqmon_param), GFP_KERNEL);
+	if(params) {
+		strncpy(params->appname, appname, NAME_LEN);
+		strncpy(params->filename, filename, NAME_LEN);
+	} else {
+		printk(KERN_WARNING "Failed to allocate memory for kthread parameters\n");
+		return -ENOMEM;
+	}
+
+	/*
+	if(kernel_thread(seqmon_thread, (void *)params, 0) < 0) {
+		printk(KERN_WARNING "Failed to start async prefetch thread\n");
+		return -EINVAL;
+	}
+	*/
+	/*
+	p = kthread_create_on_node(seqmon_thread, (void *)params, cpu_to_node(smp_processor_id()), "seqmon_thread");
+
+	if(IS_ERR(p)) 
+		return PTR_ERR(p);
+
+	wake_up_process(p);
+	*/
+
+	t = kthread_create(seqmon_thread, (void *)params, "seqmon_thread");
+	if(IS_ERR(t)) {
+		return PTR_ERR(t);
+	}
+
+	wake_up_process(t);
+
+	/*
+	t = kthread_run(seqmon_thread, (void *)params, "paralfetch_monitor");
+	if(IS_ERR(t)) 
+		return PTR_ERR(t);
+	*/
+
+	return 0;
+}
+
+#ifdef CONFIG_DEBUG_FS
+#define MAX_NAME_LENGTH	256
+char appname[MAX_NAME_LENGTH];
+struct dentry *ff_fetch_app_file;
+//PARALFETCH
+struct dentry *ff_monctl_file;                                                 
+unsigned long ff_iomem_max = 0; 
+
+static ssize_t ff_fetch_app_write(struct file *filp, const char __user *buffer,
+		                                size_t count, loff_t *ppos)
+{
+#ifdef PARALFETCH_ANDROID
+	int ret;
+#endif
+	if (count >= MAX_NAME_LENGTH)
+		return -EINVAL;
+
+	if (copy_from_user(appname, buffer, count))
+		return -EFAULT;
+
+	if(appname[count-1] == '\n') appname[count-1] = '\0';
+	else appname[count] = '\0';
+
+#ifdef PARALFETCH_ANDROID
+	ret = flashfetch_async(appname);
+	if(ret) flashfetch_genpf(appname, appname);
+#else
+	flashfetch_sync(appname);
+#endif
+
+	return count;
+}
+
+//PARALFETCH
+#define MONCTL_ACCSEQ1 1                                                       
+#define MONCTL_ACCSEQ2 2                                                       
+#define MONCTL_REFTRC1 3                                                       
+#define MONCTL_REFTRC2 4                                                       
+#define MONCTL_PREFTRC 5
+#define MONCTL_MIN 1                                                           
+#define MONCTL_MAX 5                                                           
+                                                                               
+#define MONCTL_ACCSEQ_PROGRESS 1                                               
+#define MONCTL_REFTRC_PROGRESS 2                                               
+unsigned int monctl_in_progress = 0;                                           
+unsigned int odd_referenced_meta = 0;                                          
+unsigned int even_referenced_meta = 0;                                         
+unsigned int odd_referenced_file = 0;                                          
+unsigned int even_referenced_file = 0;
+
+extern resource_size_t find_last_iomem_res(char *name,
+                                      bool first_level_children_only);
+
+extern int walk_system_ram_res_all(void *arg, int (*func)(u64, u64, void *));
+
+int print_system_ram(u64 start, u64 end, void *arg)
+{
+	printk("System RAM: 0x%llx - 0x%llx\n", start, end);
+	return 0;
+}
+
+int set_page_unreferenced(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PageReferenced(page_ptr)) {
+				ClearPageReferenced(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+void print_page_cache_page(struct page *page)
+{
+	int lru;
+	unsigned long inum;
+	unsigned long offset;
+	struct inode *host;
+	unsigned int dev;
+
+	lru = page_lru(page);
+	if(is_file_lru(lru)) {
+		offset = page->index;
+		if(page->mapping && (((unsigned long)page->mapping & 3) == 0) && page->mapping->host) {
+			host = page->mapping->host;
+			inum = host->i_ino;
+			if(inum != 0) {
+				if(host->i_sb != NULL)
+					dev = host->i_sb->s_dev;
+				else
+					dev = 0;
+			} else
+				dev = host->i_rdev;
+		} else {
+			inum = 0;
+			dev = 0;
+		}
+	} else {
+		offset = 0;
+		inum = 0;
+		dev = 0;
+	}
+
+	if(dev) printk("-PCP- dev: 0x%x, inum: %lu, offset: %lu\n",
+			dev, inum, offset);
+}
+
+int log_page_referenced(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PageReferenced(page_ptr)) {
+				//                             if(l%100 == 0) print_page_cache_page(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+int log_page_prefetched(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PagePrefetch(page_ptr)) {
+				//                             if(l%100 == 0) print_page_cache_page(page_ptr);
+				print_page_cache_page(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+static ssize_t ff_monctl_write(struct file *filp, const char __user *buffer,
+                                               size_t count, loff_t *ppos)
+{
+#define MAX_NUM_LENGTH 10
+	char monstr[MAX_NUM_LENGTH];
+	long monctl;
+	unsigned int ref_count;
+
+	if (count >= MAX_NUM_LENGTH)
+		return -EINVAL;
+
+	if (copy_from_user(monstr, buffer, count))
+		return -EFAULT;
+
+	if(monstr[count-1] == '\n') monstr[count-1] = '\0';
+	else monstr[count] = '\0';
+
+
+	monctl = simple_strtol(monstr, NULL, 10);
+	if(monctl >= MONCTL_MIN && monctl <= MONCTL_MAX) {
+		switch(monctl) {
+			case MONCTL_ACCSEQ1:
+				monctl_in_progress = MONCTL_ACCSEQ_PROGRESS;
+				break;
+			case MONCTL_ACCSEQ2:
+				monctl_in_progress = 0;
+				break;
+			case MONCTL_REFTRC1:
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				odd_referenced_meta = 0;
+				even_referenced_meta = 0;
+				odd_referenced_file = 0;
+				even_referenced_file = 0;
+				walk_system_ram_res_all(&ref_count, set_page_unreferenced);
+				//walk_system_ram_res_all(NULL, print_system_ram);
+				printk("Paralfetch ref_count: %u\n", ref_count);
+				monctl_in_progress = MONCTL_REFTRC_PROGRESS;
+				break;
+			case MONCTL_REFTRC2:
+				monctl_in_progress = 0;
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				walk_system_ram_res_all(&ref_count, log_page_referenced);
+				//walk_system_ram_res_all(NULL, print_system_ram);
+				printk("Paralfetch ref_count: %u, odd_referenced_meta: %u, even_referenced_meta: %u, odd_referenced_file: %u, even_referenced_file: %u\n", ref_count, odd_referenced_meta, even_referenced_meta, odd_referenced_file, even_referenced_file);
+				break;
+			case MONCTL_PREFTRC:
+				monctl_in_progress = 0;
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				walk_system_ram_res_all(&ref_count, log_page_prefetched);
+				printk("Paralfetch prefetched count: %u\n", ref_count);
+				break;
+			default:
+				break;
+		}
+	}
+
+	return count;
+}
+//PARALFETCH END
+
+static const struct file_operations ff_fetch_app_fops = {
+	.owner =        THIS_MODULE,
+	.open =         simple_open,
+	.write =        ff_fetch_app_write,
+	.llseek =       noop_llseek,
+};
+
+//PARALFETCH
+static const struct file_operations ff_monctl_fops = {
+       .owner =        THIS_MODULE,
+       .open =         simple_open,
+       .write =        ff_monctl_write,
+       .llseek =       noop_llseek,
+};
+#endif
+
+static int __init init_flashfetch(void)
+{
+	int ret;
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *root = debugfs_create_dir("flashfetch", NULL);
+	if (root == NULL)
+		return -ENXIO;
+	ff_fetch_app_file = debugfs_create_file("fetch_app", S_IWUGO, root, NULL, &ff_fetch_app_fops);
+
+	//PARALFETCH
+	ff_monctl_file = debugfs_create_file("monctl", S_IWUGO, root, NULL, &ff_monctl_fops);
+
+	debugfs_create_u32("flashfetch_monitor_blkio", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_blkio);
+	debugfs_create_u32("flashfetch_monitor_rq_complete", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_rq_complete);
+	debugfs_create_u32("flashfetch_monitor_timeout", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_timeout);
+	debugfs_create_u64("evaluator_lba", S_IRUGO|S_IWUGO, root, &evaluator_lba);
+	debugfs_create_u32("io_distance_allowed", S_IRUGO|S_IWUGO, root, &io_distance_allowed);
+	debugfs_create_u32("io_infill_distance_allowed", S_IRUGO|S_IWUGO, root, &io_infill_distance_allowed);
+	debugfs_create_u32("io_infill_hole_allowed_blks", S_IRUGO|S_IWUGO, root, &io_infill_hole_allowed_blks);
+	debugfs_create_u32("metadata_shift_way", S_IRUGO|S_IWUGO, root, &metadata_shift_way);
+	debugfs_create_u32("metadata_shift_value", S_IRUGO|S_IWUGO, root, &metadata_shift_value);
+
+	//PARALFETCH
+	debugfs_create_u32("rotational", S_IRUGO|S_IWUGO, root, &rotational_value);
+	debugfs_create_u32("pfault_trace", S_IRUGO|S_IWUGO, root, &pfault_trace);
+	debugfs_create_u32("pfault_debug", S_IRUGO|S_IWUGO, root, &pfault_debug);
+	debugfs_create_u64("flashfetch_readahead_hit", S_IRUGO, root,
+			&flashfetch_readahead_hit);
+	debugfs_create_u64("flashfetch_prefetch_cnt", S_IRUGO, root,
+			&flashfetch_prefetch_cnt);
+	debugfs_create_u64("flashfetch_prefetch_hit", S_IRUGO, root,
+			&flashfetch_prefetch_hit);
+#endif
+
+	flashfetch_lock = __SPIN_LOCK_UNLOCKED(flashfetch_lock);
+	ret = alloc_disk_trace(&flashfetch_trace);
+	if(ret < 0) flashfetch_trace = NULL;
+
+	return 0;
+}
+
+static void __exit exit_flashfetch(void)
+{
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(ff_fetch_app_file);
+	//PARALFETCH
+	debugfs_remove(ff_monctl_file);
+#endif
+	return ;
+}
+
+core_initcall(init_flashfetch);
+module_exit(exit_flashfetch);
+MODULE_LICENSE("DUAL BSD/GPL");
diff -uNr linux-4.12.9/fs/flashfetch_evaluate.c linux-4.12.9-pf/fs/flashfetch_evaluate.c
--- linux-4.12.9/fs/flashfetch_evaluate.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_evaluate.c	2019-01-02 21:24:54.309591885 +0900
@@ -0,0 +1,76 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/buffer_head.h>
+#include <linux/gfp.h>
+#include "ext4/ext4.h"
+
+unsigned long long int evaluator_lba = 0;
+EXPORT_SYMBOL(evaluator_lba);
+
+void __invalidate_pagecache_entry(struct address_space *mapping, pgoff_t start, pgoff_t end);
+
+void __invalidate_buffercache_entry(struct block_device *bdev, sector_t block, unsigned size)
+{
+	struct buffer_head *bh;
+	pgoff_t index;
+	struct page *page;
+	
+	bh = __find_get_block(bdev, block, size);
+
+	if(bh) {
+		printk("bh ok!\n");
+		lock_buffer(bh);
+		clear_buffer_uptodate(bh);
+		unlock_buffer(bh);
+		put_bh(bh);
+	}
+	if(bdev && bdev->bd_inode) {
+		printk("bdev->bd_inode ok!\n");
+		index = block >> (PAGE_SHIFT - bdev->bd_inode->i_blkbits);
+		if(bdev->bd_inode->i_mapping) {
+			page = find_get_page(bdev->bd_inode->i_mapping, index);
+			clear_bit(PG_uptodate, &page->flags);
+			//page_cache_release(page);
+		}
+		//__invalidate_pagecache_entry(bdev->bd_inode->i_mapping, block, block-1+(size/4096));
+	}
+}
+
+void __invalidate_pagecache_entry(struct address_space *mapping, pgoff_t start, pgoff_t end)
+{
+	if(end >= start)
+		invalidate_mapping_pages(mapping, start, end);
+}
+
+void invalidate_buffercache_entry(const char *pathname, sector_t block, unsigned size)
+{
+	struct block_device *bdev;
+	
+	bdev = lookup_bdev(pathname);
+	if(bdev) 
+		__invalidate_buffercache_entry(bdev, block, size);
+}
+EXPORT_SYMBOL(invalidate_buffercache_entry);
+
+void invalidate_pagecache_entry(const char *pathname, unsigned long ino, pgoff_t start, pgoff_t end)
+{
+	struct super_block *sb;
+	struct block_device *bdev;
+	struct inode *inode;
+	
+	bdev = lookup_bdev(pathname);
+	if(!bdev) return;
+
+	sb = get_super(bdev);
+	if(!sb) return;
+
+	inode = ext4_iget(sb, ino);
+	if(inode) {
+		__invalidate_pagecache_entry(inode->i_mapping, start, end);
+		iput(inode);
+	}
+
+	if(sb) drop_super(sb);
+}
+EXPORT_SYMBOL(invalidate_pagecache_entry);
diff -uNr linux-4.12.9/fs/flashfetch_fetch.c linux-4.12.9-pf/fs/flashfetch_fetch.c
--- linux-4.12.9/fs/flashfetch_fetch.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_fetch.c	2019-01-02 21:24:54.147590967 +0900
@@ -0,0 +1,593 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/fs.h>
+#include <linux/file.h>
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+#include "internal.h"
+//#include <linux/ext2_fs.h>
+#include "ext4/ext4.h"
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/kthread.h>
+#include <linux/flashfetch_trace.h>
+#include <linux/flashfetch_fetch.h>
+#include <linux/flashfetch_scheduler.h>
+
+extern ssize_t kernel_write(struct file *file, const char *buf, size_t count, loff_t pos);
+int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count);
+int flashfetch_load_trace_from_file(char *filename, replay_info **trace_pptr, int *trace_cnt);
+int flashfetch_do_prefetcher(replay_info *trace_ptr, int trace_cnt);
+void print_paralfetch_header(paralfetch_header *header);
+int flashfetch_do_async_thread(void *params);
+
+//PARALFETCH
+//state, 0:nothing, 1:in prefetching, 2: waiting
+int prefetch_state = 0;
+
+extern int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct scheduler_entry **sched_last);
+
+extern struct file *get_empty_filp(void);
+//extern struct inode *ext3_iget(struct super_block *sb, unsigned long ino);
+
+void print_paralfetch_header(paralfetch_header *header)
+{
+	if(!header) return;
+	printk("-- header --\n");
+	printk("magic code : %c%c%c%c%c%c%c%c\n", header->magic[0], header->magic[1], header->magic[2], header->magic[3], header->magic[4], header->magic[5], header->magic[6], header->magic[7]);
+	printk("version : %u\n", header->version);
+	printk("total io count : %u\n", header->io_count);
+	printk("uncertain io count : %u\n", header->uncertain_io_count);
+	printk("uncertain io size : %u\n", header->uncertain_io_size);
+	printk("harmful io count : %u\n", header->harmful_io_count);
+	printk("harmful io size : %u\n", header->harmful_io_size);
+	printk("private : %u\n", header->private);
+}
+
+typedef struct _async_params {
+	replay_info *trace_ptr;
+	int trace_cnt;
+} async_params;
+
+int flashfetch_prefetch_sync(char *app_name)
+{
+	char app_path[NAME_LEN + 1];
+	paralfetch_header *header;
+	replay_info *trace_ptr;
+	int trace_cnt;
+	int ret;
+	unsigned long long nstime;
+	async_params *params;
+
+	nstime = ktime_to_ns(ktime_get());
+	
+	printk(KERN_NOTICE "PS %llu %s\n", nstime, app_name);
+
+	prefetch_state = 1;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", app_name);
+
+	ret = flashfetch_load_trace_from_file(app_path, (replay_info **)&header, &trace_cnt);
+	if(ret < 0) return ret;
+
+	print_paralfetch_header(header);
+
+	printk(KERN_NOTICE "OKOK!:%u, %d\n", header->private, header->private);
+
+	trace_ptr = (replay_info *)(&(header->trace_data));
+
+	header->private = 0;
+
+	if(header->private == 0) {
+		printk(KERN_NOTICE "PRIVATE=0\n");
+		flashfetch_do_prefetcher(trace_ptr, trace_cnt);
+	} else if(header->private < (unsigned short)trace_cnt) {
+		printk(KERN_NOTICE "PRIVATE>0:%d\n", header->private);
+		flashfetch_do_prefetcher(trace_ptr, header->private);
+		params = kmalloc(sizeof(async_params), GFP_KERNEL);
+		if(params == NULL) {
+			kfree(trace_ptr);
+			return -ENOMEM;
+		}
+
+		params->trace_ptr = trace_ptr + header->private;
+		params->trace_cnt = trace_cnt - header->private;
+
+		if(kernel_thread(flashfetch_do_async_thread, (void *)params, 0) < 0) {
+			printk(KERN_WARNING "Failed to start async prefetch thread\n");
+			kfree(trace_ptr);
+			return -EINVAL;
+		}
+	}
+
+	kfree(trace_ptr);
+
+	nstime = ktime_to_ns(ktime_get());
+	
+	prefetch_state = 0;
+
+	printk(KERN_NOTICE "PE %llu %s\n", nstime, app_name);
+
+	return 0;
+}
+
+int flashfetch_do_async_thread(void *params)
+{
+	async_params *params_tmp = (async_params *)params;
+
+	printk(KERN_CRIT "ASYNC THREAD START\n");
+
+	prefetch_state = 1;
+
+	flashfetch_do_prefetcher(params_tmp->trace_ptr, params_tmp->trace_cnt);
+
+	prefetch_state = 0;
+
+	printk(KERN_CRIT "ASYNC THREAD END\n");
+
+	kfree((void *)container_of((void *)(params_tmp->trace_ptr), struct __paralfetch_header, trace_data));
+	// kfree(params_tmp->trace_ptr);
+	kfree(params_tmp);
+
+	return 0;
+}
+
+int flashfetch_prefetch_async(char *app_name)
+{
+	char app_path[NAME_LEN + 1];
+	paralfetch_header *header;
+	replay_info *trace_ptr;
+	int trace_cnt;
+	async_params *params;
+	int ret;
+	struct task_struct *prefetch_task;
+
+	params = kmalloc(sizeof(async_params), GFP_KERNEL);
+	if(params == NULL) return -ENOMEM;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", app_name);
+
+	ret = flashfetch_load_trace_from_file(app_path, (replay_info **)&header, &trace_cnt);
+	trace_ptr = (replay_info *)(&(header->trace_data));
+	//ret = flashfetch_load_trace_from_file(app_path, &trace_ptr, &trace_cnt);
+	if(ret < 0) {
+		kfree(params);
+		return ret;
+	}
+
+	print_paralfetch_header(header);
+
+	params->trace_ptr = trace_ptr;
+	params->trace_cnt = trace_cnt;
+
+	prefetch_task = kthread_create(flashfetch_do_async_thread, (void *)params, "pf_task");
+	if(IS_ERR(prefetch_task)){
+		printk("Unable to start prefetch thread.\n");
+	} else
+		wake_up_process(prefetch_task);
+	/*
+	   if(kernel_thread(flashfetch_do_async_thread, (void *)params, 0) < 0) {
+	   printk(KERN_CRIT "Failed to start async prefetch thread\n");
+	   return -EINVAL;
+	   }
+	   */
+	printk(KERN_CRIT "Succeeded to start async prefetch thread\n");
+
+	return 0;
+}
+
+// Read permission of flashfetch trace file must be allowed to every user.
+struct file *kernel_open(char const *file_name, int flags, int mode)
+{
+	struct file *file = NULL;
+#if BITS_PER_LONG != 32
+	flags |= O_LARGEFILE;
+#endif
+
+	file = filp_open(file_name, flags, mode);
+
+	return file;
+}
+
+int kernel_close(struct file *file)
+{
+	if(file->f_op && file->f_op->flush) {
+		file->f_op->flush(file, current->files);
+	}
+	fput(file);
+
+	return 0;
+}
+
+int write_disk_replayinfo_raw(char *appname, replay_info *trace_buffer, int trace_count)
+{
+	struct file *file;
+	//int i;
+	int ret;
+	loff_t data_write;
+	loff_t file_size;
+	char app_path[NAME_LEN + 1];
+	paralfetch_header header;
+
+	if(!trace_count) return -ENOENT;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", appname);
+
+	file = kernel_open(app_path, O_RDWR | O_CREAT | O_TRUNC, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for writing. errno=%d\n", appname, ret);
+		return ret;
+	}
+
+	header.magic[0] = 'P';
+	header.magic[1] = 'A';
+	header.magic[2] = 'R';
+	header.magic[3] = 'F';
+	header.magic[4] = 'E';
+	header.magic[5] = 'T';
+	header.magic[6] = 'C';
+	header.magic[7] = 'H';
+	header.version = 1;
+	header.io_count = trace_count;
+	header.uncertain_io_count = 0;
+	header.uncertain_io_size = 0;
+	header.harmful_io_count = 0;
+	header.harmful_io_size = 0;
+	header.private = 0;
+
+	ret = kernel_write(file, (char *)&header, sizeof(paralfetch_header), 0);
+	printk("header write size : %d\n", ret);
+
+	if(ret < 0 || ret == 0) {
+		printk("Error while writing from file %s, error=%d\n", appname, ret);
+		goto out_close_free;
+	}
+
+	data_write = 0;
+	file_size = sizeof(replay_info) * trace_count;
+
+	while(data_write < file_size) {
+		ret = kernel_write(file, (char *)trace_buffer + data_write, file_size - data_write, data_write + sizeof(paralfetch_header));
+		printk("trace write size : %d\n", ret);
+
+		if(ret < 0) {
+			printk("Error while writing from file %s, error=%d\n", appname, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data write=%lld, expected size=%lld\n", data_write, file_size);
+			goto out_close_free;
+		}
+		data_write += ret;
+	}
+
+	if(trace_buffer) kfree(trace_buffer);
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+	//out_close:
+	kernel_close(file);
+	return ret;
+}
+
+int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count)
+{
+	struct file *file;
+	paralfetch_header header;
+	replay_info *trace_buffer;
+	int i;
+	int ret;
+	loff_t data_write;
+	loff_t file_size;
+	char app_path[NAME_LEN + 1];
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", appname);
+
+	file = kernel_open(app_path, O_RDWR | O_CREAT | O_TRUNC, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for writing. errno=%d\n", appname, ret);
+		return ret;
+	}
+
+	header.magic[0] = 'P';
+	header.magic[1] = 'A';
+	header.magic[2] = 'R';
+	header.magic[3] = 'F';
+	header.magic[4] = 'E';
+	header.magic[5] = 'T';
+	header.magic[6] = 'C';
+	header.magic[7] = 'H';
+	header.version = 1;
+	header.io_count = log_count;
+	header.uncertain_io_count = 0;
+	header.uncertain_io_size = 0;
+	header.harmful_io_count = 0;
+	header.harmful_io_size = 0;
+	header.private = 0;
+
+	ret = kernel_write(file, (char *)&header, sizeof(paralfetch_header), 0);
+	printk("header write size : %d\n", ret);
+
+	if(ret < 0 || ret == 0) {
+		printk("Error while writing from file %s, error=%d\n", appname, ret);
+		goto out_close;
+	}
+
+	trace_buffer = kmalloc(sizeof(replay_info) * log_count, GFP_KERNEL);
+	if(!trace_buffer) {
+		ret = -ENOMEM;
+		goto out_close;
+	}
+
+	for(i=0; i<log_count; i++)
+	{
+		trace_buffer[i].dev = logs[i].dev;
+		trace_buffer[i].ino = logs[i].ino;
+		trace_buffer[i].start = logs[i].blk_num;
+		trace_buffer[i].length = logs[i].blk_len;
+	}
+
+	data_write = 0;
+	file_size = sizeof(replay_info) * log_count;
+
+	while(data_write < file_size) {
+		ret = kernel_write(file, (char *)trace_buffer + data_write, file_size - data_write, data_write + sizeof(paralfetch_header));
+		printk("trace write size : %d\n", ret);
+
+		if(ret < 0) {
+			printk("Error while writing from file %s, error=%d\n", appname, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data write=%lld, expected size=%lld\n", data_write, file_size);
+			break;
+		}
+		data_write += ret;
+	}
+
+	if(trace_buffer) kfree(trace_buffer);
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+out_close:
+	kernel_close(file);
+	return ret;
+
+}
+
+int flashfetch_load_trace_from_file(char *filename, replay_info **trace_pptr, int *trace_cnt)
+{
+	struct file *file;
+	replay_info *trace_buffer = NULL;
+	char *trace_cbuf;
+	int ret = 0;
+	int file_size;
+	int data_read = 0;
+
+	file = kernel_open(filename, O_RDONLY, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for reading. errno=%d\n", filename, ret);
+		return ret;
+	}
+	
+	file_size = file->f_mapping->host->i_size;
+
+	trace_buffer = kmalloc(file_size, GFP_KERNEL);
+	if(!trace_buffer) {
+		ret = -ENOMEM;
+		goto out_close;
+	}
+
+	trace_cbuf = (char *)trace_buffer;
+
+	while(data_read < file_size) {
+		ret = kernel_read(file, data_read, trace_cbuf + data_read, file_size - data_read);
+
+		if(ret < 0) {
+			printk("Error while reading from file %s, error=%d\n", filename, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data read=%d, expected size=%d\n", data_read, file_size);
+			break;
+		}
+		data_read += ret;
+	}
+
+	if(data_read == file_size) {
+		*trace_pptr = trace_buffer;
+		*trace_cnt = (file_size-sizeof(paralfetch_header))/sizeof(replay_info);
+		//printk("file_size=%d, sizeof(replay_info)=%d\n", file_size, sizeof(replay_info));
+	} else {
+		printk("Trace file size changed beneath us, cancelling read\n");
+		ret = -ETXTBSY;
+		goto out_close_free;
+	}
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+out_close:
+	kernel_close(file);
+	return ret;
+}
+
+extern unsigned long long int flashfetch_prefetch_cnt;
+
+int flashfetch_do_prefetcher(replay_info *trace_ptr, int trace_cnt)
+{
+	int i, ret = 0;
+	int j;
+	int end;
+	struct block_device *bdev_prev = NULL;
+	dev_t dev_prev = 0;
+	unsigned int bs_prev;
+	replay_info *trace_curr = NULL;
+	struct file *file_prev = NULL;
+	struct inode *ino_prev = NULL;
+	struct super_block *sb = NULL;
+
+	struct blk_plug bh_plug;
+
+	//struct buffer_head *bh;
+
+	if(trace_cnt == 0 || trace_ptr == NULL) return 0;
+
+	trace_curr = trace_ptr;
+
+	printk("trace count : %d\n", trace_cnt);
+
+	sb = user_get_super(trace_curr->dev);
+
+	if(!sb) {
+		return -ENODEV;
+	}
+
+	dev_prev = trace_curr->dev;
+	bdev_prev = sb->s_bdev;
+	bs_prev = 1 << sb->s_blocksize_bits;
+
+	for(i=0; i<trace_cnt; i++, trace_curr++)
+	{
+		if(trace_curr->ino == 0) {
+			if(likely(dev_prev == trace_curr->dev)) {
+				// printk("1) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				end = (trace_curr->length+bs_prev-1)/bs_prev;
+				if(end > 1) {
+					blk_start_plug(&bh_plug);
+					for(j=0; j<end; j++)
+					{
+						// printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start+j, bs_prev);
+						__breadahead(sb->s_bdev, trace_curr->start+j, bs_prev);
+						//__bread(sb->s_bdev, trace_curr->start+j, bs_prev);
+					}
+					blk_finish_plug(&bh_plug);
+				} else __breadahead(sb->s_bdev, trace_curr->start, bs_prev);
+				//} else __bread(sb->s_bdev, trace_curr->start, bs_prev);
+			} else {
+				// printk("1) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				if(sb) drop_super(sb);
+				sb = user_get_super(trace_curr->dev);
+
+				if(!sb) {
+					if(ino_prev) iput(ino_prev);
+					if(file_prev) put_filp(file_prev);
+					return -ENOENT;
+				}
+
+				dev_prev = trace_curr->dev;
+				bdev_prev = sb->s_bdev;
+				bs_prev = 1 << sb->s_blocksize_bits;
+
+				// printk("2) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				end = (trace_curr->length+bs_prev-1)/bs_prev;
+
+				if(end > 1) {
+					blk_start_plug(&bh_plug);
+					for(j=0; j<((trace_curr->length+bs_prev-1)/bs_prev); j++)
+					{
+						//printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start+j, bs_prev);
+						__breadahead(sb->s_bdev, trace_curr->start+j, bs_prev);
+						//__bread(sb->s_bdev, trace_curr->start+j, bs_prev);
+					}
+					blk_finish_plug(&bh_plug);
+				} else 
+					//printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, bs_prev);
+					__breadahead(sb->s_bdev, trace_curr->start, bs_prev);
+					//__bread(sb->s_bdev, trace_curr->start, bs_prev);
+			}
+		} else {
+			if(dev_prev != trace_curr->dev || !ino_prev || ino_prev->i_ino != trace_curr->ino) {
+				if(dev_prev != trace_curr->dev) {
+					if(sb) drop_super(sb);
+					sb = user_get_super(trace_curr->dev);
+
+					if(!sb) {
+						if(ino_prev) iput(ino_prev);
+						if(file_prev) put_filp(file_prev);
+						return -ENOENT;
+					}
+
+					dev_prev = trace_curr->dev;
+					bdev_prev = sb->s_bdev;
+					bs_prev = 1 << sb->s_blocksize_bits;
+				}
+
+				if(ino_prev) iput(ino_prev);
+
+				//ino_prev = ext3_iget(sb, trace_curr->ino);
+				ino_prev = ext4_iget(sb, trace_curr->ino);
+
+				if(IS_ERR_OR_NULL(ino_prev)) {
+					ino_prev = NULL;
+
+					continue;
+				}
+
+				if(file_prev) put_filp(file_prev);
+
+				file_prev = get_empty_filp();
+
+				if(file_prev == NULL) continue;
+
+				file_prev->f_op = ino_prev->i_fop;
+				file_prev->f_mapping = ino_prev->i_mapping;
+				file_prev->f_mode = FMODE_READ;
+				file_prev->f_flags = O_RDONLY;
+			}
+
+			if(ino_prev == NULL) continue;
+
+			//printk("data readahead %lu (%llu, %d);\n", trace_curr->ino, trace_curr->start, trace_curr->length);
+			
+			ret = force_page_cache_readahead(ino_prev->i_mapping, file_prev, (pgoff_t)trace_curr->start, (unsigned long)trace_curr->length, 1);
+
+			//PARALFETCH
+			flashfetch_prefetch_cnt++;
+
+			io_schedule(); //newly-added
+
+			if(ret < 0) {
+				printk("failed: ino : %lu (%llu, %d);\n", trace_curr->ino, trace_curr->start, trace_curr->length);
+			}
+		}
+	}
+
+	if(ino_prev) iput(ino_prev);
+	if(file_prev) put_filp(file_prev);
+	if(sb) drop_super(sb);
+
+	return i;
+}
diff -uNr linux-4.12.9/fs/flashfetch_scheduler.c linux-4.12.9-pf/fs/flashfetch_scheduler.c
--- linux-4.12.9/fs/flashfetch_scheduler.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_scheduler.c	2019-01-02 21:24:54.218591369 +0900
@@ -0,0 +1,1156 @@
+/*
+ * 0926: update rb_tree search algorithm
+ */
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/rbtree.h>
+#include <linux/flashfetch_scheduler.h>
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
+#include "ext4/ext4.h"
+
+//PARALFETCH
+extern struct super_block *user_get_super(dev_t dev);
+
+int blocksize_dev(dev_t dev)
+{
+	struct super_block *sb = NULL;
+	int bs;
+
+	sb = user_get_super(dev);
+
+	if(!sb) {
+		return -ENODEV;
+	}
+
+	bs = 1 << sb->s_blocksize_bits;
+
+	if(sb) drop_super(sb);
+
+	return bs;
+}
+
+struct scheduler_entry *ff_rb_search(struct rb_root *root, dev_t dev, u64 pblk_num)
+{
+	struct rb_node *node = root->rb_node;
+	struct scheduler_entry *curr;
+
+	while(node)
+	{
+		curr = rb_entry(node, struct scheduler_entry, rb_lba);
+
+		if(dev < curr->dev) {
+			node = node->rb_left;
+		} else if(dev == curr->dev) {
+			if(pblk_num < curr->pblk_num)
+				node = node->rb_left;
+			else if(pblk_num == curr->pblk_num)
+				return curr;
+			else
+				node = node->rb_right;
+		} else {
+			node = node->rb_right;
+		}
+	}
+	return NULL;
+}
+
+struct scheduler_entry *__ff_rb_insert_lba(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct rb_node **p =  &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct scheduler_entry *curr;
+	dev_t dev = new->dev;
+	u64 pblk_num = new->pblk_num;
+
+	while(*p)
+	{
+		parent = *p;
+		curr = rb_entry(parent, struct scheduler_entry, rb_lba);
+
+		if(dev < curr->dev) {
+			p = &(*p)->rb_left;
+		} else if(dev == curr->dev) {
+			if(pblk_num < curr->pblk_num)
+				p = &(*p)->rb_left;
+			else if(pblk_num == curr->pblk_num) 
+				return curr;
+			else
+				p = &(*p)->rb_right;
+		} else {
+			p = &(*p)->rb_right;
+		}
+	}
+	rb_link_node(&(new->rb_lba), parent, p);
+
+	return NULL;
+}
+
+void ff_rb_erase_lba(struct rb_node *victim, struct rb_root *tree)
+{
+	rb_erase(victim, tree);
+}
+
+void ff_rb_erase_lba_del(struct rb_node *victim, struct rb_root *tree)
+{
+	struct scheduler_entry *victim_str;
+	victim_str = rb_entry(victim, struct scheduler_entry, rb_lba);
+
+	rb_erase(victim, tree);
+
+	kfree(victim_str);
+}
+
+struct scheduler_entry *prev_entry(struct scheduler_entry *entry)
+{
+	struct rb_node *node;
+
+	node = rb_prev(&(entry->rb_lba));
+	if(node != NULL)
+		return rb_entry(node, struct scheduler_entry, rb_lba);
+	else
+		return NULL;
+}
+
+struct scheduler_entry *next_entry(struct scheduler_entry *entry)
+{
+	struct rb_node *node;
+
+	node = rb_next(&(entry->rb_lba));
+	if(node != NULL)
+		return rb_entry(node, struct scheduler_entry, rb_lba);
+	else
+		return NULL;
+}
+
+#define FF_END		1
+#define FF_FIND		2
+
+//HERE
+struct scheduler_entry *ff_rb_insert_lba(struct rb_root *root, struct scheduler_entry *new)
+{
+	u64 r_blk_end;
+	struct scheduler_entry *ret = NULL;
+	struct scheduler_entry *prev = NULL;
+	struct scheduler_entry *curr = NULL;
+	struct scheduler_entry *next = NULL;
+
+	r_blk_end = new->blk_num + new->blk_len - 1;
+
+	// the request size of buffered page cache is the size of FS block.
+	ret = __ff_rb_insert_lba(root, new);
+
+	if(new->ino == 0) {
+		if(ret != NULL) {
+			return ret;
+		} else {
+			rb_insert_color(&(new->rb_lba), root);
+			return NULL;
+		}
+	}
+
+	if(ret == NULL) { // start offset no-conflict
+		rb_insert_color(&(new->rb_lba), root);
+
+		prev = prev_entry(new);
+		next = next_entry(new);
+
+		if(!(prev && prev->dev == new->dev && prev->ino == new->ino && prev->pblk_num+prev->blk_len >= new->pblk_num))
+			prev = NULL;
+		if(!(next && next->dev == new->dev && next->ino == new->ino && next->pblk_num < new->pblk_num+new->blk_len))
+			next = NULL;
+
+		if(prev == NULL) {
+			if(next == NULL) { // case 1: no conflict at all
+				printk("NO-CONFLICT\n");
+				return NULL;
+			} else { // next != NULL
+			      	 // case 2: subset of prev node
+				printk("SUBSET of prev, new i:%lu, (%llu, %u), next i:%lu, (%llu, %u) --> new->blk_len %u\n", 
+						new->ino, new->blk_num, new->blk_len, next->ino, next->blk_num, next->blk_len,
+					(unsigned int)(next->pblk_num - new->pblk_num));
+				new->blk_len = (unsigned int)(next->pblk_num - new->pblk_num);
+				curr = next;
+				ret = NULL;
+			}
+		} else { // prev != NULL
+			if(next == NULL) {
+				prev->blk_len = (unsigned int)(new->blk_len + (unsigned int)(new->blk_num - prev->blk_num));
+				rb_erase(&(new->rb_lba), root);
+				return prev; // case 3: extension of prev node
+			} else { // next != NULL
+				prev->blk_len = (unsigned int)(next->pblk_num - prev->pblk_num);
+				curr = next;
+				rb_erase(&(new->rb_lba), root);
+				ret = prev;
+			}
+		}
+	} else {
+		curr = ret;
+	}
+	next = next_entry(curr);
+
+	while(next != NULL && curr->dev == next->dev && curr->ino == next->ino && (r_blk_end >= next->blk_num - 1))
+	{
+		printk("EXTEND2 r_blk_end: %llu, i:%lu, %u to %u\n", r_blk_end, curr->ino, curr->blk_len, (unsigned int)(next->blk_num - curr->blk_num));
+		curr->blk_len = (unsigned int)(next->pblk_num - curr->pblk_num);
+		curr = next;
+		next = next_entry(next);
+	}
+
+	if(new->dev == curr->dev && new->ino == curr->ino && curr->blk_num + curr->blk_len - 1 < r_blk_end) {
+		printk("EXTEND3 curr i:%lu, (%llu, %u), new i:%lu, (%llu, %u) --> curr->blk_len %u\n", 
+				curr->ino, curr->blk_num, curr->blk_len, new->ino, new->blk_num, new->blk_len,
+			(unsigned int)(r_blk_end - curr->blk_num + 1));
+		curr->blk_len = (unsigned int)(r_blk_end - curr->blk_num + 1);
+	}
+
+	return ret;
+}
+
+struct scheduler_entry *__ff_rb_insert_seq(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct rb_node **p =  &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct scheduler_entry *curr;
+
+	u64 ts = new->ts;
+
+	while(*p)
+	{
+		parent = *p;
+		curr = rb_entry(parent, struct scheduler_entry, rb_seq);
+
+		if(curr->ts > ts)
+			p = &(*p)->rb_left;
+		else
+			p = &(*p)->rb_right;
+	}
+
+	rb_link_node(&(new->rb_seq), parent, p);
+
+	return NULL;
+}
+
+struct scheduler_entry *ff_rb_insert_seq(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct scheduler_entry *ret;
+	if((ret = __ff_rb_insert_seq(root, new)))
+		goto out;
+	rb_insert_color(&(new->rb_seq), root);
+	return ret;
+out:
+	//printk("already exist! %d, %d\n", new->blk_len, ret->blk_len);
+
+	if(new->blk_len > ret->blk_len) 
+		ret->blk_len = new->blk_len;
+	return ret;
+}
+
+void ff_rb_erase_seq(struct rb_node *victim, struct rb_root *tree)
+{
+	rb_erase(victim, tree);
+}
+
+void ff_rb_erase_seq_del(struct rb_node *victim, struct rb_root *tree)
+{
+	struct scheduler_entry *victim_str;
+	victim_str = rb_entry(victim, struct scheduler_entry, rb_seq);
+
+	rb_erase(victim, tree);
+
+	kfree(victim_str);
+}
+
+int io_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size //TBD
+
+		if((sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) != sched_entry->pblk_num) return 0;
+	} else {
+		if((sched_prev->pblk_num + sched_prev->blk_len) != sched_entry->pblk_num) return 0;
+	}
+
+	return 1;
+}
+
+extern int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct dependency_entry **dep_ptr);
+
+int io_distance_merge_onoff = 0;
+int io_distance_allowed = 10;
+
+int iodist_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int io_distance = 0;
+	struct scheduler_entry *sched_curr;
+	struct rb_node *rb_seq_curr;
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size
+
+		if((sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) != sched_entry->pblk_num) return 0;
+	} else {
+		if((sched_prev->pblk_num + sched_prev->blk_len) != sched_entry->pblk_num) return 0;
+	}
+
+	//printk("prev %llu, %llu, %u - curr %llu, %llu, %u\n", sched_prev->ts, sched_prev->pblk_num, sched_prev->blk_len, sched_entry->ts, sched_entry->pblk_num, sched_entry->blk_len);
+
+	if(sched_prev->ts <= sched_entry->ts) {
+		rb_seq_curr = &(sched_prev->rb_seq);
+		sched_curr = sched_prev;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_entry) {
+				if(io_distance <= io_distance_allowed) {
+					//printk("< dist:%d\n", io_distance);
+					return 1;
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	} else if(sched_prev->ts > sched_entry->ts) {
+		rb_seq_curr = &(sched_entry->rb_seq);
+		sched_curr = sched_entry;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_prev) {
+				if(io_distance <= io_distance_allowed) {
+					return -1;
+					//printk("> dist:%d\n", io_distance);
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	}
+
+	return 0;
+}
+
+#define DISTANCE_UNLIMITED 10000
+int io_infill_distance_merge_onoff = 1;
+int io_infill_distance_allowed = 1;
+int io_infill_hole_allowed_blks = 8;
+
+int ioinfill_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int io_distance = 0;
+	struct scheduler_entry *sched_curr;
+	struct rb_node *rb_seq_curr;
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size
+
+		if(sched_entry->pblk_num - (sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) > io_infill_hole_allowed_blks) return 0;
+	} else {
+		if(sched_entry->pblk_num - (sched_prev->pblk_num + sched_prev->blk_len) > io_infill_hole_allowed_blks) return 0;
+	}
+
+	if(io_infill_distance_allowed == DISTANCE_UNLIMITED) {
+		if(sched_prev->ts <= sched_entry->ts) return 1;
+		else return -1;
+	}
+
+	if(sched_prev->ts <= sched_entry->ts) {
+		rb_seq_curr = &(sched_prev->rb_seq);
+		sched_curr = sched_prev;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_entry) {
+				if(io_distance <= io_infill_distance_allowed) {
+					//printk("< dist:%d\n", io_distance);
+					return 1;
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_infill_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	} else {
+		rb_seq_curr = &(sched_entry->rb_seq);
+		sched_curr = sched_entry;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_prev) {
+				if(io_distance <= io_infill_distance_allowed) {
+					return -1;
+					//printk("> dist:%d\n", io_distance);
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_infill_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	}
+
+	return 0;
+}
+
+struct shift_cq *alloc_shift_cq(int distance)
+{
+	struct shift_cq *cq;
+
+	if(distance < 0) return NULL;
+
+	cq = kmalloc(sizeof(struct shift_cq), GFP_KERNEL);
+	if(!cq) return NULL;
+
+	cq->distance = distance;
+	cq->wall_distance = 0;
+	cq->head = NULL;
+	cq->tail = NULL;
+
+	printk("init_shift_cq() distance : %d\n", distance);
+
+	return cq;
+}
+
+void inc_dist(struct shift_cq *cq, int incr)
+{
+	if(incr <= 0) return;
+	cq->wall_distance += incr;
+}
+
+void destroy_shift_cq(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	
+	if(!cq) return;
+
+	while(cq->head != NULL)
+	{
+		sft_entry = cq->head;
+		cq->head = sft_entry->next;
+		// NEW
+		//if(sft_entry->sched_entry) kfree(sft_entry->sched_entry);
+		//sft_entry->sched_entry = NULL;
+		//
+		kfree(sft_entry);
+	}
+	kfree(cq);
+}
+
+int insert_shift_cq(struct shift_cq *cq, struct scheduler_entry *sched_entry)
+{
+	struct shift_entry *sft_entry;
+
+	sft_entry = kmalloc(sizeof(struct shift_entry), GFP_KERNEL);
+	if(sft_entry == NULL) return -ENOMEM;
+
+	sft_entry->expire = cq->wall_distance + cq->distance;
+	sft_entry->sched_entry = sched_entry;
+	sft_entry->next = NULL;
+
+	if(cq->head == NULL) {
+		cq->head = sft_entry;
+		cq->tail = sft_entry;
+	} else {
+		cq->tail->next = sft_entry;
+		cq->tail = sft_entry;
+	}
+	return 0;
+}
+
+struct scheduler_entry *get_expired_entry(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	struct scheduler_entry *sched_entry;
+	
+	/*
+	if(cq->head != NULL) printk("cq:0x%p, head:0x%p, expire:%d, wall_dist:%d\n", cq, cq->head, cq->head->expire, cq->wall_distance);
+	else printk("cq:0x%p, head:NULL, wall_dist:%d\n", cq, cq->wall_distance);
+	*/
+	if(cq == NULL) return NULL;
+
+	if(cq->head == NULL) return NULL;
+	else {
+		if(cq->head->expire <= cq->wall_distance) {
+			sft_entry = cq->head;
+			cq->head = sft_entry->next;
+			sched_entry = sft_entry->sched_entry;
+			kfree(sft_entry);
+			return sched_entry;
+		}
+	}
+	return NULL;
+}
+
+struct scheduler_entry *get_cq_entry(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	
+	if(cq->head == NULL) return NULL;
+	else {
+		sft_entry = cq->head;
+		cq->head = sft_entry->next;
+		return sft_entry->sched_entry;
+	}
+}
+
+#define IOCOUNT		0
+#define IOSIZE		1
+int metadata_shift_onoff = 1;
+int metadata_shift_way = IOCOUNT;
+int metadata_shift_value = 8;
+
+//PARALFETCH
+int rotational_value = 0;
+
+int write_disk_trace_sched_flash(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count)
+{
+	//struct disk_log_entry *disk_log;
+	struct rb_root rb_tree_lba;
+	struct rb_root rb_tree_seq;
+	struct scheduler_entry *sched_entry;
+	struct scheduler_entry *sched_prev;
+	struct rb_node *rb_node;
+	struct rb_node *rb_prev;
+	struct scheduler_entry *ret;
+	struct dependency_entry *head;
+	struct dependency_entry *prev;
+	struct inode *inode;
+	struct super_block *sb;
+	int i,idx;
+	replay_info *trace_buf;
+	int node_count = 0;
+	int merge_count = 0;
+	int miss_count = 0;
+	int blocksize;
+        unsigned long long nstime;
+
+	struct shift_cq *cq;
+
+	rb_tree_lba.rb_node = NULL;
+	rb_tree_seq.rb_node = NULL;
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED start: %llu\n", nstime);
+
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino == 0) {
+			sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+			if(sched_entry) {
+				sched_entry->dev = logs[i].dev;
+				sched_entry->pblk_num = logs[i].blk_num;
+				sched_entry->ts = logs[i].ts;
+				//sched_entry->type = logs[i].type;
+				sched_entry->ino = logs[i].ino;
+				sched_entry->blk_num = logs[i].blk_num;
+				sched_entry->blk_len = logs[i].blk_len;
+				ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+				if(!ret) {
+					ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+					node_count++;
+				} else {
+					//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+					kfree(sched_entry);
+				}
+			}
+			//printk("dev:0x%x, blk_num:%llu, blk_len:%u\n", logs[i].dev, logs[i].blk_num, logs[i].blk_len);
+		} else {
+			sb = user_get_super(logs[i].dev);
+			if(!sb) continue;
+
+			inode = ext4_iget(sb, logs[i].ino);
+			if(IS_ERR_OR_NULL(inode)) {
+				drop_super(sb);
+				continue;
+			}
+
+			head = NULL;
+
+			ext4_ext_walk_range(inode, logs[i].blk_num, logs[i].blk_len, &head);
+
+			if(head) {
+				prev = head;
+				while(prev) {
+					sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+					if(sched_entry) {
+						sched_entry->dev = logs[i].dev;
+						sched_entry->pblk_num = prev->pblk_num;
+						sched_entry->ts = logs[i].ts;
+						//sched_entry->type = logs[i].type;
+						if(prev->type == TYPE_META) sched_entry->ino = 0;
+						else sched_entry->ino = logs[i].ino;
+						sched_entry->blk_num = prev->blk_num;
+						sched_entry->blk_len = prev->blk_len;
+						ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+						if(!ret) {
+							ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+							node_count++;
+							if(prev->type == TYPE_META) miss_count++;
+						} else {
+							//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+							kfree(sched_entry);
+						}
+					}
+					//printk("type:%u pblk_num:%llu blk_num:%llu blk_len:%u\n", prev->type, prev->pblk_num, prev->blk_num, prev->blk_len);
+					head = prev;
+					prev = prev->next;
+					kfree(head);
+				}
+			}
+			iput(inode);
+			drop_super(sb);
+		}
+	}
+	printk("missed_count : %d\n", miss_count);
+
+	if(io_distance_merge_onoff == 0) goto out_iodist_merge;
+
+	printk("********************** IODISTANCE MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = iodist_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			if(idx > 0) {
+				sched_prev->blk_len += sched_entry->blk_len;
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				//BUG FIXED
+				sched_entry->blk_num = sched_prev->blk_num;
+
+				sched_entry->blk_len += sched_prev->blk_len;
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, iodist_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+out_iodist_merge:
+
+	if(io_infill_distance_merge_onoff == 0) goto out_io_infill_distance_merge;
+
+	merge_count = 0;
+	printk("********************** IOINFILL MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = ioinfill_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			blocksize = blocksize_dev(sched_prev->dev);
+			if(idx > 0) {
+				if(sched_prev->ino == 0) {
+					sched_prev->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_prev->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				if(sched_entry->ino == 0) {
+					sched_entry->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_entry->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+				
+				sched_entry->blk_num = sched_prev->blk_num;
+				sched_entry->pblk_num = sched_prev->pblk_num;
+
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, ioinfill_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+out_io_infill_distance_merge:
+
+	for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+	}
+	rb_tree_lba.rb_node = NULL;
+
+	cq = alloc_shift_cq(metadata_shift_value);
+
+	if(cq == NULL) {
+		for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+		{
+			if(rb_node == NULL) return 0;
+
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+		}
+		return 0;
+	}
+
+	trace_buf = kmalloc(sizeof(replay_info) * (node_count+100), GFP_KERNEL);
+	idx = 0;
+
+	for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+		if(sched_entry->ino == 0) {
+//			printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+			if(trace_buf) {
+				trace_buf[idx].dev = sched_entry->dev;
+				trace_buf[idx].ino = sched_entry->ino;
+				trace_buf[idx].start = sched_entry->blk_num;
+				trace_buf[idx++].length = sched_entry->blk_len;
+			}
+
+			if(metadata_shift_way == IOSIZE) inc_dist(cq, (sched_entry->blk_len / 4096 /*blocksize*/));
+			else inc_dist(cq, 1); //else if(metadata_shift_way == IOCOUNT) inc_dist(cq, 1);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+
+			sched_prev = get_expired_entry(cq);
+
+			while(sched_prev != NULL)
+			{
+				if(sched_prev != NULL) {
+					//	printk("E: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+					if(trace_buf) {
+						trace_buf[idx].dev = sched_prev->dev;
+						trace_buf[idx].ino = sched_prev->ino;
+						trace_buf[idx].start = sched_prev->blk_num;
+						trace_buf[idx++].length = sched_prev->blk_len;
+					}
+					ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+					sched_prev = get_expired_entry(cq);
+				} else break;
+			}
+
+			sched_prev = NULL;
+
+		} else {
+			//PARALFETCH2
+			//printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+			//inc_dist(cq, 1);
+			insert_shift_cq(cq, sched_entry);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+
+			sched_prev = get_expired_entry(cq);
+			while(sched_prev != NULL)
+			{
+				if(sched_prev != NULL) {
+					//printk("E: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+					if(trace_buf) {
+						trace_buf[idx].dev = sched_prev->dev;
+						trace_buf[idx].ino = sched_prev->ino;
+						trace_buf[idx].start = sched_prev->blk_num;
+						trace_buf[idx++].length = sched_prev->blk_len;
+					}
+					ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+					sched_prev = get_expired_entry(cq);
+				} else break;
+			}
+		}
+	}
+
+	sched_prev = get_cq_entry(cq);
+	while(sched_prev != NULL)
+	{
+		if(sched_prev != NULL) {
+			//printk("X: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+			if(trace_buf) {
+				trace_buf[idx].dev = sched_prev->dev;
+				trace_buf[idx].ino = sched_prev->ino;
+				trace_buf[idx].start = sched_prev->blk_num;
+				trace_buf[idx++].length = sched_prev->blk_len;
+			}
+			ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+			sched_prev = get_cq_entry(cq);
+		} else break;
+	}
+
+	if(trace_buf) {
+		printk("node_count:%d, idx:%d\n", node_count, idx);
+		*trace_buffer = trace_buf;
+		*trace_count = idx;
+	} else {
+		*trace_buffer = NULL;
+		*trace_count = 0;
+	}
+
+
+	rb_tree_seq.rb_node = NULL;
+
+	if(cq) destroy_shift_cq(cq);
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED end: %llu\n", nstime);
+
+	return 0;
+
+	/*
+	   printk("**********************  LBA ***********************\n");
+	   for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	   {
+	   if(!rb_node) break;
+
+	   sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+	   printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+	   rb_node = rb_next(rb_node);
+	   }
+
+*/
+	/*
+	   printk("**********************  SEQ ***********************\n");
+	   for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	   {
+	   if(!rb_node) break;
+
+	   sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+	   printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+
+	   rb_prev = rb_node;
+	   rb_node = rb_next(rb_node);
+	   ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+	   }
+	return 0;
+	   */
+}
+
+int write_disk_trace_sched_hard(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count)
+{
+	struct rb_root rb_tree_lba;
+	struct rb_root rb_tree_seq;
+	struct scheduler_entry *sched_entry;
+	struct scheduler_entry *sched_prev;
+	struct rb_node *rb_node;
+	struct rb_node *rb_prev;
+	struct scheduler_entry *ret;
+	struct dependency_entry *head;
+	struct dependency_entry *prev;
+	struct inode *inode;
+	struct super_block *sb;
+	int i,idx;
+	replay_info *trace_buf;
+	int node_count = 0;
+	int merge_count = 0;
+	int miss_count = 0;
+	int blocksize;
+        unsigned long long nstime;
+
+	rb_tree_lba.rb_node = NULL;
+	rb_tree_seq.rb_node = NULL;
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED start: %llu\n", nstime);
+
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino == 0) {
+			sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+			if(sched_entry) {
+				sched_entry->dev = logs[i].dev;
+				sched_entry->pblk_num = logs[i].blk_num;
+				sched_entry->ts = logs[i].ts;
+				//sched_entry->type = logs[i].type;
+				sched_entry->ino = logs[i].ino;
+				sched_entry->blk_num = logs[i].blk_num;
+				sched_entry->blk_len = logs[i].blk_len;
+				ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+				if(!ret) {
+					ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+					node_count++;
+				} else {
+					//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+					kfree(sched_entry);
+				}
+			}
+			//printk("dev:0x%x, blk_num:%llu, blk_len:%u\n", logs[i].dev, logs[i].blk_num, logs[i].blk_len);
+		}
+	}
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino != 0) {
+			sb = user_get_super(logs[i].dev);
+			if(!sb) continue;
+
+			inode = ext4_iget(sb, logs[i].ino);
+			if(IS_ERR_OR_NULL(inode)) {
+				drop_super(sb);
+				continue;
+			}
+
+			ext4_ext_walk_range(inode, logs[i].blk_num, logs[i].blk_len, &head);
+
+			if(head) {
+				prev = head;
+				while(prev) {
+					sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+					if(sched_entry) {
+						sched_entry->dev = logs[i].dev;
+						sched_entry->pblk_num = prev->pblk_num;
+						sched_entry->ts = logs[i].ts;
+						//sched_entry->type = logs[i].type;
+						sched_entry->blk_num = prev->blk_num;
+						sched_entry->blk_len = prev->blk_len;
+						if(prev->type == TYPE_META) sched_entry->ino = 0;
+						else {
+							sched_entry->ino = logs[i].ino;
+							//printk("ts:%llu ino:%lu dev:0x%x blk_num:%llu blk_len:%u\n", sched_entry->ts, sched_entry->ino, sched_entry->dev, sched_entry->blk_num, sched_entry->blk_len);
+						}
+						ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+						if(!ret) {
+							ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+							node_count++;
+							if(prev->type == TYPE_META) miss_count++;
+						} else {
+							//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+							kfree(sched_entry);
+						}
+					}
+					//printk("pblk_num:%llu type:%u dev:0x%x inode:%lu blk_num:%llu blk_len:%u\n", prev->pblk_num, prev->type, logs[i].dev, logs[i].ino, prev->blk_num, prev->blk_len);
+					head = prev;
+					prev = prev->next;
+					kfree(head);
+				}
+			}
+			iput(inode);
+			drop_super(sb);
+		}
+	}
+	printk("missed_count : %d\n", miss_count);
+
+/*
+	printk("**********************  START: WRITE DETAILED LAUNCH SEQUENCE ***********************\n");
+
+	rb_node = rb_first(&rb_tree_seq);
+	while(rb_node)
+	{
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+		printk("%llu %x %lu %llu %u %llu\n", sched_entry->ts, sched_entry->dev, sched_entry->ino, sched_entry->blk_num, sched_entry->blk_len, sched_entry->pblk_num);
+		rb_node = rb_next(rb_node);
+	}
+	printk("**********************  END: WRITE DETAILED LAUNCH SEQUENCE ***********************\n");
+*/
+
+	printk("**********************  MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		if(io_mergeable(sched_prev, sched_entry)) {
+			sched_prev->blk_len += sched_entry->blk_len;
+			merge_count++;
+			ff_rb_erase_lba(rb_node, &rb_tree_lba);
+			ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+			rb_node = rb_next(rb_prev);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+	merge_count = 0;
+	printk("********************** IOINFILL MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = ioinfill_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			blocksize = blocksize_dev(sched_prev->dev);
+			if(idx > 0) {
+				if(sched_prev->ino == 0) {
+					sched_prev->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_prev->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				if(sched_entry->ino == 0) {
+					sched_entry->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_entry->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				sched_entry->blk_num = sched_prev->blk_num;
+				sched_entry->pblk_num = sched_prev->pblk_num;
+
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, ioinfill_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+	for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	{
+		if(rb_node == NULL) return 0;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_seq(rb_prev, &rb_tree_seq);
+	}
+	rb_tree_seq.rb_node = NULL;
+
+	trace_buf = kmalloc(sizeof(replay_info) * (node_count+100), GFP_KERNEL);
+	idx = 0;
+
+	//printk("**********************  LBA ***********************\n");
+	for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		//	printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+		if(trace_buf) {
+			trace_buf[idx].dev = sched_entry->dev;
+			trace_buf[idx].ino = sched_entry->ino;
+			trace_buf[idx].start = sched_entry->blk_num;
+			trace_buf[idx++].length = sched_entry->blk_len;
+		}
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_lba_del(rb_prev, &rb_tree_lba);
+	}
+
+	rb_tree_lba.rb_node = NULL;
+
+	if(trace_buf) {
+		printk("node_count:%d, idx:%d\n", node_count, idx);
+		*trace_buffer = trace_buf;
+		*trace_count = idx;
+	} else {
+		*trace_buffer = NULL;
+		*trace_count = 0;
+	}
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED end: %llu\n", nstime);
+
+	return 0;
+}
diff -uNr linux-4.12.9/fs/flashfetch_trace.c linux-4.12.9-pf/fs/flashfetch_trace.c
--- linux-4.12.9/fs/flashfetch_trace.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_trace.c	2019-01-02 21:24:54.172591108 +0900
@@ -0,0 +1,57 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/flashfetch_trace.h>
+
+int alloc_disk_trace(struct disk_trace **disktrc)
+{
+	*disktrc = kzalloc(ALIGN(sizeof(struct disk_trace), cache_line_size()), GFP_KERNEL);
+
+	if(!(*disktrc)) {
+		printk("alloc_disk_trace failed\n");
+		return -ENOMEM;
+	} else {
+		(*disktrc)->lock = __SPIN_LOCK_UNLOCKED(disktrc->lock); 
+		(*disktrc)->pos = 0;
+	}
+
+	return 0;
+}
+
+void release_disk_trace(struct disk_trace *disktrc)
+{
+	kfree(disktrc);
+}
+
+void reset_disk_trace(struct disk_trace *disktrc)
+{
+	disktrc->lock = __SPIN_LOCK_UNLOCKED(disktrc->lock); 
+	disktrc->pos = 0;
+}
+
+int get_log_entry(struct disk_trace *disktrc)
+{
+	int curr_pos;
+	int ret = -ENOENT;
+
+	spin_lock(&(disktrc->lock));
+
+	curr_pos = disktrc->pos;
+	if(curr_pos < MAX_LOG_ENT) {
+		ret = curr_pos;
+		disktrc->pos = curr_pos + 1;
+	}
+
+	spin_unlock(&(disktrc->lock));
+	
+	return ret;
+}
diff -uNr linux-4.12.9/fs/Kconfig linux-4.12.9-pf/fs/Kconfig
--- linux-4.12.9/fs/Kconfig	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/Kconfig	2019-01-02 21:24:54.300591834 +0900
@@ -10,6 +10,13 @@
 
 if BLOCK
 
+config PARALFETCH
+	bool "Paralfetch for quick application launches"
+	depends on EXT4_FS
+	default n
+	help
+	  An execution-time prefetcher for quick application launches.
+
 config FS_IOMAP
 	bool
 
diff -uNr linux-4.12.9/fs/Makefile linux-4.12.9-pf/fs/Makefile
--- linux-4.12.9/fs/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/Makefile	2019-01-02 21:24:54.301591840 +0900
@@ -11,7 +11,10 @@
 		attr.o bad_inode.o file.o filesystems.o namespace.o \
 		seq_file.o xattr.o libfs.o fs-writeback.o \
 		pnode.o splice.o sync.o utimes.o \
-		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o
+		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o 
+
+obj-$(CONFIG_PARALFETCH)	+= flashfetch_core.o flashfetch_trace.o \
+		flashfetch_fetch.o flashfetch_scheduler.o flashfetch_evaluate.o
 
 ifeq ($(CONFIG_BLOCK),y)
 obj-y +=	buffer.o block_dev.o direct-io.o mpage.o
diff -uNr linux-4.12.9/fs/namespace.c linux-4.12.9-pf/fs/namespace.c
--- linux-4.12.9/fs/namespace.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/namespace.c	2019-01-02 21:24:54.312591902 +0900
@@ -3025,6 +3025,18 @@
 }
 EXPORT_SYMBOL(mount_subtree);
 
+//PARALFETCH
+extern unsigned int bootfetch;
+#define MAXDEVNAME 19
+extern char bootdev[MAXDEVNAME+1];
+extern void do_bootfetch(int bootfetch);
+int boot_in_progress = 0;
+/*
+extern struct file *kernel_open(char const *file_name, int flags, int mode);
+extern int kernel_close(struct file *file);
+struct file *pf_file;
+*/
+
 SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,
 		char __user *, type, unsigned long, flags, void __user *, data)
 {
@@ -3050,12 +3062,41 @@
 
 	ret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);
 
+	//PARALFETCH
+        //printk("MOUNT: kernel_dev:%s, kernel_dir:%s\n", kernel_dev, kernel_dir);
+	
+	if(!ret && bootfetch && !boot_in_progress) {
+                if(strcmp(kernel_dev, "securityfs") == 0 /*&& strcmp(kernel_dir, "/sys/kernel/security") == 0*/) {
+                        printk("BOOTFETCH runs\n");
+                        boot_in_progress = 1;
+                        do_bootfetch(bootfetch);
+                }
+        }
+        /*
+        if(!ret && bootfetch && !boot_in_progress) {
+                if(strncmp(kernel_dev, bootdev, MAXDEVNAME) == 0) {
+                        printk("MOUNT: kernel_dev:%s, kernel_dir:%s\n", kernel_dev, kernel_dir);
+                        //boot_in_progress = 1;
+                        //do_bootfetch(bootfetch);
+                }
+        }
+        */
+
 	kfree(options);
 out_data:
 	kfree(kernel_dev);
 out_dev:
 	kfree(kernel_type);
 out_type:
+	//PARALFETCH
+        /*
+        pf_file = kernel_open("/flashfetch/kernel_boot.pf", O_RDONLY, 0644);
+        if(IS_ERR(pf_file)) printk("MOUNT: kernel_open error = %ld\n", PTR_ERR(pf_file));
+        else {
+                printk("MOUNT: kernel_open succeed!\n");
+                kernel_close(pf_file);
+        }
+        */
 	return ret;
 }
 
diff -uNr linux-4.12.9/fs/read_write.c linux-4.12.9-pf/fs/read_write.c
--- linux-4.12.9/fs/read_write.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/read_write.c	2019-01-02 21:24:54.311591897 +0900
@@ -443,9 +443,23 @@
 	return ret;
 }
 
+//PARALFETCH2
+/*
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+*/
+
 ssize_t __vfs_read(struct file *file, char __user *buf, size_t count,
 		   loff_t *pos)
 {
+	//PARALFETCH2
+	/*
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                printk("PARALFETCH: __vfs_read pos:%lld count:%lu\n",
+                                *pos, count);
+        }
+	*/
+
 	if (file->f_op->read)
 		return file->f_op->read(file, buf, count, pos);
 	else if (file->f_op->read_iter)
diff -uNr linux-4.12.9/fs/sync.c linux-4.12.9-pf/fs/sync.c
--- linux-4.12.9/fs/sync.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/sync.c	2019-01-02 21:24:54.303591851 +0900
@@ -94,6 +94,21 @@
 	filemap_fdatawait_keep_errors(bdev->bd_inode->i_mapping);
 }
 
+//PARALFETCH
+void sync_filesystems_ff(int dummy)
+{
+        int nowait = 0, wait = 1;
+
+        wakeup_flusher_threads(0, WB_REASON_SYNC);
+        iterate_supers(sync_inodes_one_sb, NULL);
+        iterate_supers(sync_fs_one_sb, &nowait);
+        iterate_supers(sync_fs_one_sb, &wait);
+        iterate_bdevs(fdatawrite_one_bdev, NULL);
+        iterate_bdevs(fdatawait_one_bdev, NULL);
+        if (unlikely(laptop_mode))
+                laptop_sync_completion();
+}
+
 /*
  * Sync everything. We start by waking flusher threads so that most of
  * writeback runs on all devices in parallel. Then we sync all inodes reliably
diff -uNr linux-4.12.9/include/linux/flashfetch_fetch.h linux-4.12.9-pf/include/linux/flashfetch_fetch.h
--- linux-4.12.9/include/linux/flashfetch_fetch.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_fetch.h	2019-01-02 21:24:58.231614116 +0900
@@ -0,0 +1,41 @@
+#include <linux/types.h>
+
+/*
+ * METADATA (ino == 0)
+ * dev = device number
+ * ino = 0
+ * start = logical block number (block offset in the partition)
+ * length = the number of bytes to read
+ *
+ * DATA (ino != 0)
+ * dev = device number
+ * ino = inode number
+ * start = file offset in page size unit (PAGE_CACHE_SIZE = PAGE_SIZE)
+ * length = the number of pages to read
+ */
+
+#ifndef PARALFETCH_FETCH_H
+#define PARALFETCH_FETCH_H
+
+#define MAGIC_LENGTH	8
+#define NAME_LEN		255
+
+typedef struct __replay_info {
+	dev_t dev;
+	ino_t ino;
+	u64 start;
+	u32 length;
+} __attribute__((packed)) replay_info;
+
+typedef struct __paralfetch_header {
+	char magic[MAGIC_LENGTH];
+	u32 version;
+	u16 io_count;
+	u16 private;
+	u16 uncertain_io_count;
+	u16 uncertain_io_size;
+	u16 harmful_io_count;
+	u16 harmful_io_size;
+	replay_info trace_data[0];
+} paralfetch_header;
+#endif
diff -uNr linux-4.12.9/include/linux/flashfetch.h linux-4.12.9-pf/include/linux/flashfetch.h
--- linux-4.12.9/include/linux/flashfetch.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch.h	2019-01-02 21:24:58.203613957 +0900
@@ -0,0 +1,50 @@
+#ifndef PARALFETCH_H
+#define PARALFETCH
+
+//PARALFETCH
+#define FALSE				0
+#define TRUE				1
+
+//FETCHTYPE
+#define PARALFETCH
+//#define HARDFETCH
+
+#ifdef HARDFETCH
+#define FFTIMEOUT 30
+#else
+#define FFTIMEOUT 10
+#endif
+
+// prefetch type
+#define NO_PREFETCH			0
+#define ASYNC_PREFETCH		1
+#define SYNC_PREFETCH		2
+#define GEN_PREFETCH		3
+#define SEQ_REFINE1			4
+#define SEQ_REFINE2			5
+
+// name length limit
+#define NAME_LEN 			255
+
+// flashfetch flags
+#define FF_INPROGRESS			1
+
+// cache management type
+#define FT_BUFFER_CACHE			1
+#define FT_PAGE_CACHE			2
+
+struct seqmon_param {
+	char appname[NAME_LEN];
+	char filename[NAME_LEN];
+};
+
+extern bool enable_flashfetch;
+
+int flashfetch_async(char *appname);
+int flashfetch_sync(char *appname);
+int flashfetch_genpf(char *appname, const char *filename);
+
+#endif
+
+int flashfetch_prefetch_sync(char *app_name);
+int flashfetch_prefetch_async(char *app_name);
diff -uNr linux-4.12.9/include/linux/flashfetch_scheduler.h linux-4.12.9-pf/include/linux/flashfetch_scheduler.h
--- linux-4.12.9/include/linux/flashfetch_scheduler.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_scheduler.h	2019-01-02 21:24:58.263614297 +0900
@@ -0,0 +1,45 @@
+#ifndef PARALFETCH_SCHEDULER_H
+#define PARALFETCH_SCHEDULER_H
+
+#include <linux/types.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/rbtree.h>
+
+struct scheduler_entry {
+	struct rb_node rb_seq;
+	struct rb_node rb_lba;
+	dev_t dev;
+	u64 pblk_num;
+	u64 ts;
+//	u32 type;
+	ino_t ino;
+	u64 blk_num;
+	u32 blk_len;
+};
+
+#define TYPE_META		0
+#define TYPE_REGFILE	1
+
+struct dependency_entry {
+	u64 pblk_num;
+	u32 type;
+	u64 blk_num;
+	u32 blk_len;
+	struct dependency_entry *next;
+};
+
+struct shift_entry {
+	unsigned int expire;
+	struct scheduler_entry *sched_entry;
+	struct shift_entry *next;
+};
+
+struct shift_cq {
+	int distance;
+	int wall_distance;
+	struct shift_entry *head;
+	struct shift_entry *tail;
+};
+
+#endif
diff -uNr linux-4.12.9/include/linux/flashfetch_trace.h linux-4.12.9-pf/include/linux/flashfetch_trace.h
--- linux-4.12.9/include/linux/flashfetch_trace.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_trace.h	2019-01-02 21:24:58.237614150 +0900
@@ -0,0 +1,39 @@
+#ifndef PARALFETCH_TRACE_H
+#define PARALFETCH_TRACE_H
+
+#include <linux/types.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+
+#include <linux/flashfetch_fetch.h>
+
+#define MAX_LOG_ENT 16384
+
+struct disk_log_entry {
+	u64 ts;
+//	u32 type;
+	dev_t dev;
+	ino_t ino;
+	u64 blk_num;
+	u32 blk_len;
+};
+
+struct disk_trace {
+	struct disk_log_entry log[MAX_LOG_ENT];
+	int pos;
+	spinlock_t lock;
+};
+
+extern int alloc_disk_trace(struct disk_trace **disktrc);
+extern void release_disk_trace(struct disk_trace *disktrc);
+extern void reset_disk_trace(struct disk_trace *disktrc);
+extern int get_log_entry(struct disk_trace *disktrc);
+extern int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count);
+extern int write_disk_replayinfo_raw(char *appname, replay_info *trace_buffer, int trace_count);
+extern int write_disk_trace_sched_flash(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count);
+extern int write_disk_trace_sched_hard(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count);
+
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
+#endif
diff -uNr linux-4.12.9/include/linux/mm.h linux-4.12.9-pf/include/linux/mm.h
--- linux-4.12.9/include/linux/mm.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/linux/mm.h	2019-01-02 21:24:58.338614723 +0900
@@ -2178,8 +2178,9 @@
 #define VM_MAX_READAHEAD	128	/* kbytes */
 #define VM_MIN_READAHEAD	16	/* kbytes (includes current page) */
 
+//PARALFETCH
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
-			pgoff_t offset, unsigned long nr_to_read);
+			pgoff_t offset, unsigned long nr_to_read, int prefetch);
 
 void page_cache_sync_readahead(struct address_space *mapping,
 			       struct file_ra_state *ra,
diff -uNr linux-4.12.9/include/linux/page-flags.h linux-4.12.9-pf/include/linux/page-flags.h
--- linux-4.12.9/include/linux/page-flags.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/linux/page-flags.h	2019-01-02 21:24:58.289614445 +0900
@@ -105,6 +105,9 @@
 	PG_young,
 	PG_idle,
 #endif
+#ifdef CONFIG_PARALFETCH
+	PG_prefetch,
+#endif
 	__NR_PAGEFLAGS,
 
 	/* Filesystems */
@@ -313,6 +316,11 @@
 PAGEFLAG(Readahead, reclaim, PF_NO_COMPOUND)
 	TESTCLEARFLAG(Readahead, reclaim, PF_NO_COMPOUND)
 
+#ifdef CONFIG_PARALFETCH
+PAGEFLAG(Prefetch, prefetch, PF_NO_COMPOUND)
+	TESTCLEARFLAG(Prefetch, prefetch, PF_NO_COMPOUND)
+#endif
+
 #ifdef CONFIG_HIGHMEM
 /*
  * Must use a macro here due to header dependency issues. page_zone() is not
diff -uNr linux-4.12.9/include/trace/events/mmflags.h linux-4.12.9-pf/include/trace/events/mmflags.h
--- linux-4.12.9/include/trace/events/mmflags.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/trace/events/mmflags.h	2019-01-02 21:24:58.620616321 +0900
@@ -81,6 +81,12 @@
 #define IF_HAVE_PG_IDLE(flag,string)
 #endif
 
+#ifdef CONFIG_PARALFETCH
+#define IF_HAVE_PG_PREFETCH(flag,string) ,{1UL << flag, string}
+#else
+#define IF_HAVE_PG_PREFETCH(flag,string)
+#endif
+
 #define __def_pageflag_names						\
 	{1UL << PG_locked,		"locked"	},		\
 	{1UL << PG_waiters,		"waiters"	},		\
@@ -106,7 +112,9 @@
 IF_HAVE_PG_UNCACHED(PG_uncached,	"uncached"	)		\
 IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
 IF_HAVE_PG_IDLE(PG_young,		"young"		)		\
-IF_HAVE_PG_IDLE(PG_idle,		"idle"		)
+IF_HAVE_PG_IDLE(PG_idle,		"idle"		)		\
+IF_HAVE_PG_PREFETCH(PG_prefetch,	"prefetch"	)		
+// ^PARALFETCH
 
 #define show_page_flags(flags)						\
 	(flags) ? __print_flags(flags, "|",				\
diff -uNr linux-4.12.9/init/main.c linux-4.12.9-pf/init/main.c
--- linux-4.12.9/init/main.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/init/main.c	2019-01-02 21:24:58.719616882 +0900
@@ -162,6 +162,27 @@
 
 __setup("reset_devices", set_reset_devices);
 
+//PARALFETCH
+unsigned int bootfetch = 0;
+EXPORT_SYMBOL(bootfetch);
+#define MAXDEVNAME 19
+char bootdev[MAXDEVNAME+1];
+EXPORT_SYMBOL(bootdev);
+
+static int __init set_bootfetch(char *str)
+{
+        get_option(&str, &bootfetch);
+        return 1;
+}
+__setup("bootfetch=", set_bootfetch);
+
+static int __init set_bootdev(char *str)
+{
+        strncpy(bootdev, str, MAXDEVNAME);
+        return 1;
+}
+__setup("bootdev=", set_bootdev);
+
 static const char *argv_init[MAX_INIT_ARGS+2] = { "init", NULL, };
 const char *envp_init[MAX_INIT_ENVS+2] = { "HOME=/", "TERM=linux", NULL, };
 static const char *panic_later, *panic_param;
@@ -485,6 +506,28 @@
 	ioremap_huge_init();
 }
 
+//PARALFETCH
+extern int flashfetch_sync(char *app_name);
+extern int flashfetch_async(char *app_name);
+extern int flashfetch_genpf(char *appname, const char *filename);
+extern unsigned long flashfetch_monitor_timeout;
+
+void do_bootfetch(int bootfetch)
+{
+        if(!bootfetch) return;
+        else if(bootfetch==1) {
+                printk("BOOTFETCH:flashfetch_async()\n");
+                flashfetch_async("kernel_boot");
+        } else if(bootfetch==2) {
+                printk("BOOTFETCH:flashfetch_sync()\n");
+                flashfetch_sync("kernel_boot");
+        } else if(bootfetch==3) {
+                printk("BOOTFETCH:flashfetch_getpf()\n");
+                flashfetch_monitor_timeout = 30;
+                flashfetch_genpf("kernel_boot", "kernel");
+        } else return;
+}
+
 asmlinkage __visible void __init start_kernel(void)
 {
 	char *command_line;
@@ -970,6 +1013,9 @@
 
 	rcu_end_inkernel_boot();
 
+	//PARALFETCH
+        //do_bootfetch(bootfetch);
+
 	if (ramdisk_execute_command) {
 		ret = run_init_process(ramdisk_execute_command);
 		if (!ret)
diff -uNr linux-4.12.9/kernel/resource.c linux-4.12.9-pf/kernel/resource.c
--- linux-4.12.9/kernel/resource.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/kernel/resource.c	2019-01-02 21:24:53.916589657 +0900
@@ -351,6 +351,71 @@
 
 EXPORT_SYMBOL(release_resource);
 
+//PARALFETCH                                                                   
+static int find_next_iomem_res2(struct resource *res, unsigned long desc,        
+                               bool first_level_children_only)
+{                                                                              
+	resource_size_t start;                                                  
+	struct resource *p;                                                     
+	bool sibling_only = false;                                              
+
+	BUG_ON(!res);                                                           
+
+	start = res->start;                                                     
+
+	if (first_level_children_only)                                          
+		sibling_only = true;                                            
+
+	read_lock(&resource_lock);                                              
+
+	for (p = iomem_resource.child; p; p = next_resource(p, sibling_only)) { 
+		if ((p->flags & res->flags) != res->flags)
+			continue;
+		if ((desc != IORES_DESC_NONE) && (desc != p->desc))
+			continue;
+		if ((p->start >= start))                                        
+			break;                                                  
+	}                                                                       
+
+	read_unlock(&resource_lock);                                            
+	if (!p)                                                                 
+		return -1;                                                      
+	// copy data                                                      
+	if (res->start < p->start)                                              
+		res->start = p->start;
+
+	res->end = p->end;
+
+	return 0;
+}
+
+//PARALFETCH
+resource_size_t find_last_iomem_res(char *name,
+                      bool first_level_children_only)
+{
+       struct resource *p;
+       resource_size_t max_phys_addr = 0;
+       bool sibling_only = false;
+
+       if (first_level_children_only)
+               sibling_only = true;
+
+       read_lock(&resource_lock);
+
+       for (p = iomem_resource.child; p; p = next_resource(p, sibling_only)) {
+               if (!(p->flags & IORESOURCE_MEM))
+                       continue;
+               if (name && strcmp(p->name, name))
+                       continue;
+               if (p->end > max_phys_addr) {
+                       max_phys_addr = p->end;
+               }
+       }
+
+       read_unlock(&resource_lock);
+       return max_phys_addr;
+}
+
 /*
  * Finds the lowest iomem resource existing within [res->start.res->end).
  * The caller must specify res->start, res->end, res->flags, and optionally
@@ -468,6 +533,26 @@
 	}
 	return ret;
 }
+
+//PARALFETCH
+int walk_system_ram_res_all(void *arg,
+                               int (*func)(u64, u64, void *))
+{
+	struct resource res;
+	int ret = -1;
+
+	res.start = 0;
+	res.end = 0;
+	res.flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
+
+	while (!find_next_iomem_res2(&res, IORES_DESC_NONE, true)) {
+		ret = (*func)(res.start, res.end, arg);
+		if (ret)
+			break;
+		res.start = res.end + 1;
+	}
+	return ret;
+}
 
 #if !defined(CONFIG_ARCH_HAS_WALK_MEMORY)
 
diff -uNr linux-4.12.9/kernel/sched/cputime.c linux-4.12.9-pf/kernel/sched/cputime.c
--- linux-4.12.9/kernel/sched/cputime.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/kernel/sched/cputime.c	2019-01-02 21:24:54.072590542 +0900
@@ -7,6 +7,10 @@
 #include <linux/sched/cputime.h>
 #include "sched.h"
 
+//PARALFETCH
+#include <linux/percpu.h>
+#include <linux/blkdev.h>
+
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 
 /*
@@ -337,6 +341,16 @@
 }
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
+//PARALFETCH
+DEFINE_PER_CPU(u64, pf_user_time);
+DEFINE_PER_CPU(u64, pf_system_time);
+DEFINE_PER_CPU(u64, pf_guest_time);
+DEFINE_PER_CPU(u64, pf_idle_time);
+u64 pf_cnt = 0;
+u64 dev_req_cnt = 0;
+
+extern struct super_block *user_get_super(dev_t dev);
+
 /*
  * Account a tick to a process and cpustat
  * @p: the process that the cpu time gets accounted to
@@ -363,6 +377,14 @@
 {
 	u64 other, cputime = TICK_NSEC * ticks;
 
+	//PARALFETCH
+	//int i;
+	/*
+	struct request_queue *reqq = NULL;
+	struct block_device *bdev = NULL;
+	struct super_block *sb = NULL;
+	*/
+
 	/*
 	 * When returning from idle, many ticks can get accounted at
 	 * once, including some ticks of steal, irq, and softirq time.
@@ -376,6 +398,35 @@
 
 	cputime -= other;
 
+	pf_cnt++;
+	
+	//if((pf_cnt % 1000000) == 0) {
+		/*
+		for_each_online_cpu(i) {
+			printk("*PF* cpu: %d,  user: %llu, system: %llu, idle: %llu\n",
+					i, per_cpu(pf_user_time, i),
+					per_cpu(pf_system_time, i),
+					per_cpu(pf_idle_time, i));
+		}
+		printk("*PF* dev_req_cnt: %llu\n", dev_req_cnt);				
+		*/
+		/*
+		sb = user_get_super(0x800005);
+		if(sb) {
+			bdev = sb->s_bdev;
+			if(bdev)
+			      	reqq = bdev_get_queue(bdev);
+			if(reqq) printk("*PF* requests: %d, in_flight: %d\n", 
+					reqq->root_rl.count[0] + reqq->root_rl.count[1],
+					reqq->in_flight[0] + reqq->in_flight[1]);
+		}
+		*/
+		//printk("*PF* sched_req_cnt: %llu\n", sched_req_cnt);				
+		/*printk("*PF* sched_req_cnt: %llu, dev_req_cnt: %llu\n", sched_req_cnt,
+				dev_req_cnt);				
+				*/
+	//}
+
 	if (this_cpu_ksoftirqd() == p) {
 		/*
 		 * ksoftirqd time do not get accounted in cpu_softirq_time.
@@ -384,12 +435,16 @@
 		 */
 		account_system_index_time(p, cputime, CPUTIME_SOFTIRQ);
 	} else if (user_tick) {
+		per_cpu(pf_user_time, smp_processor_id()) += cputime;
 		account_user_time(p, cputime);
 	} else if (p == rq->idle) {
+		per_cpu(pf_idle_time, smp_processor_id()) += cputime;
 		account_idle_time(cputime);
 	} else if (p->flags & PF_VCPU) { /* System time or guest time */
+		per_cpu(pf_guest_time, smp_processor_id()) += cputime;
 		account_guest_time(p, cputime);
 	} else {
+		per_cpu(pf_system_time, smp_processor_id()) += cputime;
 		account_system_index_time(p, cputime, CPUTIME_SYSTEM);
 	}
 }
diff -uNr linux-4.12.9/Makefile linux-4.12.9-pf/Makefile
--- linux-4.12.9/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/Makefile	2019-01-02 21:24:25.831430465 +0900
@@ -1291,9 +1291,9 @@
 		  arch/*/include/generated .tmp_objdiff
 MRPROPER_FILES += .config .config.old .version .old_version \
 		  Module.symvers tags TAGS cscope* GPATH GTAGS GRTAGS GSYMS \
-		  signing_key.pem signing_key.priv signing_key.x509	\
-		  x509.genkey extra_certificates signing_key.x509.keyid	\
-		  signing_key.x509.signer vmlinux-gdb.py
+		  certs/signing_key.pem certs/signing_key.priv certs/signing_key.x509   \
+		  certs/x509.genkey certs/extra_certificates certs/signing_key.x509.keyid \
+		  certs/signing_key.x509.signer vmlinux-gdb.py
 
 # clean - Delete most, but leave enough to build external modules
 #
diff -uNr linux-4.12.9/mm/fadvise.c linux-4.12.9-pf/mm/fadvise.c
--- linux-4.12.9/mm/fadvise.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/fadvise.c	2019-01-02 21:24:57.454609712 +0900
@@ -109,8 +109,9 @@
 		 * Ignore return value because fadvise() shall return
 		 * success even if filesystem can't retrieve a hint,
 		 */
+		//PARALFETCH
 		force_page_cache_readahead(mapping, f.file, start_index,
-					   nrpages);
+					   nrpages, 0);
 		break;
 	case POSIX_FADV_NOREUSE:
 		break;
diff -uNr linux-4.12.9/mm/filemap.c linux-4.12.9-pf/mm/filemap.c
--- linux-4.12.9/mm/filemap.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/filemap.c	2019-01-02 21:24:57.458609734 +0900
@@ -48,6 +48,10 @@
 
 #include <asm/mman.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>                                                   
+#include <linux/flashfetch_trace.h>
+
 /*
  * Shared mappings implemented 30.11.1994. It's not fully working yet,
  * though.
@@ -1759,6 +1763,23 @@
 	ra->ra_pages /= 4;
 }
 
+//PARALFETCH
+extern unsigned long long int flashfetch_readahead_hit;
+extern unsigned long long int flashfetch_prefetch_hit;
+
+//PARALFETCH2
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+unsigned int p_dev=0;
+unsigned long p_ino=0;
+unsigned long p_offs=0;
+unsigned long p_len=0;
+unsigned int n_dev=0;
+unsigned long n_ino=0;
+unsigned long n_offs=0;
+unsigned long n_len=0;
+
+
 /**
  * do_generic_file_read - generic file read routine
  * @filp:	the file to read
@@ -1795,6 +1816,42 @@
 	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
 	offset = *ppos & ~PAGE_MASK;
 
+        //PARALFETCH2
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+		n_dev = inode->i_sb->s_dev;
+		n_ino = inode->i_ino;
+		n_offs = index;
+		n_len = last_index - index;
+		if(n_dev == p_dev && n_ino == p_ino) {
+			if(n_offs <= p_offs && n_offs+n_len >= p_offs+p_len) {
+				p_offs = n_offs;
+				p_len = n_len;
+			} else if(n_offs >= p_offs && n_offs+n_len <= p_offs+p_len) {
+				;
+			} else if(n_offs+n_len <= p_offs && n_offs+n_len <= p_offs+p_len) {
+				p_offs = n_offs;
+				p_len = p_len + p_offs - n_offs;
+			} else if(n_offs >= p_offs && n_offs <= p_offs+p_len) {
+				p_len = n_len + n_offs - p_offs;
+			} else {
+				printk("PARALFETCH: DGFR1: dev:0x%x ino:%lu (%lu, %lu)\n",
+					p_dev, p_ino, p_offs, p_len);
+
+				p_dev = n_dev;
+				p_ino = n_ino;
+				p_offs = n_offs;
+				p_len = n_len;
+			}
+		} else {
+			printk("PARALFETCH: DGFR2: dev:0x%x ino:%lu (%lu, %lu)\n",
+				p_dev, p_ino, p_offs, p_len);
+			p_dev = n_dev;
+			p_ino = n_ino;
+			p_offs = n_offs;
+			p_len = n_len;
+		}
+	}
+
 	for (;;) {
 		struct page *page;
 		pgoff_t end_index;
@@ -1817,12 +1874,20 @@
 			if (unlikely(page == NULL))
 				goto no_cached_page;
 		}
+#ifdef CONFIG_PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+#endif
 		if (PageReadahead(page)) {
 			page_cache_async_readahead(mapping,
 					ra, filp, page,
 					index, last_index - index);
 		}
 		if (!PageUptodate(page)) {
+#ifdef CONFIG_PARALFETCH
+			if (TestClearPagePrefetch(page)) 
+				flashfetch_readahead_hit++;
+#endif
 			/*
 			 * See comment in do_read_cache_page on why
 			 * wait_on_page_locked is used to avoid unnecessarily
@@ -1850,6 +1915,11 @@
 				goto page_not_up_to_date_locked;
 			unlock_page(page);
 		}
+#ifdef CONFIG_PARALFETCH
+		//PARALFECH
+		else if (TestClearPagePrefetch(page)) 
+				flashfetch_prefetch_hit++;
+#endif
 page_ok:
 		/*
 		 * i_size must be checked after we know the page is Uptodate.
@@ -2119,6 +2189,9 @@
 {
 	struct address_space *mapping = file->f_mapping;
 
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
 		return;
@@ -2126,6 +2199,13 @@
 		return;
 
 	if (vma->vm_flags & VM_SEQ_READ) {
+		//PARALFETCH
+		/*
+                printk("PF:SMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
 		page_cache_sync_readahead(mapping, ra, file, offset,
 					  ra->ra_pages);
 		return;
@@ -2148,6 +2228,14 @@
 	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
 	ra->size = ra->ra_pages;
 	ra->async_size = ra->ra_pages / 4;
+
+	//PARALFETCH
+	/*
+        printk("PF:SMRAR:dev:0x%x ino:%lu offs:%lu, size:%u",
+                        inode->i_sb->s_dev, inode->i_ino,
+                        ra->start, ra->size);
+			*/
+
 	ra_submit(ra, mapping, file);
 }
 
@@ -2163,14 +2251,25 @@
 {
 	struct address_space *mapping = file->f_mapping;
 
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
 		return;
 	if (ra->mmap_miss > 0)
 		ra->mmap_miss--;
-	if (PageReadahead(page))
+	if (PageReadahead(page)) {
+		//PARALFETCH
+		/*
+                printk("PF:AMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
 		page_cache_async_readahead(mapping, ra, file,
 					   page, offset, ra->ra_pages);
+	}
 }
 
 /**
@@ -2317,6 +2416,13 @@
 }
 EXPORT_SYMBOL(filemap_fault);
 
+//PARALFETCH
+extern pid_t launch_pid;
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+extern int pfault_trace;
+extern int pfault_debug;
+
 void filemap_map_pages(struct vm_fault *vmf,
 		pgoff_t start_pgoff, pgoff_t end_pgoff)
 {
@@ -2328,15 +2434,62 @@
 	unsigned long max_idx;
 	struct page *head, *page;
 
+	//PARALFETCH
+        struct inode *inode = mapping->host;
+	int idx;
+        struct disk_log_entry *ff_log;
+	int should_log = 0;
+	u64 log_start_pgoff = 0;
+	u64 log_ts = 0;
+	/* previously commented
+        if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid))
+	       	printk("PF:filemap_map_pages:dev:0x%x ino:%lu start_pgoff:%lu, end_pgoff:%lu",
+		inode->i_sb->s_dev, inode->i_ino,
+		start_pgoff, end_pgoff);
+		*/
+	if(flashfetch_monitor_blkio && flashfetch_trace && pfault_trace) {
+		if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid)) {
+			if(inode && inode->i_ino != 0) {
+				if(inode->i_sb && inode->i_sb->s_dev) {
+					should_log = 1;
+					log_start_pgoff = start_pgoff;
+					log_ts = ktime_to_ns(ktime_get());
+				}
+			}
+		}
+	}
+
 	rcu_read_lock();
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter,
 			start_pgoff) {
-		if (iter.index > end_pgoff)
+		if (iter.index > end_pgoff) {
+			// make log;
+			if(should_log && log_start_pgoff <= end_pgoff) {
+				idx = get_log_entry(flashfetch_trace);
+				if(idx >= 0) {
+					ff_log = &(flashfetch_trace->log[idx]);
+					ff_log->ts = log_ts;
+					ff_log->dev = inode->i_sb->s_dev;
+					ff_log->ino = inode->i_ino;
+					ff_log->blk_num = (u64)log_start_pgoff;
+					ff_log->blk_len = (u32)(end_pgoff - log_start_pgoff + 1);
+
+					if(pfault_debug)
+						printk("PFAULT: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+					if(ff_log->dev == 0)
+						printk("FFFF: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+				}
+			}
 			break;
+		}
 repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			goto next;
+		//PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+		
 		if (radix_tree_exception(page)) {
 			if (radix_tree_deref_retry(page)) {
 				slot = radix_tree_iter_retry(&iter);
@@ -2394,6 +2547,27 @@
 		/* Huge page is mapped? No need to proceed. */
 		if (pmd_trans_huge(*vmf->pmd))
 			break;
+
+		// make log and reset log start index
+		if(should_log && (log_start_pgoff != iter.index)) {
+			idx = get_log_entry(flashfetch_trace);
+			if(idx >= 0) {
+				ff_log = &(flashfetch_trace->log[idx]);
+				ff_log->ts = log_ts;
+
+				ff_log->dev = inode->i_sb->s_dev;
+				ff_log->ino = inode->i_ino;
+				ff_log->blk_num = (u64)log_start_pgoff;
+				ff_log->blk_len = (u32)(iter.index - log_start_pgoff + 1);
+
+				if(pfault_debug)
+					printk("PFAULT: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+				if(ff_log->dev == 0)
+					printk("FFFFF: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+			}
+			log_start_pgoff = iter.index + 1;
+		}
+
 		if (iter.index == end_pgoff)
 			break;
 	}
@@ -2411,7 +2585,7 @@
 	file_update_time(vmf->vma->vm_file);
 	lock_page(page);
 	if (page->mapping != inode->i_mapping) {
-		unlock_page(page);
+	unlock_page(page);
 		ret = VM_FAULT_NOPAGE;
 		goto out;
 	}
diff -uNr linux-4.12.9/mm/internal.h linux-4.12.9-pf/mm/internal.h
--- linux-4.12.9/mm/internal.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/internal.h	2019-01-02 21:24:57.460609746 +0900
@@ -53,9 +53,10 @@
 			     unsigned long addr, unsigned long end,
 			     struct zap_details *details);
 
+//PARALFETCH
 extern int __do_page_cache_readahead(struct address_space *mapping,
 		struct file *filp, pgoff_t offset, unsigned long nr_to_read,
-		unsigned long lookahead_size);
+		unsigned long lookahead_size, int prefetch);
 
 /*
  * Submit IO for the read-ahead request in file_ra_state.
@@ -63,8 +64,9 @@
 static inline unsigned long ra_submit(struct file_ra_state *ra,
 		struct address_space *mapping, struct file *filp)
 {
+	//PARALFETCH
 	return __do_page_cache_readahead(mapping, filp,
-					ra->start, ra->size, ra->async_size);
+				ra->start, ra->size, ra->async_size, 0);
 }
 
 /*
diff -uNr linux-4.12.9/mm/madvise.c linux-4.12.9-pf/mm/madvise.c
--- linux-4.12.9/mm/madvise.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/madvise.c	2019-01-02 21:24:57.452609701 +0900
@@ -293,7 +293,8 @@
 		end = vma->vm_end;
 	end = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 
-	force_page_cache_readahead(file->f_mapping, file, start, end - start);
+	//PARALFETCH
+	force_page_cache_readahead(file->f_mapping, file, start, end - start, 0);
 	return 0;
 }
 
diff -uNr linux-4.12.9/mm/memory.c linux-4.12.9-pf/mm/memory.c
--- linux-4.12.9/mm/memory.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/memory.c	2019-01-02 21:24:57.457609729 +0900
@@ -3368,11 +3368,17 @@
 	 * something).
 	 */
 	if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
+		//PARALFETCH
+		//printk("1:DFA\n");
+
 		ret = do_fault_around(vmf);
 		if (ret)
 			return ret;
 	}
 
+	//PARALFETCH
+	//printk("2:DF\n");
+
 	ret = __do_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
diff -uNr linux-4.12.9/mm/readahead.c linux-4.12.9-pf/mm/readahead.c
--- linux-4.12.9/mm/readahead.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/readahead.c	2019-01-02 21:24:57.456609723 +0900
@@ -147,9 +147,10 @@
  *
  * Returns the number of pages requested, or the maximum amount of I/O allowed.
  */
+//PARALFETCH
 int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 			pgoff_t offset, unsigned long nr_to_read,
-			unsigned long lookahead_size)
+			unsigned long lookahead_size, int prefetch)
 {
 	struct inode *inode = mapping->host;
 	struct page *page;
@@ -165,6 +166,13 @@
 
 	end_index = ((isize - 1) >> PAGE_SHIFT);
 
+	//PARALFETCH
+	/*
+	if (prefetch == 1) {
+		printk("P-Page\n");
+	}
+	*/
+
 	/*
 	 * Preallocate as many pages as we will need.
 	 */
@@ -188,6 +196,12 @@
 		if (page_idx == nr_to_read - lookahead_size)
 			SetPageReadahead(page);
 		ret++;
+
+		//PARALFETCH
+		if ((prefetch == 1) && (page_offset == offset)) {
+			///printk("P-Page\n");
+			SetPagePrefetch(page);
+		}
 	}
 
 	/*
@@ -206,8 +220,9 @@
  * Chunk the readahead into 2 megabyte units, so that we don't pin too much
  * memory at once.
  */
+//PARALFETCH
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
-			       pgoff_t offset, unsigned long nr_to_read)
+			       pgoff_t offset, unsigned long nr_to_read, int prefetch)
 {
 	struct backing_dev_info *bdi = inode_to_bdi(mapping->host);
 	struct file_ra_state *ra = &filp->f_ra;
@@ -229,8 +244,9 @@
 
 		if (this_chunk > nr_to_read)
 			this_chunk = nr_to_read;
+		//PARALFETCH
 		err = __do_page_cache_readahead(mapping, filp,
-						offset, this_chunk, 0);
+					offset, this_chunk, 0, prefetch);
 		if (err < 0)
 			return err;
 
@@ -457,7 +473,8 @@
 	 * standalone, small random read
 	 * Read as is, and do not pollute the readahead state.
 	 */
-	return __do_page_cache_readahead(mapping, filp, offset, req_size, 0);
+	//PARALFETCH
+	return __do_page_cache_readahead(mapping, filp, offset, req_size, 0, 0);
 
 initial_readahead:
 	ra->start = offset;
@@ -502,7 +519,8 @@
 
 	/* be dumb */
 	if (filp && (filp->f_mode & FMODE_RANDOM)) {
-		force_page_cache_readahead(mapping, filp, offset, req_size);
+		//PARALFETCH
+		force_page_cache_readahead(mapping, filp, offset, req_size, 0);
 		return;
 	}
 
@@ -570,7 +588,8 @@
 	if (dax_mapping(mapping))
 		return 0;
 
-	return force_page_cache_readahead(mapping, filp, index, nr);
+	//PARALFETCH
+	return force_page_cache_readahead(mapping, filp, index, nr, 0);
 }
 
 SYSCALL_DEFINE3(readahead, int, fd, loff_t, offset, size_t, count)
diff -uNr linux-4.12.9/tools/vm/page-types.c linux-4.12.9-pf/tools/vm/page-types.c
--- linux-4.12.9/tools/vm/page-types.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/tools/vm/page-types.c	2019-01-02 21:25:05.887657968 +0900
@@ -99,6 +99,9 @@
 #define KPF_SWAP		62
 #define KPF_MMAP_EXCLUSIVE	63
 
+//PARALFETCH
+#define KPF_PREFETCH		64
+
 #define KPF_ALL_BITS		((uint64_t)~0ULL)
 #define KPF_HACKERS_BITS	(0xffffULL << 32)
 #define KPF_OVERLOADED_BITS	(0xffffULL << 48)
@@ -152,6 +155,8 @@
 	[KPF_FILE]		= "F:file",
 	[KPF_SWAP]		= "w:swap",
 	[KPF_MMAP_EXCLUSIVE]	= "1:mmap_exclusive",
+//PARALFETCH
+	[KPF_PREFETCH]		= "I:prefetch",
 };
 
 
