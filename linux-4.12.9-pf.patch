diff -uNr linux-4.12.9/block/blk-core.c linux-4.12.9-pf/block/blk-core.c
--- linux-4.12.9/block/blk-core.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/blk-core.c	2019-01-02 21:24:25.705429751 +0900
@@ -2550,6 +2550,19 @@
 }
 EXPORT_SYMBOL(blk_fetch_request);
 
+//PARALFETCH                                                                   
+extern bool flashfetch_monitor_rq_complete;                                    
+extern unsigned long long int evaluator_lba;                                   
+                                                                               
+#ifdef CONFIG_ACPI_BATTERY                                                     
+extern int get_ff_battery(void);                                               
+void print_battery_status(struct work_struct *work)                            
+{                                                                              
+               printk("E-BATTERY[%d]\n", get_ff_battery());                    
+}                                                                              
+DECLARE_WORK(print_battery_work, print_battery_status);                        
+#endif
+
 /**
  * blk_update_request - Special helper function for request stacking drivers
  * @req:      the request being processed
@@ -2577,10 +2590,20 @@
 	int total_bytes;
 
 	trace_block_rq_complete(req, error, nr_bytes);
-
+	
 	if (!req->bio)
 		return false;
 
+	//PARALFETCH
+	if(evaluator_lba) {                                                     
+		if(req->bio->bi_bdev && (blk_rq_pos(req) == evaluator_lba)) {   
+			if(rq_data_dir(req) == READ) {                     
+				printk("E %llu %x %lu + %u\n", ktime_to_ns(ktime_get()), (int)req->bio->bi_bdev->bd_dev, blk_rq_pos(req), blk_rq_bytes(req));
+				//schedule_work(&print_battery_work);           
+			}                                                       
+		}                                                               
+	}
+	
 	if (error && !blk_rq_is_passthrough(req) &&
 	    !(req->rq_flags & RQF_QUIET)) {
 		char *error_type;
diff -uNr linux-4.12.9/block/fiops-iosched.c linux-4.12.9-pf/block/fiops-iosched.c
--- linux-4.12.9/block/fiops-iosched.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/block/fiops-iosched.c	2019-01-02 21:24:25.713429797 +0900
@@ -0,0 +1,762 @@
+/*
+ * IOPS based IO scheduler. Based on CFQ.
+ *  Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>
+ *  Shaohua Li <shli@kernel.org>
+ */
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/blkdev.h>
+#include <linux/elevator.h>
+#include <linux/jiffies.h>
+#include <linux/rbtree.h>
+#include <linux/ioprio.h>
+#include <linux/blktrace_api.h>
+#include "blk.h"
+
+#define VIOS_SCALE_SHIFT 10
+#define VIOS_SCALE (1 << VIOS_SCALE_SHIFT)
+
+#define VIOS_READ_SCALE (1)
+#define VIOS_WRITE_SCALE (1)
+#define VIOS_SYNC_SCALE (2)
+#define VIOS_ASYNC_SCALE (5)
+
+#define VIOS_PRIO_SCALE (5)
+
+struct fiops_rb_root {
+	struct rb_root rb;
+	struct rb_node *left;
+	unsigned count;
+
+	u64 min_vios;
+};
+#define FIOPS_RB_ROOT	(struct fiops_rb_root) { .rb = RB_ROOT}
+
+enum wl_prio_t {
+	IDLE_WORKLOAD = 0,
+	BE_WORKLOAD = 1,
+	RT_WORKLOAD = 2,
+	FIOPS_PRIO_NR,
+};
+
+struct fiops_data {
+	struct request_queue *queue;
+
+	struct fiops_rb_root service_tree[FIOPS_PRIO_NR];
+
+	unsigned int busy_queues;
+	unsigned int in_flight[2];
+
+	struct work_struct unplug_work;
+
+	unsigned int read_scale;
+	unsigned int write_scale;
+	unsigned int sync_scale;
+	unsigned int async_scale;
+};
+
+struct fiops_ioc {
+	struct io_cq icq;
+
+	unsigned int flags;
+	struct fiops_data *fiopsd;
+	struct rb_node rb_node;
+	u64 vios; /* key in service_tree */
+	struct fiops_rb_root *service_tree;
+
+	unsigned int in_flight;
+
+	struct rb_root sort_list;
+	struct list_head fifo;
+
+	pid_t pid;
+	unsigned short ioprio;
+	enum wl_prio_t wl_type;
+};
+
+#define ioc_service_tree(ioc) (&((ioc)->fiopsd->service_tree[(ioc)->wl_type]))
+#define RQ_CIC(rq)		icq_to_cic((rq)->elv.icq)
+
+enum ioc_state_flags {
+	FIOPS_IOC_FLAG_on_rr = 0,	/* on round-robin busy list */
+	FIOPS_IOC_FLAG_prio_changed,	/* task priority has changed */
+};
+
+#define FIOPS_IOC_FNS(name)						\
+static inline void fiops_mark_ioc_##name(struct fiops_ioc *ioc)	\
+{									\
+	ioc->flags |= (1 << FIOPS_IOC_FLAG_##name);			\
+}									\
+static inline void fiops_clear_ioc_##name(struct fiops_ioc *ioc)	\
+{									\
+	ioc->flags &= ~(1 << FIOPS_IOC_FLAG_##name);			\
+}									\
+static inline int fiops_ioc_##name(const struct fiops_ioc *ioc)	\
+{									\
+	return ((ioc)->flags & (1 << FIOPS_IOC_FLAG_##name)) != 0;	\
+}
+
+FIOPS_IOC_FNS(on_rr);
+FIOPS_IOC_FNS(prio_changed);
+#undef FIOPS_IOC_FNS
+
+#define fiops_log_ioc(fiopsd, ioc, fmt, args...)	\
+	blk_add_trace_msg((fiopsd)->queue, "ioc%d " fmt, (ioc)->pid, ##args)
+#define fiops_log(fiopsd, fmt, args...)	\
+	blk_add_trace_msg((fiopsd)->queue, "fiops " fmt, ##args)
+
+enum wl_prio_t fiops_wl_type(short prio_class)
+{
+	if (prio_class == IOPRIO_CLASS_RT)
+		return RT_WORKLOAD;
+	if (prio_class == IOPRIO_CLASS_BE)
+		return BE_WORKLOAD;
+	return IDLE_WORKLOAD;
+}
+
+static inline struct fiops_ioc *icq_to_cic(struct io_cq *icq)
+{
+	/* cic->icq is the first member, %NULL will convert to %NULL */
+	return container_of(icq, struct fiops_ioc, icq);
+}
+
+static inline struct fiops_ioc *fiops_cic_lookup(struct fiops_data *fiopsd,
+					       struct io_context *ioc)
+{
+	if (ioc)
+		return icq_to_cic(ioc_lookup_icq(ioc, fiopsd->queue));
+	return NULL;
+}
+
+/*
+ * The below is leftmost cache rbtree addon
+ */
+static struct fiops_ioc *fiops_rb_first(struct fiops_rb_root *root)
+{
+	/* Service tree is empty */
+	if (!root->count)
+		return NULL;
+
+	if (!root->left)
+		root->left = rb_first(&root->rb);
+
+	if (root->left)
+		return rb_entry(root->left, struct fiops_ioc, rb_node);
+
+	return NULL;
+}
+
+static void rb_erase_init(struct rb_node *n, struct rb_root *root)
+{
+	rb_erase(n, root);
+	RB_CLEAR_NODE(n);
+}
+
+static void fiops_rb_erase(struct rb_node *n, struct fiops_rb_root *root)
+{
+	if (root->left == n)
+		root->left = NULL;
+	rb_erase_init(n, &root->rb);
+	--root->count;
+}
+
+static inline u64 max_vios(u64 min_vios, u64 vios)
+{
+	s64 delta = (s64)(vios - min_vios);
+	if (delta > 0)
+		min_vios = vios;
+
+	return min_vios;
+}
+
+static void fiops_update_min_vios(struct fiops_rb_root *service_tree)
+{
+	struct fiops_ioc *ioc;
+
+	ioc = fiops_rb_first(service_tree);
+	if (!ioc)
+		return;
+	service_tree->min_vios = max_vios(service_tree->min_vios, ioc->vios);
+}
+
+/*
+ * The fiopsd->service_trees holds all pending fiops_ioc's that have
+ * requests waiting to be processed. It is sorted in the order that
+ * we will service the queues.
+ */
+static void fiops_service_tree_add(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	struct rb_node **p, *parent;
+	struct fiops_ioc *__ioc;
+	struct fiops_rb_root *service_tree = ioc_service_tree(ioc);
+	u64 vios;
+	int left;
+
+	/* New added IOC */
+	if (RB_EMPTY_NODE(&ioc->rb_node)) {
+		if (ioc->in_flight > 0)
+			vios = ioc->vios;
+		else
+			vios = max_vios(service_tree->min_vios, ioc->vios);
+	} else {
+		vios = ioc->vios;
+		/* ioc->service_tree might not equal to service_tree */
+		fiops_rb_erase(&ioc->rb_node, ioc->service_tree);
+		ioc->service_tree = NULL;
+	}
+
+	fiops_log_ioc(fiopsd, ioc, "service tree add, vios %lld", vios);
+
+	left = 1;
+	parent = NULL;
+	ioc->service_tree = service_tree;
+	p = &service_tree->rb.rb_node;
+	while (*p) {
+		struct rb_node **n;
+
+		parent = *p;
+		__ioc = rb_entry(parent, struct fiops_ioc, rb_node);
+
+		/*
+		 * sort by key, that represents service time.
+		 */
+		if (vios <  __ioc->vios)
+			n = &(*p)->rb_left;
+		else {
+			n = &(*p)->rb_right;
+			left = 0;
+		}
+
+		p = n;
+	}
+
+	if (left)
+		service_tree->left = &ioc->rb_node;
+
+	ioc->vios = vios;
+	rb_link_node(&ioc->rb_node, parent, p);
+	rb_insert_color(&ioc->rb_node, &service_tree->rb);
+	service_tree->count++;
+
+	fiops_update_min_vios(service_tree);
+}
+
+/*
+ * Update ioc's position in the service tree.
+ */
+static void fiops_resort_rr_list(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	/*
+	 * Resorting requires the ioc to be on the RR list already.
+	 */
+	if (fiops_ioc_on_rr(ioc))
+		fiops_service_tree_add(fiopsd, ioc);
+}
+
+/*
+ * add to busy list of queues for service, trying to be fair in ordering
+ * the pending list according to last request service
+ */
+static void fiops_add_ioc_rr(struct fiops_data *fiopsd, struct fiops_ioc *ioc)
+{
+	BUG_ON(fiops_ioc_on_rr(ioc));
+	fiops_mark_ioc_on_rr(ioc);
+
+	fiopsd->busy_queues++;
+
+	fiops_resort_rr_list(fiopsd, ioc);
+}
+
+/*
+ * Called when the ioc no longer has requests pending, remove it from
+ * the service tree.
+ */
+static void fiops_del_ioc_rr(struct fiops_data *fiopsd, struct fiops_ioc *ioc)
+{
+	BUG_ON(!fiops_ioc_on_rr(ioc));
+	fiops_clear_ioc_on_rr(ioc);
+
+	if (!RB_EMPTY_NODE(&ioc->rb_node)) {
+		fiops_rb_erase(&ioc->rb_node, ioc->service_tree);
+		ioc->service_tree = NULL;
+	}
+
+	BUG_ON(!fiopsd->busy_queues);
+	fiopsd->busy_queues--;
+}
+
+/*
+ * rb tree support functions
+ */
+static void fiops_del_rq_rb(struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	elv_rb_del(&ioc->sort_list, rq);
+}
+
+static void fiops_add_rq_rb(struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+	struct fiops_data *fiopsd = ioc->fiopsd;
+
+	elv_rb_add(&ioc->sort_list, rq);
+
+	if (!fiops_ioc_on_rr(ioc))
+		fiops_add_ioc_rr(fiopsd, ioc);
+}
+
+static void fiops_reposition_rq_rb(struct fiops_ioc *ioc, struct request *rq)
+{
+	elv_rb_del(&ioc->sort_list, rq);
+	fiops_add_rq_rb(rq);
+}
+
+static void fiops_remove_request(struct request *rq)
+{
+	list_del_init(&rq->queuelist);
+	fiops_del_rq_rb(rq);
+}
+
+static u64 fiops_scaled_vios(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc, struct request *rq)
+{
+	int vios = VIOS_SCALE;
+
+	if (rq_data_dir(rq) == WRITE)
+		vios = vios * fiopsd->write_scale / fiopsd->read_scale;
+
+	if (!rq_is_sync(rq))
+		vios = vios * fiopsd->async_scale / fiopsd->sync_scale;
+
+	vios +=  vios * (ioc->ioprio - IOPRIO_NORM) / VIOS_PRIO_SCALE;
+
+	return vios;
+}
+
+/* return vios dispatched */
+static u64 fiops_dispatch_request(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc)
+{
+	struct request *rq;
+	struct request_queue *q = fiopsd->queue;
+
+	rq = rq_entry_fifo(ioc->fifo.next);
+
+	fiops_remove_request(rq);
+	elv_dispatch_add_tail(q, rq);
+
+	fiopsd->in_flight[rq_is_sync(rq)]++;
+	ioc->in_flight++;
+
+	return fiops_scaled_vios(fiopsd, ioc, rq);
+}
+
+static int fiops_forced_dispatch(struct fiops_data *fiopsd)
+{
+	struct fiops_ioc *ioc;
+	int dispatched = 0;
+	int i;
+
+	for (i = RT_WORKLOAD; i >= IDLE_WORKLOAD; i--) {
+		while (!RB_EMPTY_ROOT(&fiopsd->service_tree[i].rb)) {
+			ioc = fiops_rb_first(&fiopsd->service_tree[i]);
+
+			while (!list_empty(&ioc->fifo)) {
+				fiops_dispatch_request(fiopsd, ioc);
+				dispatched++;
+			}
+			if (fiops_ioc_on_rr(ioc))
+				fiops_del_ioc_rr(fiopsd, ioc);
+		}
+	}
+	return dispatched;
+}
+
+static struct fiops_ioc *fiops_select_ioc(struct fiops_data *fiopsd)
+{
+	struct fiops_ioc *ioc;
+	struct fiops_rb_root *service_tree = NULL;
+	int i;
+	struct request *rq;
+
+	for (i = RT_WORKLOAD; i >= IDLE_WORKLOAD; i--) {
+		if (!RB_EMPTY_ROOT(&fiopsd->service_tree[i].rb)) {
+			service_tree = &fiopsd->service_tree[i];
+			break;
+		}
+	}
+
+	if (!service_tree)
+		return NULL;
+
+	ioc = fiops_rb_first(service_tree);
+
+	rq = rq_entry_fifo(ioc->fifo.next);
+	/*
+	 * we are the only async task and sync requests are in flight, delay a
+	 * moment. If there are other tasks coming, sync tasks have no chance
+	 * to be starved, don't delay
+	 */
+	if (!rq_is_sync(rq) && fiopsd->in_flight[1] != 0 &&
+			service_tree->count == 1) {
+		fiops_log_ioc(fiopsd, ioc,
+				"postpone async, in_flight async %d sync %d",
+				fiopsd->in_flight[0], fiopsd->in_flight[1]);
+		return NULL;
+	}
+
+	return ioc;
+}
+
+static void fiops_charge_vios(struct fiops_data *fiopsd,
+	struct fiops_ioc *ioc, u64 vios)
+{
+	struct fiops_rb_root *service_tree = ioc->service_tree;
+	ioc->vios += vios;
+
+	fiops_log_ioc(fiopsd, ioc, "charge vios %lld, new vios %lld", vios, ioc->vios);
+
+	if (RB_EMPTY_ROOT(&ioc->sort_list))
+		fiops_del_ioc_rr(fiopsd, ioc);
+	else
+		fiops_resort_rr_list(fiopsd, ioc);
+
+	fiops_update_min_vios(service_tree);
+}
+
+static int fiops_dispatch_requests(struct request_queue *q, int force)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *ioc;
+	u64 vios;
+
+	if (unlikely(force))
+		return fiops_forced_dispatch(fiopsd);
+
+	ioc = fiops_select_ioc(fiopsd);
+	if (!ioc)
+		return 0;
+
+	vios = fiops_dispatch_request(fiopsd, ioc);
+
+	fiops_charge_vios(fiopsd, ioc, vios);
+	return 1;
+}
+
+static void fiops_init_prio_data(struct fiops_ioc *cic)
+{
+	struct task_struct *tsk = current;
+	struct io_context *ioc = cic->icq.ioc;
+	int ioprio_class;
+
+	if (!fiops_ioc_prio_changed(cic))
+		return;
+
+	ioprio_class = IOPRIO_PRIO_CLASS(ioc->ioprio);
+	switch (ioprio_class) {
+	default:
+		printk(KERN_ERR "fiops: bad prio %x\n", ioprio_class);
+	case IOPRIO_CLASS_NONE:
+		/*
+		 * no prio set, inherit CPU scheduling settings
+		 */
+		cic->ioprio = task_nice_ioprio(tsk);
+		cic->wl_type = fiops_wl_type(task_nice_ioclass(tsk));
+		break;
+	case IOPRIO_CLASS_RT:
+		cic->ioprio = IOPRIO_PRIO_DATA(ioc->ioprio);
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_RT);
+		break;
+	case IOPRIO_CLASS_BE:
+		cic->ioprio = IOPRIO_PRIO_DATA(ioc->ioprio);
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_BE);
+		break;
+	case IOPRIO_CLASS_IDLE:
+		cic->wl_type = fiops_wl_type(IOPRIO_CLASS_IDLE);
+		cic->ioprio = 7;
+		break;
+	}
+
+	fiops_clear_ioc_prio_changed(cic);
+}
+
+static void fiops_insert_request(struct request_queue *q, struct request *rq)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	fiops_init_prio_data(ioc);
+
+	list_add_tail(&rq->queuelist, &ioc->fifo);
+
+	fiops_add_rq_rb(rq);
+}
+
+/*
+ * scheduler run of queue, if there are requests pending and no one in the
+ * driver that will restart queueing
+ */
+static inline void fiops_schedule_dispatch(struct fiops_data *fiopsd)
+{
+	if (fiopsd->busy_queues)
+		kblockd_schedule_work(&fiopsd->unplug_work);
+}
+
+static void fiops_completed_request(struct request_queue *q, struct request *rq)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+
+	fiopsd->in_flight[rq_is_sync(rq)]--;
+	ioc->in_flight--;
+
+	fiops_log_ioc(fiopsd, ioc, "in_flight %d, busy queues %d",
+		ioc->in_flight, fiopsd->busy_queues);
+
+	if (fiopsd->in_flight[0] + fiopsd->in_flight[1] == 0)
+		fiops_schedule_dispatch(fiopsd);
+}
+
+static struct request *
+fiops_find_rq_fmerge(struct fiops_data *fiopsd, struct bio *bio)
+{
+	struct task_struct *tsk = current;
+	struct fiops_ioc *cic;
+
+	cic = fiops_cic_lookup(fiopsd, tsk->io_context);
+
+	if (cic) {
+		return elv_rb_find(&cic->sort_list, bio_end_sector(bio));
+	}
+
+	return NULL;
+}
+
+static enum elv_merge fiops_merge(struct request_queue *q, struct request **req,
+		     struct bio *bio)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct request *__rq;
+
+	__rq = fiops_find_rq_fmerge(fiopsd, bio);
+	if (__rq && elv_bio_merge_ok(__rq, bio)) {
+		*req = __rq;
+		return ELEVATOR_FRONT_MERGE;
+	}
+
+	return ELEVATOR_NO_MERGE;
+}
+
+static void fiops_merged_request(struct request_queue *q, struct request *req,
+			       enum elv_merge type)
+{
+	if (type == ELEVATOR_FRONT_MERGE) {
+		struct fiops_ioc *ioc = RQ_CIC(req);
+
+		fiops_reposition_rq_rb(ioc, req);
+	}
+}
+
+static void
+fiops_merged_requests(struct request_queue *q, struct request *rq,
+		    struct request *next)
+{
+	struct fiops_ioc *ioc = RQ_CIC(rq);
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+
+	fiops_remove_request(next);
+
+	ioc = RQ_CIC(next);
+	/*
+	 * all requests of this task are merged to other tasks, delete it
+	 * from the service tree.
+	 */
+	if (fiops_ioc_on_rr(ioc) && RB_EMPTY_ROOT(&ioc->sort_list))
+		fiops_del_ioc_rr(fiopsd, ioc);
+}
+
+static int fiops_allow_bio_merge(struct request_queue *q, struct request *rq,
+			   struct bio *bio)
+{
+	struct fiops_data *fiopsd = q->elevator->elevator_data;
+	struct fiops_ioc *cic;
+
+	/*
+	 * Lookup the ioc that this bio will be queued with. Allow
+	 * merge only if rq is queued there.
+	 */
+	cic = fiops_cic_lookup(fiopsd, current->io_context);
+
+	return cic == RQ_CIC(rq);
+}
+
+static void fiops_exit_queue(struct elevator_queue *e)
+{
+	struct fiops_data *fiopsd = e->elevator_data;
+
+	cancel_work_sync(&fiopsd->unplug_work);
+
+	kfree(fiopsd);
+}
+
+static void fiops_kick_queue(struct work_struct *work)
+{
+	struct fiops_data *fiopsd =
+		container_of(work, struct fiops_data, unplug_work);
+	struct request_queue *q = fiopsd->queue;
+
+	spin_lock_irq(q->queue_lock);
+	__blk_run_queue(q);
+	spin_unlock_irq(q->queue_lock);
+}
+
+static int fiops_init_queue(struct request_queue *q, struct elevator_type *e)
+{
+	struct fiops_data *fiopsd;
+	int i;
+	struct elevator_queue *eq;
+
+	eq = elevator_alloc(q, e);
+	if (!eq)
+		return -ENOMEM;
+
+	fiopsd = kzalloc_node(sizeof(*fiopsd), GFP_KERNEL, q->node);
+	if (!fiopsd) {
+		kobject_put(&eq->kobj);
+		return -ENOMEM;
+	}
+	eq->elevator_data = fiopsd;
+
+	fiopsd->queue = q;
+	spin_lock_irq(q->queue_lock);
+	q->elevator = eq;
+	spin_unlock_irq(q->queue_lock);
+
+	for (i = IDLE_WORKLOAD; i <= RT_WORKLOAD; i++)
+		fiopsd->service_tree[i] = FIOPS_RB_ROOT;
+
+	INIT_WORK(&fiopsd->unplug_work, fiops_kick_queue);
+
+	fiopsd->read_scale = VIOS_READ_SCALE;
+	fiopsd->write_scale = VIOS_WRITE_SCALE;
+	fiopsd->sync_scale = VIOS_SYNC_SCALE;
+	fiopsd->async_scale = VIOS_ASYNC_SCALE;
+
+	return 0;
+}
+
+static void fiops_init_icq(struct io_cq *icq)
+{
+	struct fiops_data *fiopsd = icq->q->elevator->elevator_data;
+	struct fiops_ioc *ioc = icq_to_cic(icq);
+
+	RB_CLEAR_NODE(&ioc->rb_node);
+	INIT_LIST_HEAD(&ioc->fifo);
+	ioc->sort_list = RB_ROOT;
+
+	ioc->fiopsd = fiopsd;
+
+	ioc->pid = current->pid;
+	fiops_mark_ioc_prio_changed(ioc);
+}
+
+/*
+ * sysfs parts below -->
+ */
+static ssize_t
+fiops_var_show(unsigned int var, char *page)
+{
+	return sprintf(page, "%d\n", var);
+}
+
+static ssize_t
+fiops_var_store(unsigned int *var, const char *page, size_t count)
+{
+	char *p = (char *) page;
+
+	*var = simple_strtoul(p, &p, 10);
+	return count;
+}
+
+#define SHOW_FUNCTION(__FUNC, __VAR)					\
+static ssize_t __FUNC(struct elevator_queue *e, char *page)		\
+{									\
+	struct fiops_data *fiopsd = e->elevator_data;			\
+	return fiops_var_show(__VAR, (page));				\
+}
+SHOW_FUNCTION(fiops_read_scale_show, fiopsd->read_scale);
+SHOW_FUNCTION(fiops_write_scale_show, fiopsd->write_scale);
+SHOW_FUNCTION(fiops_sync_scale_show, fiopsd->sync_scale);
+SHOW_FUNCTION(fiops_async_scale_show, fiopsd->async_scale);
+#undef SHOW_FUNCTION
+
+#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)				\
+static ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)	\
+{									\
+	struct fiops_data *fiopsd = e->elevator_data;			\
+	unsigned int __data;						\
+	int ret = fiops_var_store(&__data, (page), count);		\
+	if (__data < (MIN))						\
+		__data = (MIN);						\
+	else if (__data > (MAX))					\
+		__data = (MAX);						\
+	*(__PTR) = __data;						\
+	return ret;							\
+}
+STORE_FUNCTION(fiops_read_scale_store, &fiopsd->read_scale, 1, 100);
+STORE_FUNCTION(fiops_write_scale_store, &fiopsd->write_scale, 1, 100);
+STORE_FUNCTION(fiops_sync_scale_store, &fiopsd->sync_scale, 1, 100);
+STORE_FUNCTION(fiops_async_scale_store, &fiopsd->async_scale, 1, 100);
+#undef STORE_FUNCTION
+
+#define FIOPS_ATTR(name) \
+	__ATTR(name, S_IRUGO|S_IWUSR, fiops_##name##_show, fiops_##name##_store)
+
+static struct elv_fs_entry fiops_attrs[] = {
+	FIOPS_ATTR(read_scale),
+	FIOPS_ATTR(write_scale),
+	FIOPS_ATTR(sync_scale),
+	FIOPS_ATTR(async_scale),
+	__ATTR_NULL
+};
+
+static struct elevator_type iosched_fiops = {
+	.ops.sq = {
+		.elevator_merge_fn =		fiops_merge,
+		.elevator_merged_fn =		fiops_merged_request,
+		.elevator_merge_req_fn =	fiops_merged_requests,
+		.elevator_allow_bio_merge_fn =	fiops_allow_bio_merge,
+		.elevator_dispatch_fn =		fiops_dispatch_requests,
+		.elevator_add_req_fn =		fiops_insert_request,
+		.elevator_completed_req_fn =	fiops_completed_request,
+		.elevator_former_req_fn =	elv_rb_former_request,
+		.elevator_latter_req_fn =	elv_rb_latter_request,
+		.elevator_init_icq_fn =		fiops_init_icq,
+		.elevator_init_fn =		fiops_init_queue,
+		.elevator_exit_fn =		fiops_exit_queue,
+	},
+	.icq_size	=	sizeof(struct fiops_ioc),
+	.icq_align	=	__alignof__(struct fiops_ioc),
+	.elevator_attrs =	fiops_attrs,
+	.elevator_name =	"fiops",
+	.elevator_owner =	THIS_MODULE,
+};
+
+static int __init fiops_init(void)
+{
+	return elv_register(&iosched_fiops);
+}
+
+static void __exit fiops_exit(void)
+{
+	elv_unregister(&iosched_fiops);
+}
+
+module_init(fiops_init);
+module_exit(fiops_exit);
+
+MODULE_AUTHOR("Jens Axboe, Shaohua Li <shli@kernel.org>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("IOPS based IO scheduler");
diff -uNr linux-4.12.9/block/Kconfig.iosched linux-4.12.9-pf/block/Kconfig.iosched
--- linux-4.12.9/block/Kconfig.iosched	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/Kconfig.iosched	2019-01-02 21:24:25.706429757 +0900
@@ -32,6 +32,14 @@
 
 	  This is the default I/O scheduler.
 
+config IOSCHED_FIOPS
+	tristate "IOPS based I/O scheduler"
+	default y
+	---help---
+	  This is an IOPS based I/O scheduler. It will try to distribute
+          IOPS equally among all processes in the system. It's mainly for
+          Flash based storage.
+
 config CFQ_GROUP_IOSCHED
 	bool "CFQ Group Scheduling support"
 	depends on IOSCHED_CFQ && BLK_CGROUP
diff -uNr linux-4.12.9/block/Makefile linux-4.12.9-pf/block/Makefile
--- linux-4.12.9/block/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/Makefile	2019-01-02 21:24:25.706429757 +0900
@@ -19,6 +19,7 @@
 obj-$(CONFIG_IOSCHED_NOOP)	+= noop-iosched.o
 obj-$(CONFIG_IOSCHED_DEADLINE)	+= deadline-iosched.o
 obj-$(CONFIG_IOSCHED_CFQ)	+= cfq-iosched.o
+obj-$(CONFIG_IOSCHED_FIOPS)     += fiops-iosched.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
diff -uNr linux-4.12.9/block/noop-iosched.c linux-4.12.9-pf/block/noop-iosched.c
--- linux-4.12.9/block/noop-iosched.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/block/noop-iosched.c	2019-01-02 21:24:25.707429762 +0900
@@ -18,6 +18,10 @@
 	list_del_init(&next->queuelist);
 }
 
+#ifdef CONFIG_PARALFETCH
+extern int prefetch_state;
+#endif
+
 static int noop_dispatch(struct request_queue *q, int force)
 {
 	struct noop_data *nd = q->elevator->elevator_data;
@@ -26,7 +30,16 @@
 	rq = list_first_entry_or_null(&nd->queue, struct request, queuelist);
 	if (rq) {
 		list_del_init(&rq->queuelist);
+
+#ifdef CONFIG_PARALFETCH
+		if(unlikely(prefetch_state == 0))
+			elv_dispatch_sort(q, rq);
+		else
+			elv_dispatch_add_tail(q, rq);
+#else
 		elv_dispatch_sort(q, rq);
+#endif
+
 		return 1;
 	}
 	return 0;
@@ -80,6 +93,7 @@
 	spin_lock_irq(q->queue_lock);
 	q->elevator = eq;
 	spin_unlock_irq(q->queue_lock);
+
 	return 0;
 }
 
diff -uNr linux-4.12.9/.cocciconfig linux-4.12.9-pf/.cocciconfig
--- linux-4.12.9/.cocciconfig	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/.cocciconfig	1970-01-01 09:00:00.000000000 +0900
@@ -1,3 +0,0 @@
-[spatch]
-	options = --timeout 200
-	options = --use-gitgrep
diff -uNr linux-4.12.9/drivers/acpi/battery.c linux-4.12.9-pf/drivers/acpi/battery.c
--- linux-4.12.9/drivers/acpi/battery.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/drivers/acpi/battery.c	2019-01-02 21:24:46.551547910 +0900
@@ -1208,6 +1208,9 @@
 	return ret;
 }
 
+//PARALFETCH
+struct acpi_battery *ff_battery[2]={NULL, NULL};
+
 static int acpi_battery_add(struct acpi_device *device)
 {
 	int result = 0;
@@ -1235,6 +1238,10 @@
 	if (result)
 		goto fail;
 
+	//PARALFETCH
+        if(ff_battery[0] == NULL) ff_battery[0] = battery;
+        else if(ff_battery[1] == NULL) ff_battery[1] = battery;
+
 #ifdef CONFIG_ACPI_PROCFS_POWER
 	result = acpi_battery_add_fs(device);
 #endif
@@ -1264,6 +1271,31 @@
 	return result;
 }
 
+//PARALFETCH
+int get_ff_battery(void)                                                        
+{                                                                               
+	int tot_bat = 0;                                                        
+	int result = 0;                                                         
+
+	if(ff_battery[0] != NULL) {                                             
+		result = acpi_battery_update(ff_battery[0], false);             
+		if (result)                                                     
+			goto fail;                                              
+		tot_bat += ff_battery[0]->capacity_now;                         
+	}                                                                       
+
+	if(ff_battery[1] != NULL) {                                             
+		result = acpi_battery_update(ff_battery[1], false);             
+		if (result)                                                     
+			goto fail;                                              
+		tot_bat += ff_battery[1]->capacity_now;                         
+	}                                                                       
+
+	return tot_bat;                                                         
+fail:                                                                           
+	return -ENODEV;                                                         
+}
+
 static int acpi_battery_remove(struct acpi_device *device)
 {
 	struct acpi_battery *battery = NULL;
diff -uNr linux-4.12.9/drivers/ata/libata-core.c linux-4.12.9-pf/drivers/ata/libata-core.c
--- linux-4.12.9/drivers/ata/libata-core.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/drivers/ata/libata-core.c	2019-01-02 21:24:52.766583139 +0900
@@ -176,6 +176,8 @@
 MODULE_LICENSE("GPL");
 MODULE_VERSION(DRV_VERSION);
 
+//PARALFETCH
+extern u64 dev_req_cnt;
 
 static bool ata_sstatus_online(u32 sstatus)
 {
@@ -5086,6 +5088,9 @@
 	qc->flags &= ~ATA_QCFLAG_ACTIVE;
 	ap->qc_active &= ~(1 << qc->tag);
 
+	//PARALFETCH
+	dev_req_cnt--;
+
 	/* call completion callback */
 	qc->complete_fn(qc);
 }
@@ -5332,6 +5337,15 @@
 
 	ap->ops->qc_prep(qc);
 	trace_ata_qc_issue(qc);
+
+	//PARALFETCH
+	dev_req_cnt++;
+	/*
+	if(dev_req_cnt >= 3) {
+		printk("*PF* dev_req_cnt: %llu\n", dev_req_cnt);
+	}
+	*/
+	
 	qc->err_mask |= ap->ops->qc_issue(qc);
 	if (unlikely(qc->err_mask))
 		goto err;
diff -uNr linux-4.12.9/fs/binfmt_elf.c linux-4.12.9-pf/fs/binfmt_elf.c
--- linux-4.12.9/fs/binfmt_elf.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/binfmt_elf.c	2019-01-02 21:24:54.305591862 +0900
@@ -44,6 +44,9 @@
 #include <asm/param.h>
 #include <asm/page.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+
 #ifndef user_long_t
 #define user_long_t long
 #endif
@@ -678,6 +681,19 @@
 #endif
 }
 
+//PARALFETCH
+#ifdef CONFIG_ACPI_BATTERY
+extern int get_ff_battery(void);
+#endif
+extern unsigned long long int evaluator_lba;
+extern u64 flashfetch_launch_ts;
+
+//PARALFETCH
+extern pid_t launch_pid;
+extern unsigned long long int flashfetch_prefetch_cnt;
+extern unsigned long long int flashfetch_prefetch_hit;
+extern unsigned long long int flashfetch_readahead_hit;
+
 static int load_elf_binary(struct linux_binprm *bprm)
 {
 	struct file *interpreter = NULL; /* to shut gcc up */
@@ -701,6 +717,10 @@
 	} *loc;
 	struct arch_elf_state arch_state = INIT_ARCH_ELF_STATE;
 
+	//PARALFETCH
+        char appname[NAME_LEN+1];
+        unsigned int launch_type;
+
 	loc = kmalloc(sizeof(*loc), GFP_KERNEL);
 	if (!loc) {
 		retval = -ENOMEM;
@@ -726,6 +746,64 @@
 	if (!elf_phdata)
 		goto out;
 
+	//PARALFETCH
+#if (EI_PAD < EI_NIDENT)                                                        
+        launch_type = loc->elf_ex.e_ident[EI_PAD];                              
+                                                                                
+        /*                                                                      
+        if(evaluator_lba != 0) {                                                
+                printk("TIME: %Lu, EXEC[%s],PF[%u]\n", ktime_to_ns(ktime_get()), bprm->filename, launch_type);
+                //printk("TIME:%Lu, EXEC[%s],PF[%u], BATTERY[%i]\n", ktime_to_ns(ktime_get()), bprm->filename, launch_type, get_ff_battery());
+                                                                                
+        } */ 
+
+	switch(launch_type)
+        {
+                case NO_PREFETCH:
+                        break;
+                case ASYNC_PREFETCH:
+                case SYNC_PREFETCH:
+			//PARALFETCH
+			flashfetch_prefetch_cnt = 0;
+			flashfetch_prefetch_hit = 0;
+			flashfetch_readahead_hit = 0;
+                case GEN_PREFETCH:
+			//PARALFETCH
+			launch_pid = current->pid;
+
+                        //printk("inode:%lu, argc:%d, p:%lu\n", bprm->file->f_dentry->d_inode->i_ino, bprm->argc, bprm->p);
+			flashfetch_launch_ts = ktime_to_ns(ktime_get());        
+                        if(bprm->filename) {                                    
+                                int i, len, offs;                               
+                                len = strlen(bprm->filename);                   
+                                offs = 0;                                       
+                                for(i=len-1; i>=0; i--) {                       
+                                        if(bprm->filename[i] == '/') {          
+                                                offs = i+1;                     
+                                                break;                          
+                                        }                                       
+                                }                                               
+                                strncpy(appname, &(bprm->filename[offs]), NAME_LEN);
+                                appname[len - offs] = '\0';                     
+                                                                                
+                                printk("EXEC[%s],PF[%u]\n", appname, launch_type);
+                        }                                                       
+                        if(launch_type == ASYNC_PREFETCH) {                     
+                                flashfetch_async(appname);                      
+                        } else if(launch_type == SYNC_PREFETCH) {               
+                                flashfetch_sync(appname);                       
+                        } else if(launch_type == GEN_PREFETCH) {                
+                                flashfetch_genpf(appname, bprm->filename);      
+                        }                                                       
+                        break;                                                  
+                case SEQ_REFINE1:                                               
+                case SEQ_REFINE2:                                               
+                        break;                                                  
+                default:                                                        
+                        break;                                                  
+        }                                                                       
+#endif
+
 	elf_ppnt = elf_phdata;
 	elf_bss = 0;
 	elf_brk = 0;
diff -uNr linux-4.12.9/fs/binfmt_script.c linux-4.12.9-pf/fs/binfmt_script.c
--- linux-4.12.9/fs/binfmt_script.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/binfmt_script.c	2019-01-02 21:24:54.310591891 +0900
@@ -21,6 +21,9 @@
 	struct file *file;
 	char interp[BINPRM_BUF_SIZE];
 	int retval;
+	//PARALFETCH
+	char appname[NAME_MAX+1];
+        unsigned int launch_type = 0;
 
 	if ((bprm->buf[0] != '#') || (bprm->buf[1] != '!'))
 		return -ENOEXEC;
@@ -54,7 +57,17 @@
 		else
 			break;
 	}
-	for (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\t'); cp++);
+	//PARALFETCH
+	if(bprm->buf[2] >= '0' && bprm->buf[2] <= '9' && bprm->buf[3] == '/') {
+		launch_type = (unsigned int)(bprm->buf[2] - '0');
+		printk("Paralfetch: script launch_type: %u, %s\n", launch_type,
+				bprm->filename ? bprm->filename : "<NULL>");
+		for (cp = bprm->buf+4; (*cp == ' ') || (*cp == '\t'); cp++);
+	} else { 
+		//ORG
+		for (cp = bprm->buf+2; (*cp == ' ') || (*cp == '\t'); cp++);
+	}
+
 	if (*cp == '\0') 
 		return -ENOEXEC; /* No interpreter name found */
 	i_name = cp;
diff -uNr linux-4.12.9/fs/block_dev.c linux-4.12.9-pf/fs/block_dev.c
--- linux-4.12.9/fs/block_dev.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/block_dev.c	2019-01-02 21:24:54.299591828 +0900
@@ -1908,6 +1908,10 @@
 
 	size -= pos;
 	iov_iter_truncate(to, size);
+
+	//PARALFETCH2
+	printk("blkdev_read_iter\n");
+
 	return generic_file_read_iter(iocb, to);
 }
 EXPORT_SYMBOL_GPL(blkdev_read_iter);
diff -uNr linux-4.12.9/fs/buffer.c linux-4.12.9-pf/fs/buffer.c
--- linux-4.12.9/fs/buffer.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/buffer.c	2019-01-02 21:24:54.297591817 +0900
@@ -47,6 +47,10 @@
 #include <linux/pagevec.h>
 #include <trace/events/block.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
 			 struct writeback_control *wbc);
@@ -3090,11 +3094,19 @@
 	}
 }
 
+//PARALFETCH
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
 static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
 			 struct writeback_control *wbc)
 {
 	struct bio *bio;
 
+	//PARALFETCH
+	int ret = 0;
+        struct disk_log_entry *ff_log;
+
 	BUG_ON(!buffer_locked(bh));
 	BUG_ON(!buffer_mapped(bh));
 	BUG_ON(!bh->b_end_io);
@@ -3136,6 +3148,24 @@
 		op_flags |= REQ_PRIO;
 	bio_set_op_attrs(bio, op, op_flags);
 
+	//PARALFETCH
+        if(flashfetch_monitor_blkio && (op == REQ_OP_READ)/*((rw & RW_MASK) == READ)*/ && flashfetch_trace)
+        {
+                ret = get_log_entry(flashfetch_trace);
+                if(ret >= 0) {
+                        ff_log = &(flashfetch_trace->log[ret]);
+                        ff_log->ts = ktime_to_ns(ktime_get());
+                        ff_log->dev = bh->b_bdev->bd_dev;
+                        ff_log->ino = 0;
+                        ff_log->blk_num = (u64)bh->b_blocknr;
+                        ff_log->blk_len = (u32)bh->b_size;
+                        printk("[%d]:%lld dev:0x%x offs:%llu size:%u\n", ret, ff_log->ts, ff_log->dev, ff_log->blk_num, ff_log->blk_len);
+                        if(ff_log->dev == 0) printk("BBB[%d]:%lld dev:0x%x offs:%llu size:%u\n", ret, ff_log->ts, ff_log->dev, ff_log->blk_num, ff_log->blk_len);
+                }
+                //printk("log_ent:%d dev:0x%x offs:%lu size:%lu\n", get_log_entry(flashfetch_trace), bh->b_bdev->bd_dev, bh->b_blocknr, bh->b_size);
+                //ret = 0;
+        }
+
 	submit_bio(bio);
 	return 0;
 }
diff -uNr linux-4.12.9/fs/drop_caches.c linux-4.12.9-pf/fs/drop_caches.c
--- linux-4.12.9/fs/drop_caches.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/drop_caches.c	2019-01-02 21:24:54.304591857 +0900
@@ -13,7 +13,8 @@
 /* A global variable is a bit ugly, but it keeps the code simple */
 int sysctl_drop_caches;
 
-static void drop_pagecache_sb(struct super_block *sb, void *unused)
+//PARALFETCH
+/*static*/ void drop_pagecache_sb(struct super_block *sb, void *unused)
 {
 	struct inode *inode, *toput_inode = NULL;
 
diff -uNr linux-4.12.9/fs/exec.c linux-4.12.9-pf/fs/exec.c
--- linux-4.12.9/fs/exec.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/exec.c	2019-01-02 21:24:54.298591823 +0900
@@ -1676,6 +1676,9 @@
 	return ret;
 }
 
+//PARALFETCH
+extern unsigned long long int evaluator_lba;
+
 /*
  * sys_execve() executes a new program.
  */
@@ -1709,6 +1712,9 @@
 	 * further execve() calls fail. */
 	current->flags &= ~PF_NPROC_EXCEEDED;
 
+	//PARALFETCH
+        if(evaluator_lba) printk("X %llu %s\n", ktime_to_ns(ktime_get()), filename->name);
+
 	retval = unshare_files(&displaced);
 	if (retval)
 		goto out_ret;
diff -uNr linux-4.12.9/fs/ext4/extents.c linux-4.12.9-pf/fs/ext4/extents.c
--- linux-4.12.9/fs/ext4/extents.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/ext4/extents.c	2019-01-02 21:24:55.628599361 +0900
@@ -46,6 +46,11 @@
 
 #include <trace/events/ext4.h>
 
+//PARALFETCH
+#include <linux/flashfetch_scheduler.h>
+int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct dependency_entry **dep_ptr);
+void ext4_log_block_path(struct inode *inode, struct ext4_ext_path *path, struct dependency_entry **dep_ptr);
+
 /*
  * used by extent splitting.
  */
@@ -5947,3 +5952,260 @@
 	}
 	return replaced_count;
 }
+
+int ext4_ext_walk_range(struct inode *inode,
+                                    ext4_lblk_t block, ext4_lblk_t num,
+                                    struct dependency_entry **dep_ptr)
+{
+        struct ext4_ext_path *path = NULL;
+        struct ext4_ext_path *path_tmp = NULL; //new
+        struct ext4_extent *ex;
+        struct extent_status es;
+        ext4_lblk_t next, next_del, start = 0, end = 0;
+        ext4_lblk_t last = block + num;
+        int exists, depth = 0, err = 0;
+        unsigned int flags = 0;
+        //unsigned char blksize_bits = inode->i_sb->s_blocksize_bits;
+        struct ext4_iloc iloc; //new
+        int k, l; //new
+        struct dependency_entry **last_dep; //new
+        struct dependency_entry *new_dep; //new
+
+	//PARALFETCH
+	struct dependency_entry **head_dep = NULL;
+	struct dependency_entry *tmp_dep = NULL;
+
+        BUG_ON(inode == NULL);  //new
+        last_dep = dep_ptr; //new
+        k = ext4_get_inode_loc(inode, &iloc); //new
+
+        // new-start
+        if(!k) {
+                new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                if(!new_dep) {
+                        brelse(iloc.bh);
+                        return -ENOMEM;
+                }
+                new_dep->type = TYPE_META;
+                new_dep->pblk_num = iloc.bh->b_blocknr;
+		new_dep->blk_num = iloc.bh->b_blocknr;
+                new_dep->blk_len = EXT4_BLOCK_SIZE(inode->i_sb);
+                new_dep->next = NULL;
+                *last_dep = new_dep;
+                last_dep = &(new_dep->next);
+                brelse(iloc.bh);
+		//PARALFETCH
+		head_dep = &new_dep;
+        } else return -ENOENT;
+        // new-end
+
+        while (block < last && block != EXT_MAX_BLOCKS) {
+                num = last - block;
+                /* find extent for this block */
+                down_read(&EXT4_I(inode)->i_data_sem);
+
+                path = ext4_find_extent(inode, block, &path, 0);
+                if (IS_ERR(path)) {
+                        up_read(&EXT4_I(inode)->i_data_sem);
+                        err = PTR_ERR(path);
+                        path = NULL;
+                        break;
+                }
+
+                //new-start
+                l = path->p_depth;
+                path_tmp = path;
+                for(k=0; k<=l; k++, path_tmp++) {
+                        if(path_tmp->p_idx) {
+                                new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                                if(new_dep) {
+					new_dep->type = TYPE_META;
+                                        new_dep->pblk_num = ext4_idx_pblock(path_tmp->p_idx);
+                                        new_dep->blk_num = ext4_idx_pblock(path_tmp->p_idx);
+                                        new_dep->blk_len = EXT4_BLOCK_SIZE(inode->i_sb);
+                                        new_dep->next = NULL;
+                                        *last_dep = new_dep;
+                                        last_dep = &(new_dep->next);
+					//PARALFETCH
+					if(head_dep == NULL)
+						head_dep = &new_dep;
+                                }
+                        }
+                }
+                //new-end
+
+                depth = ext_depth(inode);
+                if (unlikely(path[depth].p_hdr == NULL)) {
+                        up_read(&EXT4_I(inode)->i_data_sem);
+                        EXT4_ERROR_INODE(inode, "path[%d].p_hdr == NULL", depth);
+                        err = -EIO;
+                        break;
+                }
+                ex = path[depth].p_ext;
+                next = ext4_ext_next_allocated_block(path);
+
+                flags = 0;
+                exists = 0;
+                if (!ex) {
+                        /* there is no extent yet, so try to allocate
+                         * all requested space */
+                        start = block;
+			end = block + num;
+                } else if (le32_to_cpu(ex->ee_block) > block) {
+                        /* need to allocate space before found extent */
+                        start = block;
+                        end = le32_to_cpu(ex->ee_block);
+                        if (block + num < end)
+                                end = block + num;
+                } else if (block >= le32_to_cpu(ex->ee_block)
+                                + ext4_ext_get_actual_len(ex)) {
+                        /* need to allocate space after found extent */
+                        start = block;
+                        end = block + num;
+                        if (end >= next)
+                                end = next;
+                } else if (block >= le32_to_cpu(ex->ee_block)) {
+                        /*
+                         * some part of requested space is covered
+                         * by found extent
+                         */
+                        start = block;
+                        end = le32_to_cpu(ex->ee_block)
+                                + ext4_ext_get_actual_len(ex);
+                        if (block + num < end)
+                                end = block + num;
+                        exists = 1;
+                } else {
+                        BUG();
+                }
+                BUG_ON(end <= start);
+
+		if (!exists) {
+                        es.es_lblk = start;
+                        es.es_len = end - start;
+                        es.es_pblk = 0;
+                } else {
+                        es.es_lblk = le32_to_cpu(ex->ee_block);
+                        es.es_len = ext4_ext_get_actual_len(ex);
+                        es.es_pblk = ext4_ext_pblock(ex);
+                        if (ext4_ext_is_unwritten(ex))
+                                flags |= FIEMAP_EXTENT_UNWRITTEN;
+
+                        //new-start
+                        new_dep = kmalloc(sizeof(struct dependency_entry), GFP_KERNEL);
+                        if(new_dep) {
+                                new_dep->type = TYPE_REGFILE;
+                                new_dep->pblk_num = es.es_pblk + (block-es.es_lblk);
+                                new_dep->blk_num = block;
+                                if(es.es_lblk + es.es_len >= block + num) new_dep->blk_len = num;
+                                else new_dep->blk_len = es.es_len - (block - es.es_lblk);
+                                new_dep->next = NULL;
+                                *last_dep = new_dep;
+                                last_dep = &(new_dep->next);
+				//PARALFETCH
+				if(head_dep != NULL) {
+					tmp_dep = *head_dep;
+					while(tmp_dep != new_dep)
+					{
+						if(tmp_dep->pblk_num > new_dep->pblk_num)
+						{
+							printk("INVERSED METADATA: meta->pblk:%llu, data->pblk:%llu\n",
+									tmp_dep->pblk_num, new_dep->pblk_num);
+						}
+						tmp_dep = tmp_dep->next;
+					}
+				}
+                        }
+                        //new-end
+                }
+		/*
+                 * Find delayed extent and update es accordingly. We call
+                 * it even in !exists case to find out whether es is the
+                 * last existing extent or not.
+                 */
+                next_del = ext4_find_delayed_extent(inode, &es);
+                if (!exists && next_del) {
+                        exists = 1;
+                        flags |= (FIEMAP_EXTENT_DELALLOC |
+                                        FIEMAP_EXTENT_UNKNOWN);
+                }
+                up_read(&EXT4_I(inode)->i_data_sem);
+
+                if (unlikely(es.es_len == 0)) {
+                        EXT4_ERROR_INODE(inode, "es.es_len == 0");
+                        err = -EIO;
+                        break;
+                }
+
+                /*
+                 * This is possible iff next == next_del == EXT_MAX_BLOCKS.
+                 * we need to check next == EXT_MAX_BLOCKS because it is
+                 * possible that an extent is with unwritten and delayed
+                 * status due to when an extent is delayed allocated and
+                 * is allocated by fallocate status tree will track both of
+                 * them in a extent.
+                 *
+                 * So we could return a unwritten and delayed extent, and
+                 * its block is equal to 'next'.
+                 */
+		if (next == next_del && next == EXT_MAX_BLOCKS) {
+                        flags |= FIEMAP_EXTENT_LAST;
+                        if (unlikely(next_del != EXT_MAX_BLOCKS ||
+                                                next != EXT_MAX_BLOCKS)) {
+                                EXT4_ERROR_INODE(inode,
+                                                "next extent == %u, next "
+                                                "delalloc extent = %u",
+                                                next, next_del);
+                                err = -EIO;
+                                break;
+                        }
+                }
+
+                //new-start
+                /*
+                if (exists) {
+                        err = fiemap_fill_next_extent(fieinfo,
+                                        (__u64)es.es_lblk << blksize_bits,
+                                        (__u64)es.es_pblk << blksize_bits,
+                                        (__u64)es.es_len << blksize_bits,
+                                        flags);
+                        if (err < 0)
+                                break;
+                        if (err == 1) {
+                                err = 0;
+                                break;
+                        }
+                }
+                */
+                //new-end
+
+		block = es.es_lblk + es.es_len;
+
+		//PARALFETCH
+		head_dep = NULL;
+        }
+
+        ext4_ext_drop_refs(path);
+        kfree(path);
+        return err;
+}
+
+void ext4_log_block_path(struct inode *inode, struct ext4_ext_path *path, struct dependency_entry **dep_ptr)
+{
+        struct ext4_iloc iloc;
+        int k, l = path->p_depth;
+
+        k = ext4_get_inode_loc(inode, &iloc);
+
+        if(!k) printk("inum: 0x%x:%lu(loc:%lu) ", inode->i_sb->s_dev, inode->i_ino, iloc.bh->b_blocknr);
+        else printk("inum: 0x%x:%lu(loc:unknown) ", inode->i_sb->s_dev, inode->i_ino);
+
+        for(k=0; k<=l; k++, path++) {
+                if(path->p_idx) printk("I %d->%llu ", le32_to_cpu(path->p_idx->ei_block), ext4_idx_pblock(path->p_idx));
+                //else if(path->p_ext) printk("E %d+%d->%llu ", le32_to_cpu(path->p_ext->ee_block), ext4_ext_get_actual_len(path->p_ext), ext4_ext_pblock(path->p_ext));
+                //else printk("]");
+        }
+        printk("\n");
+
+        brelse(iloc.bh);
+}
diff -uNr linux-4.12.9/fs/ext4/inode.c linux-4.12.9-pf/fs/ext4/inode.c
--- linux-4.12.9/fs/ext4/inode.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/ext4/inode.c	2019-01-02 21:24:55.629599367 +0900
@@ -48,6 +48,10 @@
 
 #define MPAGE_DA_EXTENT_TAIL 0x01
 
+//PARALFETCH
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
 static __u32 ext4_inode_csum(struct inode *inode, struct ext4_inode *raw,
 			      struct ext4_inode_info *ei)
 {
@@ -3270,11 +3274,38 @@
 	return generic_block_bmap(mapping, block, ext4_get_block);
 }
 
+//PARALFETCH
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
 static int ext4_readpage(struct file *file, struct page *page)
 {
 	int ret = -EAGAIN;
 	struct inode *inode = page->mapping->host;
 
+	//PARALFETCH
+        sector_t block_in_file;
+        int idx;
+        struct disk_log_entry *ff_log;
+
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                if(inode) {
+                        block_in_file = (sector_t)page->index << (PAGE_SHIFT - inode->i_blkbits);
+                        if(inode->i_sb && inode->i_sb->s_dev) {
+                                idx = get_log_entry(flashfetch_trace);
+                                if(idx >= 0) {
+                                        ff_log = &(flashfetch_trace->log[idx]);
+                                        ff_log->ts = ktime_to_ns(ktime_get());
+                                        ff_log->dev = inode->i_sb->s_dev;
+                                        ff_log->ino = inode->i_ino;
+                                        ff_log->blk_num = (u64)block_in_file;
+                                        ff_log->blk_len = (u32)1 /*PAGE_SIZE*/;
+                                        /*if(ff_log->dev == 0)*/ printk("RRR [%d]:%lld dev:0x%x ino:%lu offs:%llu size:%u\n", idx, ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+                                }
+                        }
+                }
+        }
+
 	trace_ext4_readpage(page);
 
 	if (ext4_has_inline_data(inode))
@@ -3291,6 +3322,30 @@
 		struct list_head *pages, unsigned nr_pages)
 {
 	struct inode *inode = mapping->host;
+	//PARALFETCH
+        sector_t block_in_file;
+        struct page *page;
+        int idx;
+        struct disk_log_entry *ff_log;
+
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                if(inode) {
+                        page = list_entry(pages->prev, struct page, lru);
+                        block_in_file = (sector_t)page->index << (PAGE_SHIFT - inode->i_blkbits);
+                        if(inode->i_sb && inode->i_sb->s_dev) {
+                                idx = get_log_entry(flashfetch_trace);
+                                if(idx >= 0) {
+                                        ff_log = &(flashfetch_trace->log[idx]);
+                                        ff_log->ts = ktime_to_ns(ktime_get());
+                                        ff_log->dev = inode->i_sb->s_dev;
+                                        ff_log->ino = inode->i_ino;
+                                        ff_log->blk_num = (u64)block_in_file;
+                                        ff_log->blk_len = (u32)nr_pages/* * PAGE_SIZE */;
+                                        /*if(ff_log->dev == 0)*/ printk("RRRR [%d]:%lld dev:0x%x ino:%lu offs:%llu size:%u\n", idx, ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+                                }
+                        }
+                }
+        }
 
 	/* If the file has inline data, no need to do readpages. */
 	if (ext4_has_inline_data(inode))
diff -uNr linux-4.12.9/fs/flashfetch_core.c linux-4.12.9-pf/fs/flashfetch_core.c
--- linux-4.12.9/fs/flashfetch_core.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_core.c	2019-01-02 21:24:54.295591806 +0900
@@ -0,0 +1,688 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/writeback.h>
+#include <linux/sysctl.h>
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/flashfetch.h>
+#include <linux/jiffies.h>
+#include <linux/param.h>
+
+#include <linux/debugfs.h>
+#include <linux/kthread.h>
+
+//PARALFETCH
+#include <linux/mm_inline.h>
+
+#include <linux/flashfetch_trace.h>
+#include <linux/signal.h>
+
+unsigned int flashfetch_monitor_blkio = 0;
+EXPORT_SYMBOL(flashfetch_monitor_blkio);
+
+unsigned int flashfetch_monitor_rq_complete = 0;
+EXPORT_SYMBOL(flashfetch_monitor_rq_complete);
+
+unsigned int flashfetch_monitor_timeout = FFTIMEOUT;
+EXPORT_SYMBOL(flashfetch_monitor_timeout);
+
+struct disk_trace *flashfetch_trace = NULL;
+EXPORT_SYMBOL(flashfetch_trace);
+
+unsigned int flashfetch_flags = 0;
+EXPORT_SYMBOL(flashfetch_flags);
+
+u64 flashfetch_launch_ts = 0;
+EXPORT_SYMBOL(flashfetch_launch_ts);
+
+//PARALFETCH
+int pfault_trace = 0;
+int pfault_debug = 0;
+
+unsigned long long int flashfetch_readahead_hit = 0;
+EXPORT_SYMBOL(flashfetch_readahead_hit);
+
+unsigned long long int flashfetch_prefetch_cnt = 0;
+EXPORT_SYMBOL(flashfetch_prefetch_cnt);
+
+unsigned long long int flashfetch_prefetch_hit = 0;
+EXPORT_SYMBOL(flashfetch_prefetch_hit);
+
+
+//PARALFETCH
+pid_t launch_pid = 0;
+
+spinlock_t flashfetch_lock = __SPIN_LOCK_UNLOCKED(flashfetch_lock);
+
+int flashfetch_async(char *appname)
+{
+	int ret;
+	//printk("flashfetch_async: %s\n", appname);
+	ret = flashfetch_prefetch_async(appname);
+	return ret;
+}
+
+int flashfetch_sync(char *appname)
+{
+	int ret;
+	//printk("flashfetch_sync: %s\n", appname);
+	printk("jiffies before prefetch : %lu\n", jiffies);
+	ret = flashfetch_prefetch_sync(appname);
+	printk("jiffies after prefetch : %lu\n", jiffies);
+	return ret;
+}
+
+extern void drop_pagecache_sb(struct super_block *sb, void *unused);
+extern void drop_slab(void);
+
+extern int metadata_shift_onoff;
+extern int metadata_shift_way;
+extern int metadata_shift_value;
+extern int io_distance_merge_onoff;
+extern int io_distance_allowed;
+extern int io_infill_distance_merge_onoff;
+extern int io_infill_distance_allowed;
+extern int io_infill_hole_allowed_blks;
+extern unsigned long long int evaluator_lba;
+
+//PARALFETCH
+extern int rotational_value;
+extern pid_t launch_pid;
+
+int seqmon_thread(void *params)
+//void seqmon_thread(struct work_struct *work)
+{
+	struct seqmon_param *param;
+	struct disk_log_entry *logs;
+	int i, log_count;
+	replay_info *trace_buffer = NULL;
+	int trace_count;
+	//int j;
+	
+	//param = (struct seqmon_param *)work->data;
+	param = (struct seqmon_param *)params;
+
+	printk(KERN_WARNING "seqmon_thread appname:%s, filename:%s\n", param->appname, param->filename);
+	
+	//daemonize("flashfetch_seqmon");
+	allow_signal(SIGKILL);
+	printk("jiffies before timeout : %lu\n", jiffies);
+	schedule_timeout_uninterruptible(flashfetch_monitor_timeout * HZ);
+	printk("jiffies after timeout : %lu\n", jiffies);
+
+	//PARALFETCH
+	launch_pid = 0;
+
+	flashfetch_monitor_blkio = 0;
+	
+	logs = kmalloc(sizeof(struct disk_log_entry) * (flashfetch_trace->pos), GFP_KERNEL);
+	if(logs) {
+		memcpy((char *)logs, (char *)flashfetch_trace->log, sizeof(struct disk_log_entry) * (flashfetch_trace->pos));
+	}
+
+/*
+#define TIME_INTERVAL (unsigned long)1000000000
+#define MIN_COUNT 5
+
+	for(i=0, log_count=0; i<flashfetch_trace->pos; i++)
+	{
+		if(logs[i].ts - logs[log_count].ts < TIME_INTERVAL) continue;
+		for(j=log_count; j<i; j++) {
+			if(logs[i].ts - logs[j].ts > TIME_INTERVAL) log_count++;
+		}
+		if(i - log_count < MIN_COUNT) log_count = i;
+	}
+	*/
+
+	log_count = flashfetch_trace->pos;
+
+	reset_disk_trace(flashfetch_trace);
+	flashfetch_flags &= ~FF_INPROGRESS;
+
+	flashfetch_launch_ts = 0;
+
+	// process logs here
+	printk("S-ORIGINAL TRACE\n");
+	printk("log count: %d\n", log_count);
+	for(i=0; i<log_count; i++)
+	{
+		//if(logs[i].dev != 0) printk("%llu 0x%x %lu %llu %d\n", logs[i].ts, logs[i].dev, logs[i].ino, logs[i].blk_num, logs[i].blk_len);
+		//if(logs[i].dev == 0) printk("%d %llu 0x%x %lu %llu %d\n", i, logs[i].ts, logs[i].dev, logs[i].ino, logs[i].blk_num, logs[i].blk_len);
+	}
+	printk("E-ORIGINAL TRACE\n");
+	
+	if(logs) {
+		/*
+		write_disk_trace_raw("ORG", logs, log_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 16;
+		io_infill_distance_merge_onoff = 0;
+		io_infill_distance_allowed = 0;
+		io_infill_hole_allowed_blks = 0;
+		write_disk_trace_sched_flash("MSS64", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS64", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 32;
+		io_infill_distance_merge_onoff = 0;
+		io_infill_distance_allowed = 0;
+		io_infill_hole_allowed_blks = 0;
+		write_disk_trace_sched_flash("MSS128", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS128", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 16;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 4;
+		io_infill_hole_allowed_blks = 1;
+		write_disk_trace_sched_flash("MSS64_D4H4", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS64_D4H4", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 32;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 4;
+		io_infill_hole_allowed_blks = 1;
+		write_disk_trace_sched_flash("MSS128_D4H4", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS128_D4H4", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 16;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 4; 
+		io_infill_hole_allowed_blks = 2;
+		write_disk_trace_sched_flash("MSS64_D4H8", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS64_D4H8", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 32;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 4;
+		io_infill_hole_allowed_blks = 2;
+		write_disk_trace_sched_flash("MSS128_D4H8", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS128_D4H8", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 16;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 8;
+		io_infill_hole_allowed_blks = 1;
+		write_disk_trace_sched_flash("MSS64_D8H4", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS64_D8H4", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 32;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 8;
+		io_infill_hole_allowed_blks = 1;
+		write_disk_trace_sched_flash("MSS128_D8H4", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS128_D8H4", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 16;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 8;
+		io_infill_hole_allowed_blks = 2;
+		write_disk_trace_sched_flash("MSS64_D8H8", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS64_D8H8", trace_buffer, trace_count);
+
+		metadata_shift_onoff = 1;
+		metadata_shift_way = 1;
+		metadata_shift_value = 32;
+		io_infill_distance_merge_onoff = 1;
+		io_infill_distance_allowed = 8;
+		io_infill_hole_allowed_blks = 2;
+		write_disk_trace_sched_flash("MSS128_D8H8", logs, log_count, &trace_buffer, &trace_count);
+		write_disk_replayinfo_raw("MSS128_D8H8", trace_buffer, trace_count);
+		*/
+
+		//PARALFETCH
+		if(rotational_value != 0) 
+			write_disk_trace_sched_hard(param->appname, logs, log_count, &trace_buffer, &trace_count);
+		else
+			write_disk_trace_sched_flash(param->appname, logs, log_count, &trace_buffer, &trace_count);
+
+		write_disk_replayinfo_raw(param->appname, trace_buffer, trace_count);
+		kfree(logs);
+	}
+	
+	//complete_and_exit(NULL, 0);
+	return 0;
+	//return;
+}
+
+extern void sync_filesystems_ff(int wait);
+
+//DECLARE_DELAYED_WORK(seqmon_work, seqmon_thread);
+
+int flashfetch_genpf(char *appname, const char *filename)
+{
+	struct seqmon_param *params;
+	struct task_struct *t;
+
+	spin_lock(&flashfetch_lock);
+	if(flashfetch_flags & FF_INPROGRESS) {
+		spin_unlock(&flashfetch_lock);
+		return -EBUSY;
+	} 
+
+	flashfetch_flags |= FF_INPROGRESS;
+	spin_unlock(&flashfetch_lock);
+
+	// sync
+	wakeup_flusher_threads(0, WB_REASON_SYNC);
+	//sync_filesystems_ff(0);
+	sync_filesystems_ff(1);
+	//if (unlikely(laptop_mode))
+	//laptop_sync_completion();
+
+	// drop_caches
+	iterate_supers(drop_pagecache_sb, NULL);
+    	drop_slab();
+
+	flashfetch_monitor_blkio = 1;
+
+	printk("flashfetch_genpf: %s, filename : %s\n", appname, filename);
+
+	params = kmalloc(sizeof(struct seqmon_param), GFP_KERNEL);
+	if(params) {
+		strncpy(params->appname, appname, NAME_LEN);
+		strncpy(params->filename, filename, NAME_LEN);
+	} else {
+		printk(KERN_WARNING "Failed to allocate memory for kthread parameters\n");
+		return -ENOMEM;
+	}
+
+	/*
+	if(kernel_thread(seqmon_thread, (void *)params, 0) < 0) {
+		printk(KERN_WARNING "Failed to start async prefetch thread\n");
+		return -EINVAL;
+	}
+	*/
+	/*
+	p = kthread_create_on_node(seqmon_thread, (void *)params, cpu_to_node(smp_processor_id()), "seqmon_thread");
+
+	if(IS_ERR(p)) 
+		return PTR_ERR(p);
+
+	wake_up_process(p);
+	*/
+
+	t = kthread_create(seqmon_thread, (void *)params, "seqmon_thread");
+	if(IS_ERR(t)) {
+		return PTR_ERR(t);
+	}
+
+	wake_up_process(t);
+
+	/*
+	t = kthread_run(seqmon_thread, (void *)params, "paralfetch_monitor");
+	if(IS_ERR(t)) 
+		return PTR_ERR(t);
+	*/
+
+	return 0;
+}
+
+#ifdef CONFIG_DEBUG_FS
+#define MAX_NAME_LENGTH	256
+char appname[MAX_NAME_LENGTH];
+struct dentry *ff_fetch_app_file;
+//PARALFETCH
+struct dentry *ff_monctl_file;                                                 
+unsigned long ff_iomem_max = 0; 
+
+static ssize_t ff_fetch_app_write(struct file *filp, const char __user *buffer,
+		                                size_t count, loff_t *ppos)
+{
+#ifdef PARALFETCH_ANDROID
+	int ret;
+#endif
+	if (count >= MAX_NAME_LENGTH)
+		return -EINVAL;
+
+	if (copy_from_user(appname, buffer, count))
+		return -EFAULT;
+
+	if(appname[count-1] == '\n') appname[count-1] = '\0';
+	else appname[count] = '\0';
+
+#ifdef PARALFETCH_ANDROID
+	ret = flashfetch_async(appname);
+	if(ret) flashfetch_genpf(appname, appname);
+#else
+	flashfetch_sync(appname);
+#endif
+
+	return count;
+}
+
+//PARALFETCH
+#define MONCTL_ACCSEQ1 1                                                       
+#define MONCTL_ACCSEQ2 2                                                       
+#define MONCTL_REFTRC1 3                                                       
+#define MONCTL_REFTRC2 4                                                       
+#define MONCTL_PREFTRC 5
+#define MONCTL_MIN 1                                                           
+#define MONCTL_MAX 5                                                           
+                                                                               
+#define MONCTL_ACCSEQ_PROGRESS 1                                               
+#define MONCTL_REFTRC_PROGRESS 2                                               
+unsigned int monctl_in_progress = 0;                                           
+unsigned int odd_referenced_meta = 0;                                          
+unsigned int even_referenced_meta = 0;                                         
+unsigned int odd_referenced_file = 0;                                          
+unsigned int even_referenced_file = 0;
+
+extern resource_size_t find_last_iomem_res(char *name,
+                                      bool first_level_children_only);
+
+extern int walk_system_ram_res_all(void *arg, int (*func)(u64, u64, void *));
+
+int print_system_ram(u64 start, u64 end, void *arg)
+{
+	printk("System RAM: 0x%llx - 0x%llx\n", start, end);
+	return 0;
+}
+
+int set_page_unreferenced(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PageReferenced(page_ptr)) {
+				ClearPageReferenced(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+void print_page_cache_page(struct page *page)
+{
+	int lru;
+	unsigned long inum;
+	unsigned long offset;
+	struct inode *host;
+	unsigned int dev;
+
+	lru = page_lru(page);
+	if(is_file_lru(lru)) {
+		offset = page->index;
+		if(page->mapping && (((unsigned long)page->mapping & 3) == 0) && page->mapping->host) {
+			host = page->mapping->host;
+			inum = host->i_ino;
+			if(inum != 0) {
+				if(host->i_sb != NULL)
+					dev = host->i_sb->s_dev;
+				else
+					dev = 0;
+			} else
+				dev = host->i_rdev;
+		} else {
+			inum = 0;
+			dev = 0;
+		}
+	} else {
+		offset = 0;
+		inum = 0;
+		dev = 0;
+	}
+
+	if(dev) printk("-PCP- dev: 0x%x, inum: %lu, offset: %lu\n",
+			dev, inum, offset);
+}
+
+int log_page_referenced(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PageReferenced(page_ptr)) {
+				//                             if(l%100 == 0) print_page_cache_page(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+int log_page_prefetched(u64 start, u64 end, void *arg)
+{
+	unsigned long pg_end;
+	struct page *page_ptr;
+	unsigned int ref_count;
+	unsigned long l;
+	int lru;
+
+	ref_count = 0;
+	pg_end = (end + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for(l = (start / PAGE_SIZE); l < pg_end; l++) {
+		if(pfn_valid(l)) {
+			page_ptr = pfn_to_page(l);
+
+			lru = page_lru(page_ptr);
+
+			if(!is_file_lru(lru))
+				continue;
+
+			if(page_ptr && PagePrefetch(page_ptr)) {
+				//                             if(l%100 == 0) print_page_cache_page(page_ptr);
+				print_page_cache_page(page_ptr);
+				ref_count++;
+			}
+		}
+	}
+	if(arg) {
+		printk("Paralfetch: range: 0x%llx - 0x%llx, cur_tot: %u, ref_count: %u\n", start, end, *(unsigned int *)arg, ref_count);
+		*((unsigned int *)arg) += ref_count;
+	}
+	return 0;
+}
+
+static ssize_t ff_monctl_write(struct file *filp, const char __user *buffer,
+                                               size_t count, loff_t *ppos)
+{
+#define MAX_NUM_LENGTH 10
+	char monstr[MAX_NUM_LENGTH];
+	long monctl;
+	unsigned int ref_count;
+
+	if (count >= MAX_NUM_LENGTH)
+		return -EINVAL;
+
+	if (copy_from_user(monstr, buffer, count))
+		return -EFAULT;
+
+	if(monstr[count-1] == '\n') monstr[count-1] = '\0';
+	else monstr[count] = '\0';
+
+
+	monctl = simple_strtol(monstr, NULL, 10);
+	if(monctl >= MONCTL_MIN && monctl <= MONCTL_MAX) {
+		switch(monctl) {
+			case MONCTL_ACCSEQ1:
+				monctl_in_progress = MONCTL_ACCSEQ_PROGRESS;
+				break;
+			case MONCTL_ACCSEQ2:
+				monctl_in_progress = 0;
+				break;
+			case MONCTL_REFTRC1:
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				odd_referenced_meta = 0;
+				even_referenced_meta = 0;
+				odd_referenced_file = 0;
+				even_referenced_file = 0;
+				walk_system_ram_res_all(&ref_count, set_page_unreferenced);
+				//walk_system_ram_res_all(NULL, print_system_ram);
+				printk("Paralfetch ref_count: %u\n", ref_count);
+				monctl_in_progress = MONCTL_REFTRC_PROGRESS;
+				break;
+			case MONCTL_REFTRC2:
+				monctl_in_progress = 0;
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				walk_system_ram_res_all(&ref_count, log_page_referenced);
+				//walk_system_ram_res_all(NULL, print_system_ram);
+				printk("Paralfetch ref_count: %u, odd_referenced_meta: %u, even_referenced_meta: %u, odd_referenced_file: %u, even_referenced_file: %u\n", ref_count, odd_referenced_meta, even_referenced_meta, odd_referenced_file, even_referenced_file);
+				break;
+			case MONCTL_PREFTRC:
+				monctl_in_progress = 0;
+				printk("Paralfetch MONCTL: %ld\n", monctl);
+				ref_count = 0;
+				walk_system_ram_res_all(&ref_count, log_page_prefetched);
+				printk("Paralfetch prefetched count: %u\n", ref_count);
+				break;
+			default:
+				break;
+		}
+	}
+
+	return count;
+}
+//PARALFETCH END
+
+static const struct file_operations ff_fetch_app_fops = {
+	.owner =        THIS_MODULE,
+	.open =         simple_open,
+	.write =        ff_fetch_app_write,
+	.llseek =       noop_llseek,
+};
+
+//PARALFETCH
+static const struct file_operations ff_monctl_fops = {
+       .owner =        THIS_MODULE,
+       .open =         simple_open,
+       .write =        ff_monctl_write,
+       .llseek =       noop_llseek,
+};
+#endif
+
+static int __init init_flashfetch(void)
+{
+	int ret;
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *root = debugfs_create_dir("flashfetch", NULL);
+	if (root == NULL)
+		return -ENXIO;
+	ff_fetch_app_file = debugfs_create_file("fetch_app", S_IWUGO, root, NULL, &ff_fetch_app_fops);
+
+	//PARALFETCH
+	ff_monctl_file = debugfs_create_file("monctl", S_IWUGO, root, NULL, &ff_monctl_fops);
+
+	debugfs_create_u32("flashfetch_monitor_blkio", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_blkio);
+	debugfs_create_u32("flashfetch_monitor_rq_complete", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_rq_complete);
+	debugfs_create_u32("flashfetch_monitor_timeout", S_IRUGO|S_IWUGO, root, &flashfetch_monitor_timeout);
+	debugfs_create_u64("evaluator_lba", S_IRUGO|S_IWUGO, root, &evaluator_lba);
+	debugfs_create_u32("io_distance_allowed", S_IRUGO|S_IWUGO, root, &io_distance_allowed);
+	debugfs_create_u32("io_infill_distance_allowed", S_IRUGO|S_IWUGO, root, &io_infill_distance_allowed);
+	debugfs_create_u32("io_infill_hole_allowed_blks", S_IRUGO|S_IWUGO, root, &io_infill_hole_allowed_blks);
+	debugfs_create_u32("metadata_shift_way", S_IRUGO|S_IWUGO, root, &metadata_shift_way);
+	debugfs_create_u32("metadata_shift_value", S_IRUGO|S_IWUGO, root, &metadata_shift_value);
+
+	//PARALFETCH
+	debugfs_create_u32("rotational", S_IRUGO|S_IWUGO, root, &rotational_value);
+	debugfs_create_u32("pfault_trace", S_IRUGO|S_IWUGO, root, &pfault_trace);
+	debugfs_create_u32("pfault_debug", S_IRUGO|S_IWUGO, root, &pfault_debug);
+	debugfs_create_u64("flashfetch_readahead_hit", S_IRUGO, root,
+			&flashfetch_readahead_hit);
+	debugfs_create_u64("flashfetch_prefetch_cnt", S_IRUGO, root,
+			&flashfetch_prefetch_cnt);
+	debugfs_create_u64("flashfetch_prefetch_hit", S_IRUGO, root,
+			&flashfetch_prefetch_hit);
+#endif
+
+	flashfetch_lock = __SPIN_LOCK_UNLOCKED(flashfetch_lock);
+	ret = alloc_disk_trace(&flashfetch_trace);
+	if(ret < 0) flashfetch_trace = NULL;
+
+	return 0;
+}
+
+static void __exit exit_flashfetch(void)
+{
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(ff_fetch_app_file);
+	//PARALFETCH
+	debugfs_remove(ff_monctl_file);
+#endif
+	return ;
+}
+
+core_initcall(init_flashfetch);
+module_exit(exit_flashfetch);
+MODULE_LICENSE("DUAL BSD/GPL");
diff -uNr linux-4.12.9/fs/flashfetch_evaluate.c linux-4.12.9-pf/fs/flashfetch_evaluate.c
--- linux-4.12.9/fs/flashfetch_evaluate.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_evaluate.c	2019-01-02 21:24:54.309591885 +0900
@@ -0,0 +1,76 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/buffer_head.h>
+#include <linux/gfp.h>
+#include "ext4/ext4.h"
+
+unsigned long long int evaluator_lba = 0;
+EXPORT_SYMBOL(evaluator_lba);
+
+void __invalidate_pagecache_entry(struct address_space *mapping, pgoff_t start, pgoff_t end);
+
+void __invalidate_buffercache_entry(struct block_device *bdev, sector_t block, unsigned size)
+{
+	struct buffer_head *bh;
+	pgoff_t index;
+	struct page *page;
+	
+	bh = __find_get_block(bdev, block, size);
+
+	if(bh) {
+		printk("bh ok!\n");
+		lock_buffer(bh);
+		clear_buffer_uptodate(bh);
+		unlock_buffer(bh);
+		put_bh(bh);
+	}
+	if(bdev && bdev->bd_inode) {
+		printk("bdev->bd_inode ok!\n");
+		index = block >> (PAGE_SHIFT - bdev->bd_inode->i_blkbits);
+		if(bdev->bd_inode->i_mapping) {
+			page = find_get_page(bdev->bd_inode->i_mapping, index);
+			clear_bit(PG_uptodate, &page->flags);
+			//page_cache_release(page);
+		}
+		//__invalidate_pagecache_entry(bdev->bd_inode->i_mapping, block, block-1+(size/4096));
+	}
+}
+
+void __invalidate_pagecache_entry(struct address_space *mapping, pgoff_t start, pgoff_t end)
+{
+	if(end >= start)
+		invalidate_mapping_pages(mapping, start, end);
+}
+
+void invalidate_buffercache_entry(const char *pathname, sector_t block, unsigned size)
+{
+	struct block_device *bdev;
+	
+	bdev = lookup_bdev(pathname);
+	if(bdev) 
+		__invalidate_buffercache_entry(bdev, block, size);
+}
+EXPORT_SYMBOL(invalidate_buffercache_entry);
+
+void invalidate_pagecache_entry(const char *pathname, unsigned long ino, pgoff_t start, pgoff_t end)
+{
+	struct super_block *sb;
+	struct block_device *bdev;
+	struct inode *inode;
+	
+	bdev = lookup_bdev(pathname);
+	if(!bdev) return;
+
+	sb = get_super(bdev);
+	if(!sb) return;
+
+	inode = ext4_iget(sb, ino);
+	if(inode) {
+		__invalidate_pagecache_entry(inode->i_mapping, start, end);
+		iput(inode);
+	}
+
+	if(sb) drop_super(sb);
+}
+EXPORT_SYMBOL(invalidate_pagecache_entry);
diff -uNr linux-4.12.9/fs/flashfetch_fetch.c linux-4.12.9-pf/fs/flashfetch_fetch.c
--- linux-4.12.9/fs/flashfetch_fetch.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_fetch.c	2019-01-02 21:24:54.147590967 +0900
@@ -0,0 +1,593 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/fs.h>
+#include <linux/file.h>
+
+#include <linux/sched.h>
+#include <linux/cred.h>
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+#include "internal.h"
+//#include <linux/ext2_fs.h>
+#include "ext4/ext4.h"
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/kthread.h>
+#include <linux/flashfetch_trace.h>
+#include <linux/flashfetch_fetch.h>
+#include <linux/flashfetch_scheduler.h>
+
+extern ssize_t kernel_write(struct file *file, const char *buf, size_t count, loff_t pos);
+int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count);
+int flashfetch_load_trace_from_file(char *filename, replay_info **trace_pptr, int *trace_cnt);
+int flashfetch_do_prefetcher(replay_info *trace_ptr, int trace_cnt);
+void print_paralfetch_header(paralfetch_header *header);
+int flashfetch_do_async_thread(void *params);
+
+//PARALFETCH
+//state, 0:nothing, 1:in prefetching, 2: waiting
+int prefetch_state = 0;
+
+extern int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct scheduler_entry **sched_last);
+
+extern struct file *get_empty_filp(void);
+//extern struct inode *ext3_iget(struct super_block *sb, unsigned long ino);
+
+void print_paralfetch_header(paralfetch_header *header)
+{
+	if(!header) return;
+	printk("-- header --\n");
+	printk("magic code : %c%c%c%c%c%c%c%c\n", header->magic[0], header->magic[1], header->magic[2], header->magic[3], header->magic[4], header->magic[5], header->magic[6], header->magic[7]);
+	printk("version : %u\n", header->version);
+	printk("total io count : %u\n", header->io_count);
+	printk("uncertain io count : %u\n", header->uncertain_io_count);
+	printk("uncertain io size : %u\n", header->uncertain_io_size);
+	printk("harmful io count : %u\n", header->harmful_io_count);
+	printk("harmful io size : %u\n", header->harmful_io_size);
+	printk("private : %u\n", header->private);
+}
+
+typedef struct _async_params {
+	replay_info *trace_ptr;
+	int trace_cnt;
+} async_params;
+
+int flashfetch_prefetch_sync(char *app_name)
+{
+	char app_path[NAME_LEN + 1];
+	paralfetch_header *header;
+	replay_info *trace_ptr;
+	int trace_cnt;
+	int ret;
+	unsigned long long nstime;
+	async_params *params;
+
+	nstime = ktime_to_ns(ktime_get());
+	
+	printk(KERN_NOTICE "PS %llu %s\n", nstime, app_name);
+
+	prefetch_state = 1;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", app_name);
+
+	ret = flashfetch_load_trace_from_file(app_path, (replay_info **)&header, &trace_cnt);
+	if(ret < 0) return ret;
+
+	print_paralfetch_header(header);
+
+	printk(KERN_NOTICE "OKOK!:%u, %d\n", header->private, header->private);
+
+	trace_ptr = (replay_info *)(&(header->trace_data));
+
+	header->private = 0;
+
+	if(header->private == 0) {
+		printk(KERN_NOTICE "PRIVATE=0\n");
+		flashfetch_do_prefetcher(trace_ptr, trace_cnt);
+	} else if(header->private < (unsigned short)trace_cnt) {
+		printk(KERN_NOTICE "PRIVATE>0:%d\n", header->private);
+		flashfetch_do_prefetcher(trace_ptr, header->private);
+		params = kmalloc(sizeof(async_params), GFP_KERNEL);
+		if(params == NULL) {
+			kfree(trace_ptr);
+			return -ENOMEM;
+		}
+
+		params->trace_ptr = trace_ptr + header->private;
+		params->trace_cnt = trace_cnt - header->private;
+
+		if(kernel_thread(flashfetch_do_async_thread, (void *)params, 0) < 0) {
+			printk(KERN_WARNING "Failed to start async prefetch thread\n");
+			kfree(trace_ptr);
+			return -EINVAL;
+		}
+	}
+
+	kfree(trace_ptr);
+
+	nstime = ktime_to_ns(ktime_get());
+	
+	prefetch_state = 0;
+
+	printk(KERN_NOTICE "PE %llu %s\n", nstime, app_name);
+
+	return 0;
+}
+
+int flashfetch_do_async_thread(void *params)
+{
+	async_params *params_tmp = (async_params *)params;
+
+	printk(KERN_CRIT "ASYNC THREAD START\n");
+
+	prefetch_state = 1;
+
+	flashfetch_do_prefetcher(params_tmp->trace_ptr, params_tmp->trace_cnt);
+
+	prefetch_state = 0;
+
+	printk(KERN_CRIT "ASYNC THREAD END\n");
+
+	kfree((void *)container_of((void *)(params_tmp->trace_ptr), struct __paralfetch_header, trace_data));
+	// kfree(params_tmp->trace_ptr);
+	kfree(params_tmp);
+
+	return 0;
+}
+
+int flashfetch_prefetch_async(char *app_name)
+{
+	char app_path[NAME_LEN + 1];
+	paralfetch_header *header;
+	replay_info *trace_ptr;
+	int trace_cnt;
+	async_params *params;
+	int ret;
+	struct task_struct *prefetch_task;
+
+	params = kmalloc(sizeof(async_params), GFP_KERNEL);
+	if(params == NULL) return -ENOMEM;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", app_name);
+
+	ret = flashfetch_load_trace_from_file(app_path, (replay_info **)&header, &trace_cnt);
+	trace_ptr = (replay_info *)(&(header->trace_data));
+	//ret = flashfetch_load_trace_from_file(app_path, &trace_ptr, &trace_cnt);
+	if(ret < 0) {
+		kfree(params);
+		return ret;
+	}
+
+	print_paralfetch_header(header);
+
+	params->trace_ptr = trace_ptr;
+	params->trace_cnt = trace_cnt;
+
+	prefetch_task = kthread_create(flashfetch_do_async_thread, (void *)params, "pf_task");
+	if(IS_ERR(prefetch_task)){
+		printk("Unable to start prefetch thread.\n");
+	} else
+		wake_up_process(prefetch_task);
+	/*
+	   if(kernel_thread(flashfetch_do_async_thread, (void *)params, 0) < 0) {
+	   printk(KERN_CRIT "Failed to start async prefetch thread\n");
+	   return -EINVAL;
+	   }
+	   */
+	printk(KERN_CRIT "Succeeded to start async prefetch thread\n");
+
+	return 0;
+}
+
+// Read permission of flashfetch trace file must be allowed to every user.
+struct file *kernel_open(char const *file_name, int flags, int mode)
+{
+	struct file *file = NULL;
+#if BITS_PER_LONG != 32
+	flags |= O_LARGEFILE;
+#endif
+
+	file = filp_open(file_name, flags, mode);
+
+	return file;
+}
+
+int kernel_close(struct file *file)
+{
+	if(file->f_op && file->f_op->flush) {
+		file->f_op->flush(file, current->files);
+	}
+	fput(file);
+
+	return 0;
+}
+
+int write_disk_replayinfo_raw(char *appname, replay_info *trace_buffer, int trace_count)
+{
+	struct file *file;
+	//int i;
+	int ret;
+	loff_t data_write;
+	loff_t file_size;
+	char app_path[NAME_LEN + 1];
+	paralfetch_header header;
+
+	if(!trace_count) return -ENOENT;
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", appname);
+
+	file = kernel_open(app_path, O_RDWR | O_CREAT | O_TRUNC, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for writing. errno=%d\n", appname, ret);
+		return ret;
+	}
+
+	header.magic[0] = 'P';
+	header.magic[1] = 'A';
+	header.magic[2] = 'R';
+	header.magic[3] = 'F';
+	header.magic[4] = 'E';
+	header.magic[5] = 'T';
+	header.magic[6] = 'C';
+	header.magic[7] = 'H';
+	header.version = 1;
+	header.io_count = trace_count;
+	header.uncertain_io_count = 0;
+	header.uncertain_io_size = 0;
+	header.harmful_io_count = 0;
+	header.harmful_io_size = 0;
+	header.private = 0;
+
+	ret = kernel_write(file, (char *)&header, sizeof(paralfetch_header), 0);
+	printk("header write size : %d\n", ret);
+
+	if(ret < 0 || ret == 0) {
+		printk("Error while writing from file %s, error=%d\n", appname, ret);
+		goto out_close_free;
+	}
+
+	data_write = 0;
+	file_size = sizeof(replay_info) * trace_count;
+
+	while(data_write < file_size) {
+		ret = kernel_write(file, (char *)trace_buffer + data_write, file_size - data_write, data_write + sizeof(paralfetch_header));
+		printk("trace write size : %d\n", ret);
+
+		if(ret < 0) {
+			printk("Error while writing from file %s, error=%d\n", appname, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data write=%lld, expected size=%lld\n", data_write, file_size);
+			goto out_close_free;
+		}
+		data_write += ret;
+	}
+
+	if(trace_buffer) kfree(trace_buffer);
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+	//out_close:
+	kernel_close(file);
+	return ret;
+}
+
+int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count)
+{
+	struct file *file;
+	paralfetch_header header;
+	replay_info *trace_buffer;
+	int i;
+	int ret;
+	loff_t data_write;
+	loff_t file_size;
+	char app_path[NAME_LEN + 1];
+
+	memset(app_path, '\0', sizeof(app_path));
+	sprintf(app_path, "/flashfetch/%s.pf", appname);
+
+	file = kernel_open(app_path, O_RDWR | O_CREAT | O_TRUNC, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for writing. errno=%d\n", appname, ret);
+		return ret;
+	}
+
+	header.magic[0] = 'P';
+	header.magic[1] = 'A';
+	header.magic[2] = 'R';
+	header.magic[3] = 'F';
+	header.magic[4] = 'E';
+	header.magic[5] = 'T';
+	header.magic[6] = 'C';
+	header.magic[7] = 'H';
+	header.version = 1;
+	header.io_count = log_count;
+	header.uncertain_io_count = 0;
+	header.uncertain_io_size = 0;
+	header.harmful_io_count = 0;
+	header.harmful_io_size = 0;
+	header.private = 0;
+
+	ret = kernel_write(file, (char *)&header, sizeof(paralfetch_header), 0);
+	printk("header write size : %d\n", ret);
+
+	if(ret < 0 || ret == 0) {
+		printk("Error while writing from file %s, error=%d\n", appname, ret);
+		goto out_close;
+	}
+
+	trace_buffer = kmalloc(sizeof(replay_info) * log_count, GFP_KERNEL);
+	if(!trace_buffer) {
+		ret = -ENOMEM;
+		goto out_close;
+	}
+
+	for(i=0; i<log_count; i++)
+	{
+		trace_buffer[i].dev = logs[i].dev;
+		trace_buffer[i].ino = logs[i].ino;
+		trace_buffer[i].start = logs[i].blk_num;
+		trace_buffer[i].length = logs[i].blk_len;
+	}
+
+	data_write = 0;
+	file_size = sizeof(replay_info) * log_count;
+
+	while(data_write < file_size) {
+		ret = kernel_write(file, (char *)trace_buffer + data_write, file_size - data_write, data_write + sizeof(paralfetch_header));
+		printk("trace write size : %d\n", ret);
+
+		if(ret < 0) {
+			printk("Error while writing from file %s, error=%d\n", appname, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data write=%lld, expected size=%lld\n", data_write, file_size);
+			break;
+		}
+		data_write += ret;
+	}
+
+	if(trace_buffer) kfree(trace_buffer);
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+out_close:
+	kernel_close(file);
+	return ret;
+
+}
+
+int flashfetch_load_trace_from_file(char *filename, replay_info **trace_pptr, int *trace_cnt)
+{
+	struct file *file;
+	replay_info *trace_buffer = NULL;
+	char *trace_cbuf;
+	int ret = 0;
+	int file_size;
+	int data_read = 0;
+
+	file = kernel_open(filename, O_RDONLY, 0644);
+
+	if(IS_ERR(file)) {
+		ret = (int)PTR_ERR(file);
+		printk("Cannot open file %s for reading. errno=%d\n", filename, ret);
+		return ret;
+	}
+	
+	file_size = file->f_mapping->host->i_size;
+
+	trace_buffer = kmalloc(file_size, GFP_KERNEL);
+	if(!trace_buffer) {
+		ret = -ENOMEM;
+		goto out_close;
+	}
+
+	trace_cbuf = (char *)trace_buffer;
+
+	while(data_read < file_size) {
+		ret = kernel_read(file, data_read, trace_cbuf + data_read, file_size - data_read);
+
+		if(ret < 0) {
+			printk("Error while reading from file %s, error=%d\n", filename, ret);
+			goto out_close_free;
+		}
+		if(ret == 0) {
+			printk("File too short, data read=%d, expected size=%d\n", data_read, file_size);
+			break;
+		}
+		data_read += ret;
+	}
+
+	if(data_read == file_size) {
+		*trace_pptr = trace_buffer;
+		*trace_cnt = (file_size-sizeof(paralfetch_header))/sizeof(replay_info);
+		//printk("file_size=%d, sizeof(replay_info)=%d\n", file_size, sizeof(replay_info));
+	} else {
+		printk("Trace file size changed beneath us, cancelling read\n");
+		ret = -ETXTBSY;
+		goto out_close_free;
+	}
+	kernel_close(file);
+	return 0;
+
+out_close_free:
+	if(trace_buffer) kfree(trace_buffer);
+out_close:
+	kernel_close(file);
+	return ret;
+}
+
+extern unsigned long long int flashfetch_prefetch_cnt;
+
+int flashfetch_do_prefetcher(replay_info *trace_ptr, int trace_cnt)
+{
+	int i, ret = 0;
+	int j;
+	int end;
+	struct block_device *bdev_prev = NULL;
+	dev_t dev_prev = 0;
+	unsigned int bs_prev;
+	replay_info *trace_curr = NULL;
+	struct file *file_prev = NULL;
+	struct inode *ino_prev = NULL;
+	struct super_block *sb = NULL;
+
+	struct blk_plug bh_plug;
+
+	//struct buffer_head *bh;
+
+	if(trace_cnt == 0 || trace_ptr == NULL) return 0;
+
+	trace_curr = trace_ptr;
+
+	printk("trace count : %d\n", trace_cnt);
+
+	sb = user_get_super(trace_curr->dev);
+
+	if(!sb) {
+		return -ENODEV;
+	}
+
+	dev_prev = trace_curr->dev;
+	bdev_prev = sb->s_bdev;
+	bs_prev = 1 << sb->s_blocksize_bits;
+
+	for(i=0; i<trace_cnt; i++, trace_curr++)
+	{
+		if(trace_curr->ino == 0) {
+			if(likely(dev_prev == trace_curr->dev)) {
+				// printk("1) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				end = (trace_curr->length+bs_prev-1)/bs_prev;
+				if(end > 1) {
+					blk_start_plug(&bh_plug);
+					for(j=0; j<end; j++)
+					{
+						// printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start+j, bs_prev);
+						__breadahead(sb->s_bdev, trace_curr->start+j, bs_prev);
+						//__bread(sb->s_bdev, trace_curr->start+j, bs_prev);
+					}
+					blk_finish_plug(&bh_plug);
+				} else __breadahead(sb->s_bdev, trace_curr->start, bs_prev);
+				//} else __bread(sb->s_bdev, trace_curr->start, bs_prev);
+			} else {
+				// printk("1) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				if(sb) drop_super(sb);
+				sb = user_get_super(trace_curr->dev);
+
+				if(!sb) {
+					if(ino_prev) iput(ino_prev);
+					if(file_prev) put_filp(file_prev);
+					return -ENOENT;
+				}
+
+				dev_prev = trace_curr->dev;
+				bdev_prev = sb->s_bdev;
+				bs_prev = 1 << sb->s_blocksize_bits;
+
+				// printk("2) dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, trace_curr->length);
+				end = (trace_curr->length+bs_prev-1)/bs_prev;
+
+				if(end > 1) {
+					blk_start_plug(&bh_plug);
+					for(j=0; j<((trace_curr->length+bs_prev-1)/bs_prev); j++)
+					{
+						//printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start+j, bs_prev);
+						__breadahead(sb->s_bdev, trace_curr->start+j, bs_prev);
+						//__bread(sb->s_bdev, trace_curr->start+j, bs_prev);
+					}
+					blk_finish_plug(&bh_plug);
+				} else 
+					//printk("* dev:0x%x, start:%llu, length:%u\n", trace_curr->dev, trace_curr->start, bs_prev);
+					__breadahead(sb->s_bdev, trace_curr->start, bs_prev);
+					//__bread(sb->s_bdev, trace_curr->start, bs_prev);
+			}
+		} else {
+			if(dev_prev != trace_curr->dev || !ino_prev || ino_prev->i_ino != trace_curr->ino) {
+				if(dev_prev != trace_curr->dev) {
+					if(sb) drop_super(sb);
+					sb = user_get_super(trace_curr->dev);
+
+					if(!sb) {
+						if(ino_prev) iput(ino_prev);
+						if(file_prev) put_filp(file_prev);
+						return -ENOENT;
+					}
+
+					dev_prev = trace_curr->dev;
+					bdev_prev = sb->s_bdev;
+					bs_prev = 1 << sb->s_blocksize_bits;
+				}
+
+				if(ino_prev) iput(ino_prev);
+
+				//ino_prev = ext3_iget(sb, trace_curr->ino);
+				ino_prev = ext4_iget(sb, trace_curr->ino);
+
+				if(IS_ERR_OR_NULL(ino_prev)) {
+					ino_prev = NULL;
+
+					continue;
+				}
+
+				if(file_prev) put_filp(file_prev);
+
+				file_prev = get_empty_filp();
+
+				if(file_prev == NULL) continue;
+
+				file_prev->f_op = ino_prev->i_fop;
+				file_prev->f_mapping = ino_prev->i_mapping;
+				file_prev->f_mode = FMODE_READ;
+				file_prev->f_flags = O_RDONLY;
+			}
+
+			if(ino_prev == NULL) continue;
+
+			//printk("data readahead %lu (%llu, %d);\n", trace_curr->ino, trace_curr->start, trace_curr->length);
+			
+			ret = force_page_cache_readahead(ino_prev->i_mapping, file_prev, (pgoff_t)trace_curr->start, (unsigned long)trace_curr->length, 1);
+
+			//PARALFETCH
+			flashfetch_prefetch_cnt++;
+
+			io_schedule(); //newly-added
+
+			if(ret < 0) {
+				printk("failed: ino : %lu (%llu, %d);\n", trace_curr->ino, trace_curr->start, trace_curr->length);
+			}
+		}
+	}
+
+	if(ino_prev) iput(ino_prev);
+	if(file_prev) put_filp(file_prev);
+	if(sb) drop_super(sb);
+
+	return i;
+}
diff -uNr linux-4.12.9/fs/flashfetch_scheduler.c linux-4.12.9-pf/fs/flashfetch_scheduler.c
--- linux-4.12.9/fs/flashfetch_scheduler.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_scheduler.c	2019-01-02 21:24:54.218591369 +0900
@@ -0,0 +1,1156 @@
+/*
+ * 0926: update rb_tree search algorithm
+ */
+#include <asm/fcntl.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/stat.h>
+#include <linux/proc_fs.h>
+#include <linux/buffer_head.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/rbtree.h>
+#include <linux/flashfetch_scheduler.h>
+#include <linux/flashfetch.h>
+#include <linux/flashfetch_trace.h>
+
+#include "ext4/ext4.h"
+
+//PARALFETCH
+extern struct super_block *user_get_super(dev_t dev);
+
+int blocksize_dev(dev_t dev)
+{
+	struct super_block *sb = NULL;
+	int bs;
+
+	sb = user_get_super(dev);
+
+	if(!sb) {
+		return -ENODEV;
+	}
+
+	bs = 1 << sb->s_blocksize_bits;
+
+	if(sb) drop_super(sb);
+
+	return bs;
+}
+
+struct scheduler_entry *ff_rb_search(struct rb_root *root, dev_t dev, u64 pblk_num)
+{
+	struct rb_node *node = root->rb_node;
+	struct scheduler_entry *curr;
+
+	while(node)
+	{
+		curr = rb_entry(node, struct scheduler_entry, rb_lba);
+
+		if(dev < curr->dev) {
+			node = node->rb_left;
+		} else if(dev == curr->dev) {
+			if(pblk_num < curr->pblk_num)
+				node = node->rb_left;
+			else if(pblk_num == curr->pblk_num)
+				return curr;
+			else
+				node = node->rb_right;
+		} else {
+			node = node->rb_right;
+		}
+	}
+	return NULL;
+}
+
+struct scheduler_entry *__ff_rb_insert_lba(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct rb_node **p =  &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct scheduler_entry *curr;
+	dev_t dev = new->dev;
+	u64 pblk_num = new->pblk_num;
+
+	while(*p)
+	{
+		parent = *p;
+		curr = rb_entry(parent, struct scheduler_entry, rb_lba);
+
+		if(dev < curr->dev) {
+			p = &(*p)->rb_left;
+		} else if(dev == curr->dev) {
+			if(pblk_num < curr->pblk_num)
+				p = &(*p)->rb_left;
+			else if(pblk_num == curr->pblk_num) 
+				return curr;
+			else
+				p = &(*p)->rb_right;
+		} else {
+			p = &(*p)->rb_right;
+		}
+	}
+	rb_link_node(&(new->rb_lba), parent, p);
+
+	return NULL;
+}
+
+void ff_rb_erase_lba(struct rb_node *victim, struct rb_root *tree)
+{
+	rb_erase(victim, tree);
+}
+
+void ff_rb_erase_lba_del(struct rb_node *victim, struct rb_root *tree)
+{
+	struct scheduler_entry *victim_str;
+	victim_str = rb_entry(victim, struct scheduler_entry, rb_lba);
+
+	rb_erase(victim, tree);
+
+	kfree(victim_str);
+}
+
+struct scheduler_entry *prev_entry(struct scheduler_entry *entry)
+{
+	struct rb_node *node;
+
+	node = rb_prev(&(entry->rb_lba));
+	if(node != NULL)
+		return rb_entry(node, struct scheduler_entry, rb_lba);
+	else
+		return NULL;
+}
+
+struct scheduler_entry *next_entry(struct scheduler_entry *entry)
+{
+	struct rb_node *node;
+
+	node = rb_next(&(entry->rb_lba));
+	if(node != NULL)
+		return rb_entry(node, struct scheduler_entry, rb_lba);
+	else
+		return NULL;
+}
+
+#define FF_END		1
+#define FF_FIND		2
+
+//HERE
+struct scheduler_entry *ff_rb_insert_lba(struct rb_root *root, struct scheduler_entry *new)
+{
+	u64 r_blk_end;
+	struct scheduler_entry *ret = NULL;
+	struct scheduler_entry *prev = NULL;
+	struct scheduler_entry *curr = NULL;
+	struct scheduler_entry *next = NULL;
+
+	r_blk_end = new->blk_num + new->blk_len - 1;
+
+	// the request size of buffered page cache is the size of FS block.
+	ret = __ff_rb_insert_lba(root, new);
+
+	if(new->ino == 0) {
+		if(ret != NULL) {
+			return ret;
+		} else {
+			rb_insert_color(&(new->rb_lba), root);
+			return NULL;
+		}
+	}
+
+	if(ret == NULL) { // start offset no-conflict
+		rb_insert_color(&(new->rb_lba), root);
+
+		prev = prev_entry(new);
+		next = next_entry(new);
+
+		if(!(prev && prev->dev == new->dev && prev->ino == new->ino && prev->pblk_num+prev->blk_len >= new->pblk_num))
+			prev = NULL;
+		if(!(next && next->dev == new->dev && next->ino == new->ino && next->pblk_num < new->pblk_num+new->blk_len))
+			next = NULL;
+
+		if(prev == NULL) {
+			if(next == NULL) { // case 1: no conflict at all
+				printk("NO-CONFLICT\n");
+				return NULL;
+			} else { // next != NULL
+			      	 // case 2: subset of prev node
+				printk("SUBSET of prev, new i:%lu, (%llu, %u), next i:%lu, (%llu, %u) --> new->blk_len %u\n", 
+						new->ino, new->blk_num, new->blk_len, next->ino, next->blk_num, next->blk_len,
+					(unsigned int)(next->pblk_num - new->pblk_num));
+				new->blk_len = (unsigned int)(next->pblk_num - new->pblk_num);
+				curr = next;
+				ret = NULL;
+			}
+		} else { // prev != NULL
+			if(next == NULL) {
+				prev->blk_len = (unsigned int)(new->blk_len + (unsigned int)(new->blk_num - prev->blk_num));
+				rb_erase(&(new->rb_lba), root);
+				return prev; // case 3: extension of prev node
+			} else { // next != NULL
+				prev->blk_len = (unsigned int)(next->pblk_num - prev->pblk_num);
+				curr = next;
+				rb_erase(&(new->rb_lba), root);
+				ret = prev;
+			}
+		}
+	} else {
+		curr = ret;
+	}
+	next = next_entry(curr);
+
+	while(next != NULL && curr->dev == next->dev && curr->ino == next->ino && (r_blk_end >= next->blk_num - 1))
+	{
+		printk("EXTEND2 r_blk_end: %llu, i:%lu, %u to %u\n", r_blk_end, curr->ino, curr->blk_len, (unsigned int)(next->blk_num - curr->blk_num));
+		curr->blk_len = (unsigned int)(next->pblk_num - curr->pblk_num);
+		curr = next;
+		next = next_entry(next);
+	}
+
+	if(new->dev == curr->dev && new->ino == curr->ino && curr->blk_num + curr->blk_len - 1 < r_blk_end) {
+		printk("EXTEND3 curr i:%lu, (%llu, %u), new i:%lu, (%llu, %u) --> curr->blk_len %u\n", 
+				curr->ino, curr->blk_num, curr->blk_len, new->ino, new->blk_num, new->blk_len,
+			(unsigned int)(r_blk_end - curr->blk_num + 1));
+		curr->blk_len = (unsigned int)(r_blk_end - curr->blk_num + 1);
+	}
+
+	return ret;
+}
+
+struct scheduler_entry *__ff_rb_insert_seq(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct rb_node **p =  &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct scheduler_entry *curr;
+
+	u64 ts = new->ts;
+
+	while(*p)
+	{
+		parent = *p;
+		curr = rb_entry(parent, struct scheduler_entry, rb_seq);
+
+		if(curr->ts > ts)
+			p = &(*p)->rb_left;
+		else
+			p = &(*p)->rb_right;
+	}
+
+	rb_link_node(&(new->rb_seq), parent, p);
+
+	return NULL;
+}
+
+struct scheduler_entry *ff_rb_insert_seq(struct rb_root *root, struct scheduler_entry *new)
+{
+	struct scheduler_entry *ret;
+	if((ret = __ff_rb_insert_seq(root, new)))
+		goto out;
+	rb_insert_color(&(new->rb_seq), root);
+	return ret;
+out:
+	//printk("already exist! %d, %d\n", new->blk_len, ret->blk_len);
+
+	if(new->blk_len > ret->blk_len) 
+		ret->blk_len = new->blk_len;
+	return ret;
+}
+
+void ff_rb_erase_seq(struct rb_node *victim, struct rb_root *tree)
+{
+	rb_erase(victim, tree);
+}
+
+void ff_rb_erase_seq_del(struct rb_node *victim, struct rb_root *tree)
+{
+	struct scheduler_entry *victim_str;
+	victim_str = rb_entry(victim, struct scheduler_entry, rb_seq);
+
+	rb_erase(victim, tree);
+
+	kfree(victim_str);
+}
+
+int io_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size //TBD
+
+		if((sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) != sched_entry->pblk_num) return 0;
+	} else {
+		if((sched_prev->pblk_num + sched_prev->blk_len) != sched_entry->pblk_num) return 0;
+	}
+
+	return 1;
+}
+
+extern int ext4_ext_walk_range(struct inode *inode, ext4_lblk_t block, ext4_lblk_t num, struct dependency_entry **dep_ptr);
+
+int io_distance_merge_onoff = 0;
+int io_distance_allowed = 10;
+
+int iodist_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int io_distance = 0;
+	struct scheduler_entry *sched_curr;
+	struct rb_node *rb_seq_curr;
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size
+
+		if((sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) != sched_entry->pblk_num) return 0;
+	} else {
+		if((sched_prev->pblk_num + sched_prev->blk_len) != sched_entry->pblk_num) return 0;
+	}
+
+	//printk("prev %llu, %llu, %u - curr %llu, %llu, %u\n", sched_prev->ts, sched_prev->pblk_num, sched_prev->blk_len, sched_entry->ts, sched_entry->pblk_num, sched_entry->blk_len);
+
+	if(sched_prev->ts <= sched_entry->ts) {
+		rb_seq_curr = &(sched_prev->rb_seq);
+		sched_curr = sched_prev;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_entry) {
+				if(io_distance <= io_distance_allowed) {
+					//printk("< dist:%d\n", io_distance);
+					return 1;
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	} else if(sched_prev->ts > sched_entry->ts) {
+		rb_seq_curr = &(sched_entry->rb_seq);
+		sched_curr = sched_entry;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_prev) {
+				if(io_distance <= io_distance_allowed) {
+					return -1;
+					//printk("> dist:%d\n", io_distance);
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	}
+
+	return 0;
+}
+
+#define DISTANCE_UNLIMITED 10000
+int io_infill_distance_merge_onoff = 1;
+int io_infill_distance_allowed = 1;
+int io_infill_hole_allowed_blks = 8;
+
+int ioinfill_mergeable(struct scheduler_entry *sched_prev, struct scheduler_entry *sched_entry)
+{
+	int io_distance = 0;
+	struct scheduler_entry *sched_curr;
+	struct rb_node *rb_seq_curr;
+	int blocksize;
+
+	if((sched_prev->dev != sched_entry->dev) || (sched_prev->ino != sched_entry->ino)) return 0;
+	if(sched_prev->ino == 0) {
+		blocksize = blocksize_dev(sched_entry->dev);
+		if(blocksize <= 0) blocksize = 4096; //default size
+
+		if(sched_entry->pblk_num - (sched_prev->pblk_num + (sched_prev->blk_len / blocksize)) > io_infill_hole_allowed_blks) return 0;
+	} else {
+		if(sched_entry->pblk_num - (sched_prev->pblk_num + sched_prev->blk_len) > io_infill_hole_allowed_blks) return 0;
+	}
+
+	if(io_infill_distance_allowed == DISTANCE_UNLIMITED) {
+		if(sched_prev->ts <= sched_entry->ts) return 1;
+		else return -1;
+	}
+
+	if(sched_prev->ts <= sched_entry->ts) {
+		rb_seq_curr = &(sched_prev->rb_seq);
+		sched_curr = sched_prev;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_entry) {
+				if(io_distance <= io_infill_distance_allowed) {
+					//printk("< dist:%d\n", io_distance);
+					return 1;
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_infill_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	} else {
+		rb_seq_curr = &(sched_entry->rb_seq);
+		sched_curr = sched_entry;
+		rb_seq_curr = rb_next(rb_seq_curr);
+		sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		while(rb_seq_curr)
+		{
+			io_distance++;
+			if(sched_curr == sched_prev) {
+				if(io_distance <= io_infill_distance_allowed) {
+					return -1;
+					//printk("> dist:%d\n", io_distance);
+				}
+				else return 0;
+			}
+
+			if(io_distance > io_infill_distance_allowed) return 0;
+
+			rb_seq_curr = rb_next(rb_seq_curr);
+			sched_curr = rb_entry(rb_seq_curr, struct scheduler_entry, rb_seq);
+		}
+		return 0;
+	}
+
+	return 0;
+}
+
+struct shift_cq *alloc_shift_cq(int distance)
+{
+	struct shift_cq *cq;
+
+	if(distance < 0) return NULL;
+
+	cq = kmalloc(sizeof(struct shift_cq), GFP_KERNEL);
+	if(!cq) return NULL;
+
+	cq->distance = distance;
+	cq->wall_distance = 0;
+	cq->head = NULL;
+	cq->tail = NULL;
+
+	printk("init_shift_cq() distance : %d\n", distance);
+
+	return cq;
+}
+
+void inc_dist(struct shift_cq *cq, int incr)
+{
+	if(incr <= 0) return;
+	cq->wall_distance += incr;
+}
+
+void destroy_shift_cq(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	
+	if(!cq) return;
+
+	while(cq->head != NULL)
+	{
+		sft_entry = cq->head;
+		cq->head = sft_entry->next;
+		// NEW
+		//if(sft_entry->sched_entry) kfree(sft_entry->sched_entry);
+		//sft_entry->sched_entry = NULL;
+		//
+		kfree(sft_entry);
+	}
+	kfree(cq);
+}
+
+int insert_shift_cq(struct shift_cq *cq, struct scheduler_entry *sched_entry)
+{
+	struct shift_entry *sft_entry;
+
+	sft_entry = kmalloc(sizeof(struct shift_entry), GFP_KERNEL);
+	if(sft_entry == NULL) return -ENOMEM;
+
+	sft_entry->expire = cq->wall_distance + cq->distance;
+	sft_entry->sched_entry = sched_entry;
+	sft_entry->next = NULL;
+
+	if(cq->head == NULL) {
+		cq->head = sft_entry;
+		cq->tail = sft_entry;
+	} else {
+		cq->tail->next = sft_entry;
+		cq->tail = sft_entry;
+	}
+	return 0;
+}
+
+struct scheduler_entry *get_expired_entry(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	struct scheduler_entry *sched_entry;
+	
+	/*
+	if(cq->head != NULL) printk("cq:0x%p, head:0x%p, expire:%d, wall_dist:%d\n", cq, cq->head, cq->head->expire, cq->wall_distance);
+	else printk("cq:0x%p, head:NULL, wall_dist:%d\n", cq, cq->wall_distance);
+	*/
+	if(cq == NULL) return NULL;
+
+	if(cq->head == NULL) return NULL;
+	else {
+		if(cq->head->expire <= cq->wall_distance) {
+			sft_entry = cq->head;
+			cq->head = sft_entry->next;
+			sched_entry = sft_entry->sched_entry;
+			kfree(sft_entry);
+			return sched_entry;
+		}
+	}
+	return NULL;
+}
+
+struct scheduler_entry *get_cq_entry(struct shift_cq *cq)
+{
+	struct shift_entry *sft_entry;
+	
+	if(cq->head == NULL) return NULL;
+	else {
+		sft_entry = cq->head;
+		cq->head = sft_entry->next;
+		return sft_entry->sched_entry;
+	}
+}
+
+#define IOCOUNT		0
+#define IOSIZE		1
+int metadata_shift_onoff = 1;
+int metadata_shift_way = IOCOUNT;
+int metadata_shift_value = 8;
+
+//PARALFETCH
+int rotational_value = 0;
+
+int write_disk_trace_sched_flash(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count)
+{
+	//struct disk_log_entry *disk_log;
+	struct rb_root rb_tree_lba;
+	struct rb_root rb_tree_seq;
+	struct scheduler_entry *sched_entry;
+	struct scheduler_entry *sched_prev;
+	struct rb_node *rb_node;
+	struct rb_node *rb_prev;
+	struct scheduler_entry *ret;
+	struct dependency_entry *head;
+	struct dependency_entry *prev;
+	struct inode *inode;
+	struct super_block *sb;
+	int i,idx;
+	replay_info *trace_buf;
+	int node_count = 0;
+	int merge_count = 0;
+	int miss_count = 0;
+	int blocksize;
+        unsigned long long nstime;
+
+	struct shift_cq *cq;
+
+	rb_tree_lba.rb_node = NULL;
+	rb_tree_seq.rb_node = NULL;
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED start: %llu\n", nstime);
+
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino == 0) {
+			sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+			if(sched_entry) {
+				sched_entry->dev = logs[i].dev;
+				sched_entry->pblk_num = logs[i].blk_num;
+				sched_entry->ts = logs[i].ts;
+				//sched_entry->type = logs[i].type;
+				sched_entry->ino = logs[i].ino;
+				sched_entry->blk_num = logs[i].blk_num;
+				sched_entry->blk_len = logs[i].blk_len;
+				ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+				if(!ret) {
+					ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+					node_count++;
+				} else {
+					//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+					kfree(sched_entry);
+				}
+			}
+			//printk("dev:0x%x, blk_num:%llu, blk_len:%u\n", logs[i].dev, logs[i].blk_num, logs[i].blk_len);
+		} else {
+			sb = user_get_super(logs[i].dev);
+			if(!sb) continue;
+
+			inode = ext4_iget(sb, logs[i].ino);
+			if(IS_ERR_OR_NULL(inode)) {
+				drop_super(sb);
+				continue;
+			}
+
+			head = NULL;
+
+			ext4_ext_walk_range(inode, logs[i].blk_num, logs[i].blk_len, &head);
+
+			if(head) {
+				prev = head;
+				while(prev) {
+					sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+					if(sched_entry) {
+						sched_entry->dev = logs[i].dev;
+						sched_entry->pblk_num = prev->pblk_num;
+						sched_entry->ts = logs[i].ts;
+						//sched_entry->type = logs[i].type;
+						if(prev->type == TYPE_META) sched_entry->ino = 0;
+						else sched_entry->ino = logs[i].ino;
+						sched_entry->blk_num = prev->blk_num;
+						sched_entry->blk_len = prev->blk_len;
+						ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+						if(!ret) {
+							ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+							node_count++;
+							if(prev->type == TYPE_META) miss_count++;
+						} else {
+							//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+							kfree(sched_entry);
+						}
+					}
+					//printk("type:%u pblk_num:%llu blk_num:%llu blk_len:%u\n", prev->type, prev->pblk_num, prev->blk_num, prev->blk_len);
+					head = prev;
+					prev = prev->next;
+					kfree(head);
+				}
+			}
+			iput(inode);
+			drop_super(sb);
+		}
+	}
+	printk("missed_count : %d\n", miss_count);
+
+	if(io_distance_merge_onoff == 0) goto out_iodist_merge;
+
+	printk("********************** IODISTANCE MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = iodist_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			if(idx > 0) {
+				sched_prev->blk_len += sched_entry->blk_len;
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				//BUG FIXED
+				sched_entry->blk_num = sched_prev->blk_num;
+
+				sched_entry->blk_len += sched_prev->blk_len;
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, iodist_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+out_iodist_merge:
+
+	if(io_infill_distance_merge_onoff == 0) goto out_io_infill_distance_merge;
+
+	merge_count = 0;
+	printk("********************** IOINFILL MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = ioinfill_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			blocksize = blocksize_dev(sched_prev->dev);
+			if(idx > 0) {
+				if(sched_prev->ino == 0) {
+					sched_prev->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_prev->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				if(sched_entry->ino == 0) {
+					sched_entry->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_entry->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+				
+				sched_entry->blk_num = sched_prev->blk_num;
+				sched_entry->pblk_num = sched_prev->pblk_num;
+
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, ioinfill_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+out_io_infill_distance_merge:
+
+	for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+	}
+	rb_tree_lba.rb_node = NULL;
+
+	cq = alloc_shift_cq(metadata_shift_value);
+
+	if(cq == NULL) {
+		for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+		{
+			if(rb_node == NULL) return 0;
+
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+		}
+		return 0;
+	}
+
+	trace_buf = kmalloc(sizeof(replay_info) * (node_count+100), GFP_KERNEL);
+	idx = 0;
+
+	for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+		if(sched_entry->ino == 0) {
+//			printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+			if(trace_buf) {
+				trace_buf[idx].dev = sched_entry->dev;
+				trace_buf[idx].ino = sched_entry->ino;
+				trace_buf[idx].start = sched_entry->blk_num;
+				trace_buf[idx++].length = sched_entry->blk_len;
+			}
+
+			if(metadata_shift_way == IOSIZE) inc_dist(cq, (sched_entry->blk_len / 4096 /*blocksize*/));
+			else inc_dist(cq, 1); //else if(metadata_shift_way == IOCOUNT) inc_dist(cq, 1);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+
+			sched_prev = get_expired_entry(cq);
+
+			while(sched_prev != NULL)
+			{
+				if(sched_prev != NULL) {
+					//	printk("E: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+					if(trace_buf) {
+						trace_buf[idx].dev = sched_prev->dev;
+						trace_buf[idx].ino = sched_prev->ino;
+						trace_buf[idx].start = sched_prev->blk_num;
+						trace_buf[idx++].length = sched_prev->blk_len;
+					}
+					ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+					sched_prev = get_expired_entry(cq);
+				} else break;
+			}
+
+			sched_prev = NULL;
+
+		} else {
+			//PARALFETCH2
+			//printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+			//inc_dist(cq, 1);
+			insert_shift_cq(cq, sched_entry);
+
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+
+			sched_prev = get_expired_entry(cq);
+			while(sched_prev != NULL)
+			{
+				if(sched_prev != NULL) {
+					//printk("E: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+					if(trace_buf) {
+						trace_buf[idx].dev = sched_prev->dev;
+						trace_buf[idx].ino = sched_prev->ino;
+						trace_buf[idx].start = sched_prev->blk_num;
+						trace_buf[idx++].length = sched_prev->blk_len;
+					}
+					ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+					sched_prev = get_expired_entry(cq);
+				} else break;
+			}
+		}
+	}
+
+	sched_prev = get_cq_entry(cq);
+	while(sched_prev != NULL)
+	{
+		if(sched_prev != NULL) {
+			//printk("X: 0x%x %lu, p%llu (%llu %u)\n", sched_prev->dev, sched_prev->ino, sched_prev->pblk_num, sched_prev->blk_num, sched_prev->blk_len);
+			if(trace_buf) {
+				trace_buf[idx].dev = sched_prev->dev;
+				trace_buf[idx].ino = sched_prev->ino;
+				trace_buf[idx].start = sched_prev->blk_num;
+				trace_buf[idx++].length = sched_prev->blk_len;
+			}
+			ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+			sched_prev = get_cq_entry(cq);
+		} else break;
+	}
+
+	if(trace_buf) {
+		printk("node_count:%d, idx:%d\n", node_count, idx);
+		*trace_buffer = trace_buf;
+		*trace_count = idx;
+	} else {
+		*trace_buffer = NULL;
+		*trace_count = 0;
+	}
+
+
+	rb_tree_seq.rb_node = NULL;
+
+	if(cq) destroy_shift_cq(cq);
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED end: %llu\n", nstime);
+
+	return 0;
+
+	/*
+	   printk("**********************  LBA ***********************\n");
+	   for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	   {
+	   if(!rb_node) break;
+
+	   sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+	   printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+	   rb_node = rb_next(rb_node);
+	   }
+
+*/
+	/*
+	   printk("**********************  SEQ ***********************\n");
+	   for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	   {
+	   if(!rb_node) break;
+
+	   sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+	   printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+
+	   rb_prev = rb_node;
+	   rb_node = rb_next(rb_node);
+	   ff_rb_erase_seq_del(rb_prev, &rb_tree_seq);
+	   }
+	return 0;
+	   */
+}
+
+int write_disk_trace_sched_hard(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count)
+{
+	struct rb_root rb_tree_lba;
+	struct rb_root rb_tree_seq;
+	struct scheduler_entry *sched_entry;
+	struct scheduler_entry *sched_prev;
+	struct rb_node *rb_node;
+	struct rb_node *rb_prev;
+	struct scheduler_entry *ret;
+	struct dependency_entry *head;
+	struct dependency_entry *prev;
+	struct inode *inode;
+	struct super_block *sb;
+	int i,idx;
+	replay_info *trace_buf;
+	int node_count = 0;
+	int merge_count = 0;
+	int miss_count = 0;
+	int blocksize;
+        unsigned long long nstime;
+
+	rb_tree_lba.rb_node = NULL;
+	rb_tree_seq.rb_node = NULL;
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED start: %llu\n", nstime);
+
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino == 0) {
+			sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+			if(sched_entry) {
+				sched_entry->dev = logs[i].dev;
+				sched_entry->pblk_num = logs[i].blk_num;
+				sched_entry->ts = logs[i].ts;
+				//sched_entry->type = logs[i].type;
+				sched_entry->ino = logs[i].ino;
+				sched_entry->blk_num = logs[i].blk_num;
+				sched_entry->blk_len = logs[i].blk_len;
+				ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+				if(!ret) {
+					ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+					node_count++;
+				} else {
+					//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+					kfree(sched_entry);
+				}
+			}
+			//printk("dev:0x%x, blk_num:%llu, blk_len:%u\n", logs[i].dev, logs[i].blk_num, logs[i].blk_len);
+		}
+	}
+	for(i=0; i<log_count; i++)
+	{
+		if(logs[i].ino != 0) {
+			sb = user_get_super(logs[i].dev);
+			if(!sb) continue;
+
+			inode = ext4_iget(sb, logs[i].ino);
+			if(IS_ERR_OR_NULL(inode)) {
+				drop_super(sb);
+				continue;
+			}
+
+			ext4_ext_walk_range(inode, logs[i].blk_num, logs[i].blk_len, &head);
+
+			if(head) {
+				prev = head;
+				while(prev) {
+					sched_entry = kmalloc(sizeof(struct scheduler_entry), GFP_KERNEL);
+					if(sched_entry) {
+						sched_entry->dev = logs[i].dev;
+						sched_entry->pblk_num = prev->pblk_num;
+						sched_entry->ts = logs[i].ts;
+						//sched_entry->type = logs[i].type;
+						sched_entry->blk_num = prev->blk_num;
+						sched_entry->blk_len = prev->blk_len;
+						if(prev->type == TYPE_META) sched_entry->ino = 0;
+						else {
+							sched_entry->ino = logs[i].ino;
+							//printk("ts:%llu ino:%lu dev:0x%x blk_num:%llu blk_len:%u\n", sched_entry->ts, sched_entry->ino, sched_entry->dev, sched_entry->blk_num, sched_entry->blk_len);
+						}
+						ret = ff_rb_insert_lba(&rb_tree_lba, sched_entry);
+						if(!ret) {
+							ff_rb_insert_seq(&rb_tree_seq, sched_entry);
+							node_count++;
+							if(prev->type == TYPE_META) miss_count++;
+						} else {
+							//printk("EXIST: 0x%x %lu, p%llu (%llu %u)\n", sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+							kfree(sched_entry);
+						}
+					}
+					//printk("pblk_num:%llu type:%u dev:0x%x inode:%lu blk_num:%llu blk_len:%u\n", prev->pblk_num, prev->type, logs[i].dev, logs[i].ino, prev->blk_num, prev->blk_len);
+					head = prev;
+					prev = prev->next;
+					kfree(head);
+				}
+			}
+			iput(inode);
+			drop_super(sb);
+		}
+	}
+	printk("missed_count : %d\n", miss_count);
+
+/*
+	printk("**********************  START: WRITE DETAILED LAUNCH SEQUENCE ***********************\n");
+
+	rb_node = rb_first(&rb_tree_seq);
+	while(rb_node)
+	{
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+		printk("%llu %x %lu %llu %u %llu\n", sched_entry->ts, sched_entry->dev, sched_entry->ino, sched_entry->blk_num, sched_entry->blk_len, sched_entry->pblk_num);
+		rb_node = rb_next(rb_node);
+	}
+	printk("**********************  END: WRITE DETAILED LAUNCH SEQUENCE ***********************\n");
+*/
+
+	printk("**********************  MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		if(io_mergeable(sched_prev, sched_entry)) {
+			sched_prev->blk_len += sched_entry->blk_len;
+			merge_count++;
+			ff_rb_erase_lba(rb_node, &rb_tree_lba);
+			ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+			rb_node = rb_next(rb_prev);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+	merge_count = 0;
+	printk("********************** IOINFILL MERGE ***********************\n");
+	rb_prev = rb_first(&rb_tree_lba);
+	rb_node = rb_next(rb_prev);
+	sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+	sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+
+	for(i=1; i<node_count; i++)
+	{
+		idx = ioinfill_mergeable(sched_prev, sched_entry);
+
+		if(idx) {
+			blocksize = blocksize_dev(sched_prev->dev);
+			if(idx > 0) {
+				if(sched_prev->ino == 0) {
+					sched_prev->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_prev->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				ff_rb_erase_lba(rb_node, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_entry->rb_seq), &rb_tree_seq);
+				rb_node = rb_next(rb_prev);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			} else {
+				if(sched_entry->ino == 0) {
+					sched_entry->blk_len = sched_entry->blk_len + ((sched_entry->blk_num - sched_prev->blk_num) * blocksize);
+				} else sched_entry->blk_len = sched_entry->blk_len + sched_entry->blk_num - sched_prev->blk_num;
+
+				sched_entry->blk_num = sched_prev->blk_num;
+				sched_entry->pblk_num = sched_prev->pblk_num;
+
+				ff_rb_erase_lba(rb_prev, &rb_tree_lba);
+				ff_rb_erase_seq_del(&(sched_prev->rb_seq), &rb_tree_seq);
+				rb_prev = rb_node;
+				rb_node = rb_next(rb_node);
+				sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+				sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+			}
+			merge_count++;
+		} else {
+			rb_prev = rb_node;
+			rb_node = rb_next(rb_node);
+			sched_prev = rb_entry(rb_prev, struct scheduler_entry, rb_lba);
+			sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		}
+	}
+
+	printk("node_count:%d, ioinfill_merge_count:%d\n", node_count, merge_count);
+	node_count -= merge_count;
+
+	for(i=0, rb_node = rb_first(&rb_tree_seq); i<node_count; i++)
+	{
+		if(rb_node == NULL) return 0;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_seq);
+
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_seq(rb_prev, &rb_tree_seq);
+	}
+	rb_tree_seq.rb_node = NULL;
+
+	trace_buf = kmalloc(sizeof(replay_info) * (node_count+100), GFP_KERNEL);
+	idx = 0;
+
+	//printk("**********************  LBA ***********************\n");
+	for(i=0, rb_node = rb_first(&rb_tree_lba); i<node_count; i++)
+	{
+		if(!rb_node) break;
+
+		sched_entry = rb_entry(rb_node, struct scheduler_entry, rb_lba);
+		//	printk("%d: 0x%x %lu, p%llu (%llu %u)\n", i, sched_entry->dev, sched_entry->ino, sched_entry->pblk_num, sched_entry->blk_num, sched_entry->blk_len);
+		if(trace_buf) {
+			trace_buf[idx].dev = sched_entry->dev;
+			trace_buf[idx].ino = sched_entry->ino;
+			trace_buf[idx].start = sched_entry->blk_num;
+			trace_buf[idx++].length = sched_entry->blk_len;
+		}
+		rb_prev = rb_node;
+		rb_node = rb_next(rb_node);
+		ff_rb_erase_lba_del(rb_prev, &rb_tree_lba);
+	}
+
+	rb_tree_lba.rb_node = NULL;
+
+	if(trace_buf) {
+		printk("node_count:%d, idx:%d\n", node_count, idx);
+		*trace_buffer = trace_buf;
+		*trace_count = idx;
+	} else {
+		*trace_buffer = NULL;
+		*trace_count = 0;
+	}
+
+        nstime = ktime_to_ns(ktime_get());
+	printk("SCHED end: %llu\n", nstime);
+
+	return 0;
+}
diff -uNr linux-4.12.9/fs/flashfetch_trace.c linux-4.12.9-pf/fs/flashfetch_trace.c
--- linux-4.12.9/fs/flashfetch_trace.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/fs/flashfetch_trace.c	2019-01-02 21:24:54.172591108 +0900
@@ -0,0 +1,57 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/coda.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+
+#include <linux/flashfetch_trace.h>
+
+int alloc_disk_trace(struct disk_trace **disktrc)
+{
+	*disktrc = kzalloc(ALIGN(sizeof(struct disk_trace), cache_line_size()), GFP_KERNEL);
+
+	if(!(*disktrc)) {
+		printk("alloc_disk_trace failed\n");
+		return -ENOMEM;
+	} else {
+		(*disktrc)->lock = __SPIN_LOCK_UNLOCKED(disktrc->lock); 
+		(*disktrc)->pos = 0;
+	}
+
+	return 0;
+}
+
+void release_disk_trace(struct disk_trace *disktrc)
+{
+	kfree(disktrc);
+}
+
+void reset_disk_trace(struct disk_trace *disktrc)
+{
+	disktrc->lock = __SPIN_LOCK_UNLOCKED(disktrc->lock); 
+	disktrc->pos = 0;
+}
+
+int get_log_entry(struct disk_trace *disktrc)
+{
+	int curr_pos;
+	int ret = -ENOENT;
+
+	spin_lock(&(disktrc->lock));
+
+	curr_pos = disktrc->pos;
+	if(curr_pos < MAX_LOG_ENT) {
+		ret = curr_pos;
+		disktrc->pos = curr_pos + 1;
+	}
+
+	spin_unlock(&(disktrc->lock));
+	
+	return ret;
+}
diff -uNr linux-4.12.9/fs/Kconfig linux-4.12.9-pf/fs/Kconfig
--- linux-4.12.9/fs/Kconfig	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/Kconfig	2019-01-02 21:24:54.300591834 +0900
@@ -10,6 +10,13 @@
 
 if BLOCK
 
+config PARALFETCH
+	bool "Paralfetch for quick application launches"
+	depends on EXT4_FS
+	default n
+	help
+	  An execution-time prefetcher for quick application launches.
+
 config FS_IOMAP
 	bool
 
diff -uNr linux-4.12.9/fs/Makefile linux-4.12.9-pf/fs/Makefile
--- linux-4.12.9/fs/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/Makefile	2019-01-02 21:24:54.301591840 +0900
@@ -11,7 +11,10 @@
 		attr.o bad_inode.o file.o filesystems.o namespace.o \
 		seq_file.o xattr.o libfs.o fs-writeback.o \
 		pnode.o splice.o sync.o utimes.o \
-		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o
+		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o 
+
+obj-$(CONFIG_PARALFETCH)	+= flashfetch_core.o flashfetch_trace.o \
+		flashfetch_fetch.o flashfetch_scheduler.o flashfetch_evaluate.o
 
 ifeq ($(CONFIG_BLOCK),y)
 obj-y +=	buffer.o block_dev.o direct-io.o mpage.o
diff -uNr linux-4.12.9/fs/namespace.c linux-4.12.9-pf/fs/namespace.c
--- linux-4.12.9/fs/namespace.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/namespace.c	2019-01-02 21:24:54.312591902 +0900
@@ -3025,6 +3025,18 @@
 }
 EXPORT_SYMBOL(mount_subtree);
 
+//PARALFETCH
+extern unsigned int bootfetch;
+#define MAXDEVNAME 19
+extern char bootdev[MAXDEVNAME+1];
+extern void do_bootfetch(int bootfetch);
+int boot_in_progress = 0;
+/*
+extern struct file *kernel_open(char const *file_name, int flags, int mode);
+extern int kernel_close(struct file *file);
+struct file *pf_file;
+*/
+
 SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,
 		char __user *, type, unsigned long, flags, void __user *, data)
 {
@@ -3050,12 +3062,41 @@
 
 	ret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);
 
+	//PARALFETCH
+        //printk("MOUNT: kernel_dev:%s, kernel_dir:%s\n", kernel_dev, kernel_dir);
+	
+	if(!ret && bootfetch && !boot_in_progress) {
+                if(strcmp(kernel_dev, "securityfs") == 0 /*&& strcmp(kernel_dir, "/sys/kernel/security") == 0*/) {
+                        printk("BOOTFETCH runs\n");
+                        boot_in_progress = 1;
+                        do_bootfetch(bootfetch);
+                }
+        }
+        /*
+        if(!ret && bootfetch && !boot_in_progress) {
+                if(strncmp(kernel_dev, bootdev, MAXDEVNAME) == 0) {
+                        printk("MOUNT: kernel_dev:%s, kernel_dir:%s\n", kernel_dev, kernel_dir);
+                        //boot_in_progress = 1;
+                        //do_bootfetch(bootfetch);
+                }
+        }
+        */
+
 	kfree(options);
 out_data:
 	kfree(kernel_dev);
 out_dev:
 	kfree(kernel_type);
 out_type:
+	//PARALFETCH
+        /*
+        pf_file = kernel_open("/flashfetch/kernel_boot.pf", O_RDONLY, 0644);
+        if(IS_ERR(pf_file)) printk("MOUNT: kernel_open error = %ld\n", PTR_ERR(pf_file));
+        else {
+                printk("MOUNT: kernel_open succeed!\n");
+                kernel_close(pf_file);
+        }
+        */
 	return ret;
 }
 
diff -uNr linux-4.12.9/fs/read_write.c linux-4.12.9-pf/fs/read_write.c
--- linux-4.12.9/fs/read_write.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/read_write.c	2019-01-02 21:24:54.311591897 +0900
@@ -443,9 +443,23 @@
 	return ret;
 }
 
+//PARALFETCH2
+/*
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+*/
+
 ssize_t __vfs_read(struct file *file, char __user *buf, size_t count,
 		   loff_t *pos)
 {
+	//PARALFETCH2
+	/*
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+                printk("PARALFETCH: __vfs_read pos:%lld count:%lu\n",
+                                *pos, count);
+        }
+	*/
+
 	if (file->f_op->read)
 		return file->f_op->read(file, buf, count, pos);
 	else if (file->f_op->read_iter)
diff -uNr linux-4.12.9/fs/sync.c linux-4.12.9-pf/fs/sync.c
--- linux-4.12.9/fs/sync.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/fs/sync.c	2019-01-02 21:24:54.303591851 +0900
@@ -94,6 +94,21 @@
 	filemap_fdatawait_keep_errors(bdev->bd_inode->i_mapping);
 }
 
+//PARALFETCH
+void sync_filesystems_ff(int dummy)
+{
+        int nowait = 0, wait = 1;
+
+        wakeup_flusher_threads(0, WB_REASON_SYNC);
+        iterate_supers(sync_inodes_one_sb, NULL);
+        iterate_supers(sync_fs_one_sb, &nowait);
+        iterate_supers(sync_fs_one_sb, &wait);
+        iterate_bdevs(fdatawrite_one_bdev, NULL);
+        iterate_bdevs(fdatawait_one_bdev, NULL);
+        if (unlikely(laptop_mode))
+                laptop_sync_completion();
+}
+
 /*
  * Sync everything. We start by waking flusher threads so that most of
  * writeback runs on all devices in parallel. Then we sync all inodes reliably
diff -uNr linux-4.12.9/include/linux/flashfetch_fetch.h linux-4.12.9-pf/include/linux/flashfetch_fetch.h
--- linux-4.12.9/include/linux/flashfetch_fetch.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_fetch.h	2019-01-02 21:24:58.231614116 +0900
@@ -0,0 +1,41 @@
+#include <linux/types.h>
+
+/*
+ * METADATA (ino == 0)
+ * dev = device number
+ * ino = 0
+ * start = logical block number (block offset in the partition)
+ * length = the number of bytes to read
+ *
+ * DATA (ino != 0)
+ * dev = device number
+ * ino = inode number
+ * start = file offset in page size unit (PAGE_CACHE_SIZE = PAGE_SIZE)
+ * length = the number of pages to read
+ */
+
+#ifndef PARALFETCH_FETCH_H
+#define PARALFETCH_FETCH_H
+
+#define MAGIC_LENGTH	8
+#define NAME_LEN		255
+
+typedef struct __replay_info {
+	dev_t dev;
+	ino_t ino;
+	u64 start;
+	u32 length;
+} __attribute__((packed)) replay_info;
+
+typedef struct __paralfetch_header {
+	char magic[MAGIC_LENGTH];
+	u32 version;
+	u16 io_count;
+	u16 private;
+	u16 uncertain_io_count;
+	u16 uncertain_io_size;
+	u16 harmful_io_count;
+	u16 harmful_io_size;
+	replay_info trace_data[0];
+} paralfetch_header;
+#endif
diff -uNr linux-4.12.9/include/linux/flashfetch.h linux-4.12.9-pf/include/linux/flashfetch.h
--- linux-4.12.9/include/linux/flashfetch.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch.h	2019-01-02 21:24:58.203613957 +0900
@@ -0,0 +1,50 @@
+#ifndef PARALFETCH_H
+#define PARALFETCH
+
+//PARALFETCH
+#define FALSE				0
+#define TRUE				1
+
+//FETCHTYPE
+#define PARALFETCH
+//#define HARDFETCH
+
+#ifdef HARDFETCH
+#define FFTIMEOUT 30
+#else
+#define FFTIMEOUT 10
+#endif
+
+// prefetch type
+#define NO_PREFETCH			0
+#define ASYNC_PREFETCH		1
+#define SYNC_PREFETCH		2
+#define GEN_PREFETCH		3
+#define SEQ_REFINE1			4
+#define SEQ_REFINE2			5
+
+// name length limit
+#define NAME_LEN 			255
+
+// flashfetch flags
+#define FF_INPROGRESS			1
+
+// cache management type
+#define FT_BUFFER_CACHE			1
+#define FT_PAGE_CACHE			2
+
+struct seqmon_param {
+	char appname[NAME_LEN];
+	char filename[NAME_LEN];
+};
+
+extern bool enable_flashfetch;
+
+int flashfetch_async(char *appname);
+int flashfetch_sync(char *appname);
+int flashfetch_genpf(char *appname, const char *filename);
+
+#endif
+
+int flashfetch_prefetch_sync(char *app_name);
+int flashfetch_prefetch_async(char *app_name);
diff -uNr linux-4.12.9/include/linux/flashfetch_scheduler.h linux-4.12.9-pf/include/linux/flashfetch_scheduler.h
--- linux-4.12.9/include/linux/flashfetch_scheduler.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_scheduler.h	2019-01-02 21:24:58.263614297 +0900
@@ -0,0 +1,45 @@
+#ifndef PARALFETCH_SCHEDULER_H
+#define PARALFETCH_SCHEDULER_H
+
+#include <linux/types.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+#include <linux/rbtree.h>
+
+struct scheduler_entry {
+	struct rb_node rb_seq;
+	struct rb_node rb_lba;
+	dev_t dev;
+	u64 pblk_num;
+	u64 ts;
+//	u32 type;
+	ino_t ino;
+	u64 blk_num;
+	u32 blk_len;
+};
+
+#define TYPE_META		0
+#define TYPE_REGFILE	1
+
+struct dependency_entry {
+	u64 pblk_num;
+	u32 type;
+	u64 blk_num;
+	u32 blk_len;
+	struct dependency_entry *next;
+};
+
+struct shift_entry {
+	unsigned int expire;
+	struct scheduler_entry *sched_entry;
+	struct shift_entry *next;
+};
+
+struct shift_cq {
+	int distance;
+	int wall_distance;
+	struct shift_entry *head;
+	struct shift_entry *tail;
+};
+
+#endif
diff -uNr linux-4.12.9/include/linux/flashfetch_trace.h linux-4.12.9-pf/include/linux/flashfetch_trace.h
--- linux-4.12.9/include/linux/flashfetch_trace.h	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/include/linux/flashfetch_trace.h	2019-01-02 21:24:58.237614150 +0900
@@ -0,0 +1,39 @@
+#ifndef PARALFETCH_TRACE_H
+#define PARALFETCH_TRACE_H
+
+#include <linux/types.h>
+#include <linux/kdev_t.h>
+#include <linux/spinlock.h>
+
+#include <linux/flashfetch_fetch.h>
+
+#define MAX_LOG_ENT 16384
+
+struct disk_log_entry {
+	u64 ts;
+//	u32 type;
+	dev_t dev;
+	ino_t ino;
+	u64 blk_num;
+	u32 blk_len;
+};
+
+struct disk_trace {
+	struct disk_log_entry log[MAX_LOG_ENT];
+	int pos;
+	spinlock_t lock;
+};
+
+extern int alloc_disk_trace(struct disk_trace **disktrc);
+extern void release_disk_trace(struct disk_trace *disktrc);
+extern void reset_disk_trace(struct disk_trace *disktrc);
+extern int get_log_entry(struct disk_trace *disktrc);
+extern int write_disk_trace_raw(char *appname, struct disk_log_entry *logs, int log_count);
+extern int write_disk_replayinfo_raw(char *appname, replay_info *trace_buffer, int trace_count);
+extern int write_disk_trace_sched_flash(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count);
+extern int write_disk_trace_sched_hard(char *appname, struct disk_log_entry *logs, int log_count, replay_info **trace_buffer, int *trace_count);
+
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+
+#endif
diff -uNr linux-4.12.9/include/linux/mm.h linux-4.12.9-pf/include/linux/mm.h
--- linux-4.12.9/include/linux/mm.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/linux/mm.h	2019-01-02 21:24:58.338614723 +0900
@@ -2178,8 +2178,9 @@
 #define VM_MAX_READAHEAD	128	/* kbytes */
 #define VM_MIN_READAHEAD	16	/* kbytes (includes current page) */
 
+//PARALFETCH
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
-			pgoff_t offset, unsigned long nr_to_read);
+			pgoff_t offset, unsigned long nr_to_read, int prefetch);
 
 void page_cache_sync_readahead(struct address_space *mapping,
 			       struct file_ra_state *ra,
diff -uNr linux-4.12.9/include/linux/page-flags.h linux-4.12.9-pf/include/linux/page-flags.h
--- linux-4.12.9/include/linux/page-flags.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/linux/page-flags.h	2019-01-02 21:24:58.289614445 +0900
@@ -105,6 +105,9 @@
 	PG_young,
 	PG_idle,
 #endif
+#ifdef CONFIG_PARALFETCH
+	PG_prefetch,
+#endif
 	__NR_PAGEFLAGS,
 
 	/* Filesystems */
@@ -313,6 +316,11 @@
 PAGEFLAG(Readahead, reclaim, PF_NO_COMPOUND)
 	TESTCLEARFLAG(Readahead, reclaim, PF_NO_COMPOUND)
 
+#ifdef CONFIG_PARALFETCH
+PAGEFLAG(Prefetch, prefetch, PF_NO_COMPOUND)
+	TESTCLEARFLAG(Prefetch, prefetch, PF_NO_COMPOUND)
+#endif
+
 #ifdef CONFIG_HIGHMEM
 /*
  * Must use a macro here due to header dependency issues. page_zone() is not
diff -uNr linux-4.12.9/include/trace/events/mmflags.h linux-4.12.9-pf/include/trace/events/mmflags.h
--- linux-4.12.9/include/trace/events/mmflags.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/include/trace/events/mmflags.h	2019-01-02 21:24:58.620616321 +0900
@@ -81,6 +81,12 @@
 #define IF_HAVE_PG_IDLE(flag,string)
 #endif
 
+#ifdef CONFIG_PARALFETCH
+#define IF_HAVE_PG_PREFETCH(flag,string) ,{1UL << flag, string}
+#else
+#define IF_HAVE_PG_PREFETCH(flag,string)
+#endif
+
 #define __def_pageflag_names						\
 	{1UL << PG_locked,		"locked"	},		\
 	{1UL << PG_waiters,		"waiters"	},		\
@@ -106,7 +112,9 @@
 IF_HAVE_PG_UNCACHED(PG_uncached,	"uncached"	)		\
 IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
 IF_HAVE_PG_IDLE(PG_young,		"young"		)		\
-IF_HAVE_PG_IDLE(PG_idle,		"idle"		)
+IF_HAVE_PG_IDLE(PG_idle,		"idle"		)		\
+IF_HAVE_PG_PREFETCH(PG_prefetch,	"prefetch"	)		
+// ^PARALFETCH
 
 #define show_page_flags(flags)						\
 	(flags) ? __print_flags(flags, "|",				\
diff -uNr linux-4.12.9/init/main.c linux-4.12.9-pf/init/main.c
--- linux-4.12.9/init/main.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/init/main.c	2019-01-02 21:24:58.719616882 +0900
@@ -162,6 +162,27 @@
 
 __setup("reset_devices", set_reset_devices);
 
+//PARALFETCH
+unsigned int bootfetch = 0;
+EXPORT_SYMBOL(bootfetch);
+#define MAXDEVNAME 19
+char bootdev[MAXDEVNAME+1];
+EXPORT_SYMBOL(bootdev);
+
+static int __init set_bootfetch(char *str)
+{
+        get_option(&str, &bootfetch);
+        return 1;
+}
+__setup("bootfetch=", set_bootfetch);
+
+static int __init set_bootdev(char *str)
+{
+        strncpy(bootdev, str, MAXDEVNAME);
+        return 1;
+}
+__setup("bootdev=", set_bootdev);
+
 static const char *argv_init[MAX_INIT_ARGS+2] = { "init", NULL, };
 const char *envp_init[MAX_INIT_ENVS+2] = { "HOME=/", "TERM=linux", NULL, };
 static const char *panic_later, *panic_param;
@@ -485,6 +506,28 @@
 	ioremap_huge_init();
 }
 
+//PARALFETCH
+extern int flashfetch_sync(char *app_name);
+extern int flashfetch_async(char *app_name);
+extern int flashfetch_genpf(char *appname, const char *filename);
+extern unsigned long flashfetch_monitor_timeout;
+
+void do_bootfetch(int bootfetch)
+{
+        if(!bootfetch) return;
+        else if(bootfetch==1) {
+                printk("BOOTFETCH:flashfetch_async()\n");
+                flashfetch_async("kernel_boot");
+        } else if(bootfetch==2) {
+                printk("BOOTFETCH:flashfetch_sync()\n");
+                flashfetch_sync("kernel_boot");
+        } else if(bootfetch==3) {
+                printk("BOOTFETCH:flashfetch_getpf()\n");
+                flashfetch_monitor_timeout = 30;
+                flashfetch_genpf("kernel_boot", "kernel");
+        } else return;
+}
+
 asmlinkage __visible void __init start_kernel(void)
 {
 	char *command_line;
@@ -970,6 +1013,9 @@
 
 	rcu_end_inkernel_boot();
 
+	//PARALFETCH
+        //do_bootfetch(bootfetch);
+
 	if (ramdisk_execute_command) {
 		ret = run_init_process(ramdisk_execute_command);
 		if (!ret)
diff -uNr linux-4.12.9/kernel/resource.c linux-4.12.9-pf/kernel/resource.c
--- linux-4.12.9/kernel/resource.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/kernel/resource.c	2019-01-02 21:24:53.916589657 +0900
@@ -351,6 +351,71 @@
 
 EXPORT_SYMBOL(release_resource);
 
+//PARALFETCH                                                                   
+static int find_next_iomem_res2(struct resource *res, unsigned long desc,        
+                               bool first_level_children_only)
+{                                                                              
+	resource_size_t start;                                                  
+	struct resource *p;                                                     
+	bool sibling_only = false;                                              
+
+	BUG_ON(!res);                                                           
+
+	start = res->start;                                                     
+
+	if (first_level_children_only)                                          
+		sibling_only = true;                                            
+
+	read_lock(&resource_lock);                                              
+
+	for (p = iomem_resource.child; p; p = next_resource(p, sibling_only)) { 
+		if ((p->flags & res->flags) != res->flags)
+			continue;
+		if ((desc != IORES_DESC_NONE) && (desc != p->desc))
+			continue;
+		if ((p->start >= start))                                        
+			break;                                                  
+	}                                                                       
+
+	read_unlock(&resource_lock);                                            
+	if (!p)                                                                 
+		return -1;                                                      
+	// copy data                                                      
+	if (res->start < p->start)                                              
+		res->start = p->start;
+
+	res->end = p->end;
+
+	return 0;
+}
+
+//PARALFETCH
+resource_size_t find_last_iomem_res(char *name,
+                      bool first_level_children_only)
+{
+       struct resource *p;
+       resource_size_t max_phys_addr = 0;
+       bool sibling_only = false;
+
+       if (first_level_children_only)
+               sibling_only = true;
+
+       read_lock(&resource_lock);
+
+       for (p = iomem_resource.child; p; p = next_resource(p, sibling_only)) {
+               if (!(p->flags & IORESOURCE_MEM))
+                       continue;
+               if (name && strcmp(p->name, name))
+                       continue;
+               if (p->end > max_phys_addr) {
+                       max_phys_addr = p->end;
+               }
+       }
+
+       read_unlock(&resource_lock);
+       return max_phys_addr;
+}
+
 /*
  * Finds the lowest iomem resource existing within [res->start.res->end).
  * The caller must specify res->start, res->end, res->flags, and optionally
@@ -468,6 +533,26 @@
 	}
 	return ret;
 }
+
+//PARALFETCH
+int walk_system_ram_res_all(void *arg,
+                               int (*func)(u64, u64, void *))
+{
+	struct resource res;
+	int ret = -1;
+
+	res.start = 0;
+	res.end = 0;
+	res.flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
+
+	while (!find_next_iomem_res2(&res, IORES_DESC_NONE, true)) {
+		ret = (*func)(res.start, res.end, arg);
+		if (ret)
+			break;
+		res.start = res.end + 1;
+	}
+	return ret;
+}
 
 #if !defined(CONFIG_ARCH_HAS_WALK_MEMORY)
 
diff -uNr linux-4.12.9/kernel/sched/cputime.c linux-4.12.9-pf/kernel/sched/cputime.c
--- linux-4.12.9/kernel/sched/cputime.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/kernel/sched/cputime.c	2019-01-02 21:24:54.072590542 +0900
@@ -7,6 +7,10 @@
 #include <linux/sched/cputime.h>
 #include "sched.h"
 
+//PARALFETCH
+#include <linux/percpu.h>
+#include <linux/blkdev.h>
+
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 
 /*
@@ -337,6 +341,16 @@
 }
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
+//PARALFETCH
+DEFINE_PER_CPU(u64, pf_user_time);
+DEFINE_PER_CPU(u64, pf_system_time);
+DEFINE_PER_CPU(u64, pf_guest_time);
+DEFINE_PER_CPU(u64, pf_idle_time);
+u64 pf_cnt = 0;
+u64 dev_req_cnt = 0;
+
+extern struct super_block *user_get_super(dev_t dev);
+
 /*
  * Account a tick to a process and cpustat
  * @p: the process that the cpu time gets accounted to
@@ -363,6 +377,14 @@
 {
 	u64 other, cputime = TICK_NSEC * ticks;
 
+	//PARALFETCH
+	//int i;
+	/*
+	struct request_queue *reqq = NULL;
+	struct block_device *bdev = NULL;
+	struct super_block *sb = NULL;
+	*/
+
 	/*
 	 * When returning from idle, many ticks can get accounted at
 	 * once, including some ticks of steal, irq, and softirq time.
@@ -376,6 +398,35 @@
 
 	cputime -= other;
 
+	pf_cnt++;
+	
+	//if((pf_cnt % 1000000) == 0) {
+		/*
+		for_each_online_cpu(i) {
+			printk("*PF* cpu: %d,  user: %llu, system: %llu, idle: %llu\n",
+					i, per_cpu(pf_user_time, i),
+					per_cpu(pf_system_time, i),
+					per_cpu(pf_idle_time, i));
+		}
+		printk("*PF* dev_req_cnt: %llu\n", dev_req_cnt);				
+		*/
+		/*
+		sb = user_get_super(0x800005);
+		if(sb) {
+			bdev = sb->s_bdev;
+			if(bdev)
+			      	reqq = bdev_get_queue(bdev);
+			if(reqq) printk("*PF* requests: %d, in_flight: %d\n", 
+					reqq->root_rl.count[0] + reqq->root_rl.count[1],
+					reqq->in_flight[0] + reqq->in_flight[1]);
+		}
+		*/
+		//printk("*PF* sched_req_cnt: %llu\n", sched_req_cnt);				
+		/*printk("*PF* sched_req_cnt: %llu, dev_req_cnt: %llu\n", sched_req_cnt,
+				dev_req_cnt);				
+				*/
+	//}
+
 	if (this_cpu_ksoftirqd() == p) {
 		/*
 		 * ksoftirqd time do not get accounted in cpu_softirq_time.
@@ -384,12 +435,16 @@
 		 */
 		account_system_index_time(p, cputime, CPUTIME_SOFTIRQ);
 	} else if (user_tick) {
+		per_cpu(pf_user_time, smp_processor_id()) += cputime;
 		account_user_time(p, cputime);
 	} else if (p == rq->idle) {
+		per_cpu(pf_idle_time, smp_processor_id()) += cputime;
 		account_idle_time(cputime);
 	} else if (p->flags & PF_VCPU) { /* System time or guest time */
+		per_cpu(pf_guest_time, smp_processor_id()) += cputime;
 		account_guest_time(p, cputime);
 	} else {
+		per_cpu(pf_system_time, smp_processor_id()) += cputime;
 		account_system_index_time(p, cputime, CPUTIME_SYSTEM);
 	}
 }
diff -uNr linux-4.12.9/Makefile linux-4.12.9-pf/Makefile
--- linux-4.12.9/Makefile	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/Makefile	2019-01-02 21:24:25.831430465 +0900
@@ -1291,9 +1291,9 @@
 		  arch/*/include/generated .tmp_objdiff
 MRPROPER_FILES += .config .config.old .version .old_version \
 		  Module.symvers tags TAGS cscope* GPATH GTAGS GRTAGS GSYMS \
-		  signing_key.pem signing_key.priv signing_key.x509	\
-		  x509.genkey extra_certificates signing_key.x509.keyid	\
-		  signing_key.x509.signer vmlinux-gdb.py
+		  certs/signing_key.pem certs/signing_key.priv certs/signing_key.x509   \
+		  certs/x509.genkey certs/extra_certificates certs/signing_key.x509.keyid \
+		  certs/signing_key.x509.signer vmlinux-gdb.py
 
 # clean - Delete most, but leave enough to build external modules
 #
diff -uNr linux-4.12.9/mm/fadvise.c linux-4.12.9-pf/mm/fadvise.c
--- linux-4.12.9/mm/fadvise.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/fadvise.c	2019-01-02 21:24:57.454609712 +0900
@@ -109,8 +109,9 @@
 		 * Ignore return value because fadvise() shall return
 		 * success even if filesystem can't retrieve a hint,
 		 */
+		//PARALFETCH
 		force_page_cache_readahead(mapping, f.file, start_index,
-					   nrpages);
+					   nrpages, 0);
 		break;
 	case POSIX_FADV_NOREUSE:
 		break;
diff -uNr linux-4.12.9/mm/filemap.c linux-4.12.9-pf/mm/filemap.c
--- linux-4.12.9/mm/filemap.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/filemap.c	2019-01-02 21:24:57.458609734 +0900
@@ -48,6 +48,10 @@
 
 #include <asm/mman.h>
 
+//PARALFETCH
+#include <linux/flashfetch.h>                                                   
+#include <linux/flashfetch_trace.h>
+
 /*
  * Shared mappings implemented 30.11.1994. It's not fully working yet,
  * though.
@@ -1759,6 +1763,23 @@
 	ra->ra_pages /= 4;
 }
 
+//PARALFETCH
+extern unsigned long long int flashfetch_readahead_hit;
+extern unsigned long long int flashfetch_prefetch_hit;
+
+//PARALFETCH2
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+unsigned int p_dev=0;
+unsigned long p_ino=0;
+unsigned long p_offs=0;
+unsigned long p_len=0;
+unsigned int n_dev=0;
+unsigned long n_ino=0;
+unsigned long n_offs=0;
+unsigned long n_len=0;
+
+
 /**
  * do_generic_file_read - generic file read routine
  * @filp:	the file to read
@@ -1795,6 +1816,42 @@
 	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
 	offset = *ppos & ~PAGE_MASK;
 
+        //PARALFETCH2
+        if(flashfetch_monitor_blkio && flashfetch_trace) {
+		n_dev = inode->i_sb->s_dev;
+		n_ino = inode->i_ino;
+		n_offs = index;
+		n_len = last_index - index;
+		if(n_dev == p_dev && n_ino == p_ino) {
+			if(n_offs <= p_offs && n_offs+n_len >= p_offs+p_len) {
+				p_offs = n_offs;
+				p_len = n_len;
+			} else if(n_offs >= p_offs && n_offs+n_len <= p_offs+p_len) {
+				;
+			} else if(n_offs+n_len <= p_offs && n_offs+n_len <= p_offs+p_len) {
+				p_offs = n_offs;
+				p_len = p_len + p_offs - n_offs;
+			} else if(n_offs >= p_offs && n_offs <= p_offs+p_len) {
+				p_len = n_len + n_offs - p_offs;
+			} else {
+				printk("PARALFETCH: DGFR1: dev:0x%x ino:%lu (%lu, %lu)\n",
+					p_dev, p_ino, p_offs, p_len);
+
+				p_dev = n_dev;
+				p_ino = n_ino;
+				p_offs = n_offs;
+				p_len = n_len;
+			}
+		} else {
+			printk("PARALFETCH: DGFR2: dev:0x%x ino:%lu (%lu, %lu)\n",
+				p_dev, p_ino, p_offs, p_len);
+			p_dev = n_dev;
+			p_ino = n_ino;
+			p_offs = n_offs;
+			p_len = n_len;
+		}
+	}
+
 	for (;;) {
 		struct page *page;
 		pgoff_t end_index;
@@ -1817,12 +1874,20 @@
 			if (unlikely(page == NULL))
 				goto no_cached_page;
 		}
+#ifdef CONFIG_PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+#endif
 		if (PageReadahead(page)) {
 			page_cache_async_readahead(mapping,
 					ra, filp, page,
 					index, last_index - index);
 		}
 		if (!PageUptodate(page)) {
+#ifdef CONFIG_PARALFETCH
+			if (TestClearPagePrefetch(page)) 
+				flashfetch_readahead_hit++;
+#endif
 			/*
 			 * See comment in do_read_cache_page on why
 			 * wait_on_page_locked is used to avoid unnecessarily
@@ -1850,6 +1915,11 @@
 				goto page_not_up_to_date_locked;
 			unlock_page(page);
 		}
+#ifdef CONFIG_PARALFETCH
+		//PARALFECH
+		else if (TestClearPagePrefetch(page)) 
+				flashfetch_prefetch_hit++;
+#endif
 page_ok:
 		/*
 		 * i_size must be checked after we know the page is Uptodate.
@@ -2119,6 +2189,9 @@
 {
 	struct address_space *mapping = file->f_mapping;
 
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
 		return;
@@ -2126,6 +2199,13 @@
 		return;
 
 	if (vma->vm_flags & VM_SEQ_READ) {
+		//PARALFETCH
+		/*
+                printk("PF:SMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
 		page_cache_sync_readahead(mapping, ra, file, offset,
 					  ra->ra_pages);
 		return;
@@ -2148,6 +2228,14 @@
 	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
 	ra->size = ra->ra_pages;
 	ra->async_size = ra->ra_pages / 4;
+
+	//PARALFETCH
+	/*
+        printk("PF:SMRAR:dev:0x%x ino:%lu offs:%lu, size:%u",
+                        inode->i_sb->s_dev, inode->i_ino,
+                        ra->start, ra->size);
+			*/
+
 	ra_submit(ra, mapping, file);
 }
 
@@ -2163,14 +2251,25 @@
 {
 	struct address_space *mapping = file->f_mapping;
 
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
 		return;
 	if (ra->mmap_miss > 0)
 		ra->mmap_miss--;
-	if (PageReadahead(page))
+	if (PageReadahead(page)) {
+		//PARALFETCH
+		/*
+                printk("PF:AMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
 		page_cache_async_readahead(mapping, ra, file,
 					   page, offset, ra->ra_pages);
+	}
 }
 
 /**
@@ -2317,6 +2416,13 @@
 }
 EXPORT_SYMBOL(filemap_fault);
 
+//PARALFETCH
+extern pid_t launch_pid;
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+extern int pfault_trace;
+extern int pfault_debug;
+
 void filemap_map_pages(struct vm_fault *vmf,
 		pgoff_t start_pgoff, pgoff_t end_pgoff)
 {
@@ -2328,15 +2434,62 @@
 	unsigned long max_idx;
 	struct page *head, *page;
 
+	//PARALFETCH
+        struct inode *inode = mapping->host;
+	int idx;
+        struct disk_log_entry *ff_log;
+	int should_log = 0;
+	u64 log_start_pgoff = 0;
+	u64 log_ts = 0;
+	/* previously commented
+        if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid))
+	       	printk("PF:filemap_map_pages:dev:0x%x ino:%lu start_pgoff:%lu, end_pgoff:%lu",
+		inode->i_sb->s_dev, inode->i_ino,
+		start_pgoff, end_pgoff);
+		*/
+	if(flashfetch_monitor_blkio && flashfetch_trace && pfault_trace) {
+		if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid)) {
+			if(inode && inode->i_ino != 0) {
+				if(inode->i_sb && inode->i_sb->s_dev) {
+					should_log = 1;
+					log_start_pgoff = start_pgoff;
+					log_ts = ktime_to_ns(ktime_get());
+				}
+			}
+		}
+	}
+
 	rcu_read_lock();
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter,
 			start_pgoff) {
-		if (iter.index > end_pgoff)
+		if (iter.index > end_pgoff) {
+			// make log;
+			if(should_log && log_start_pgoff <= end_pgoff) {
+				idx = get_log_entry(flashfetch_trace);
+				if(idx >= 0) {
+					ff_log = &(flashfetch_trace->log[idx]);
+					ff_log->ts = log_ts;
+					ff_log->dev = inode->i_sb->s_dev;
+					ff_log->ino = inode->i_ino;
+					ff_log->blk_num = (u64)log_start_pgoff;
+					ff_log->blk_len = (u32)(end_pgoff - log_start_pgoff + 1);
+
+					if(pfault_debug)
+						printk("PFAULT: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+					if(ff_log->dev == 0)
+						printk("FFFF: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+				}
+			}
 			break;
+		}
 repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			goto next;
+		//PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+		
 		if (radix_tree_exception(page)) {
 			if (radix_tree_deref_retry(page)) {
 				slot = radix_tree_iter_retry(&iter);
@@ -2394,6 +2547,27 @@
 		/* Huge page is mapped? No need to proceed. */
 		if (pmd_trans_huge(*vmf->pmd))
 			break;
+
+		// make log and reset log start index
+		if(should_log && (log_start_pgoff != iter.index)) {
+			idx = get_log_entry(flashfetch_trace);
+			if(idx >= 0) {
+				ff_log = &(flashfetch_trace->log[idx]);
+				ff_log->ts = log_ts;
+
+				ff_log->dev = inode->i_sb->s_dev;
+				ff_log->ino = inode->i_ino;
+				ff_log->blk_num = (u64)log_start_pgoff;
+				ff_log->blk_len = (u32)(iter.index - log_start_pgoff + 1);
+
+				if(pfault_debug)
+					printk("PFAULT: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+				if(ff_log->dev == 0)
+					printk("FFFFF: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+			}
+			log_start_pgoff = iter.index + 1;
+		}
+
 		if (iter.index == end_pgoff)
 			break;
 	}
@@ -2411,7 +2585,7 @@
 	file_update_time(vmf->vma->vm_file);
 	lock_page(page);
 	if (page->mapping != inode->i_mapping) {
-		unlock_page(page);
+	unlock_page(page);
 		ret = VM_FAULT_NOPAGE;
 		goto out;
 	}
diff -uNr linux-4.12.9/mm/filemap.c.old linux-4.12.9-pf/mm/filemap.c.old
--- linux-4.12.9/mm/filemap.c.old	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/mm/filemap.c.old	2019-01-02 21:24:57.383609310 +0900
@@ -0,0 +1,3116 @@
+/*
+ *	linux/mm/filemap.c
+ *
+ * Copyright (C) 1994-1999  Linus Torvalds
+ */
+
+/*
+ * This file handles the generic file mmap semantics used by
+ * most "normal" filesystems (but you don't /have/ to use this:
+ * the NFS filesystem used to do this differently, for example)
+ */
+#include <linux/export.h>
+#include <linux/compiler.h>
+#include <linux/dax.h>
+#include <linux/fs.h>
+#include <linux/sched/signal.h>
+#include <linux/uaccess.h>
+#include <linux/capability.h>
+#include <linux/kernel_stat.h>
+#include <linux/gfp.h>
+#include <linux/mm.h>
+#include <linux/swap.h>
+#include <linux/mman.h>
+#include <linux/pagemap.h>
+#include <linux/file.h>
+#include <linux/uio.h>
+#include <linux/hash.h>
+#include <linux/writeback.h>
+#include <linux/backing-dev.h>
+#include <linux/pagevec.h>
+#include <linux/blkdev.h>
+#include <linux/security.h>
+#include <linux/cpuset.h>
+#include <linux/hardirq.h> /* for BUG_ON(!in_atomic()) only */
+#include <linux/hugetlb.h>
+#include <linux/memcontrol.h>
+#include <linux/cleancache.h>
+#include <linux/rmap.h>
+#include "internal.h"
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/filemap.h>
+
+/*
+ * FIXME: remove all knowledge of the buffer layer from the core VM
+ */
+#include <linux/buffer_head.h> /* for try_to_free_buffers */
+
+#include <asm/mman.h>
+
+//PARALFETCH
+#include <linux/flashfetch.h>                                                   
+#include <linux/flashfetch_trace.h>
+
+/*
+ * Shared mappings implemented 30.11.1994. It's not fully working yet,
+ * though.
+ *
+ * Shared mappings now work. 15.8.1995  Bruno.
+ *
+ * finished 'unifying' the page and buffer cache and SMP-threaded the
+ * page-cache, 21.05.1999, Ingo Molnar <mingo@redhat.com>
+ *
+ * SMP-threaded pagemap-LRU 1999, Andrea Arcangeli <andrea@suse.de>
+ */
+
+/*
+ * Lock ordering:
+ *
+ *  ->i_mmap_rwsem		(truncate_pagecache)
+ *    ->private_lock		(__free_pte->__set_page_dirty_buffers)
+ *      ->swap_lock		(exclusive_swap_page, others)
+ *        ->mapping->tree_lock
+ *
+ *  ->i_mutex
+ *    ->i_mmap_rwsem		(truncate->unmap_mapping_range)
+ *
+ *  ->mmap_sem
+ *    ->i_mmap_rwsem
+ *      ->page_table_lock or pte_lock	(various, mainly in memory.c)
+ *        ->mapping->tree_lock	(arch-dependent flush_dcache_mmap_lock)
+ *
+ *  ->mmap_sem
+ *    ->lock_page		(access_process_vm)
+ *
+ *  ->i_mutex			(generic_perform_write)
+ *    ->mmap_sem		(fault_in_pages_readable->do_page_fault)
+ *
+ *  bdi->wb.list_lock
+ *    sb_lock			(fs/fs-writeback.c)
+ *    ->mapping->tree_lock	(__sync_single_inode)
+ *
+ *  ->i_mmap_rwsem
+ *    ->anon_vma.lock		(vma_adjust)
+ *
+ *  ->anon_vma.lock
+ *    ->page_table_lock or pte_lock	(anon_vma_prepare and various)
+ *
+ *  ->page_table_lock or pte_lock
+ *    ->swap_lock		(try_to_unmap_one)
+ *    ->private_lock		(try_to_unmap_one)
+ *    ->tree_lock		(try_to_unmap_one)
+ *    ->zone_lru_lock(zone)	(follow_page->mark_page_accessed)
+ *    ->zone_lru_lock(zone)	(check_pte_range->isolate_lru_page)
+ *    ->private_lock		(page_remove_rmap->set_page_dirty)
+ *    ->tree_lock		(page_remove_rmap->set_page_dirty)
+ *    bdi.wb->list_lock		(page_remove_rmap->set_page_dirty)
+ *    ->inode->i_lock		(page_remove_rmap->set_page_dirty)
+ *    ->memcg->move_lock	(page_remove_rmap->lock_page_memcg)
+ *    bdi.wb->list_lock		(zap_pte_range->set_page_dirty)
+ *    ->inode->i_lock		(zap_pte_range->set_page_dirty)
+ *    ->private_lock		(zap_pte_range->__set_page_dirty_buffers)
+ *
+ * ->i_mmap_rwsem
+ *   ->tasklist_lock            (memory_failure, collect_procs_ao)
+ */
+
+static int page_cache_tree_insert(struct address_space *mapping,
+				  struct page *page, void **shadowp)
+{
+	struct radix_tree_node *node;
+	void **slot;
+	int error;
+
+	error = __radix_tree_create(&mapping->page_tree, page->index, 0,
+				    &node, &slot);
+	if (error)
+		return error;
+	if (*slot) {
+		void *p;
+
+		p = radix_tree_deref_slot_protected(slot, &mapping->tree_lock);
+		if (!radix_tree_exceptional_entry(p))
+			return -EEXIST;
+
+		mapping->nrexceptional--;
+		if (!dax_mapping(mapping)) {
+			if (shadowp)
+				*shadowp = p;
+		} else {
+			/* DAX can replace empty locked entry with a hole */
+			WARN_ON_ONCE(p !=
+				dax_radix_locked_entry(0, RADIX_DAX_EMPTY));
+			/* Wakeup waiters for exceptional entry lock */
+			dax_wake_mapping_entry_waiter(mapping, page->index, p,
+						      true);
+		}
+	}
+	__radix_tree_replace(&mapping->page_tree, node, slot, page,
+			     workingset_update_node, mapping);
+	mapping->nrpages++;
+	return 0;
+}
+
+static void page_cache_tree_delete(struct address_space *mapping,
+				   struct page *page, void *shadow)
+{
+	int i, nr;
+
+	/* hugetlb pages are represented by one entry in the radix tree */
+	nr = PageHuge(page) ? 1 : hpage_nr_pages(page);
+
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	VM_BUG_ON_PAGE(nr != 1 && shadow, page);
+
+	for (i = 0; i < nr; i++) {
+		struct radix_tree_node *node;
+		void **slot;
+
+		__radix_tree_lookup(&mapping->page_tree, page->index + i,
+				    &node, &slot);
+
+		VM_BUG_ON_PAGE(!node && nr != 1, page);
+
+		radix_tree_clear_tags(&mapping->page_tree, node, slot);
+		__radix_tree_replace(&mapping->page_tree, node, slot, shadow,
+				     workingset_update_node, mapping);
+	}
+
+	if (shadow) {
+		mapping->nrexceptional += nr;
+		/*
+		 * Make sure the nrexceptional update is committed before
+		 * the nrpages update so that final truncate racing
+		 * with reclaim does not see both counters 0 at the
+		 * same time and miss a shadow entry.
+		 */
+		smp_wmb();
+	}
+	mapping->nrpages -= nr;
+}
+
+/*
+ * Delete a page from the page cache and free it. Caller has to make
+ * sure the page is locked and that nobody else uses it - or that usage
+ * is safe.  The caller must hold the mapping's tree_lock.
+ */
+void __delete_from_page_cache(struct page *page, void *shadow)
+{
+	struct address_space *mapping = page->mapping;
+	int nr = hpage_nr_pages(page);
+
+	trace_mm_filemap_delete_from_page_cache(page);
+	/*
+	 * if we're uptodate, flush out into the cleancache, otherwise
+	 * invalidate any existing cleancache entries.  We can't leave
+	 * stale data around in the cleancache once our page is gone
+	 */
+	if (PageUptodate(page) && PageMappedToDisk(page))
+		cleancache_put_page(page);
+	else
+		cleancache_invalidate_page(mapping, page);
+
+	VM_BUG_ON_PAGE(PageTail(page), page);
+	VM_BUG_ON_PAGE(page_mapped(page), page);
+	if (!IS_ENABLED(CONFIG_DEBUG_VM) && unlikely(page_mapped(page))) {
+		int mapcount;
+
+		pr_alert("BUG: Bad page cache in process %s  pfn:%05lx\n",
+			 current->comm, page_to_pfn(page));
+		dump_page(page, "still mapped when deleted");
+		dump_stack();
+		add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+
+		mapcount = page_mapcount(page);
+		if (mapping_exiting(mapping) &&
+		    page_count(page) >= mapcount + 2) {
+			/*
+			 * All vmas have already been torn down, so it's
+			 * a good bet that actually the page is unmapped,
+			 * and we'd prefer not to leak it: if we're wrong,
+			 * some other bad page check should catch it later.
+			 */
+			page_mapcount_reset(page);
+			page_ref_sub(page, mapcount);
+		}
+	}
+
+	page_cache_tree_delete(mapping, page, shadow);
+
+	page->mapping = NULL;
+	/* Leave page->index set: truncation lookup relies upon it */
+
+	/* hugetlb pages do not participate in page cache accounting. */
+	if (!PageHuge(page))
+		__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, -nr);
+	if (PageSwapBacked(page)) {
+		__mod_node_page_state(page_pgdat(page), NR_SHMEM, -nr);
+		if (PageTransHuge(page))
+			__dec_node_page_state(page, NR_SHMEM_THPS);
+	} else {
+		VM_BUG_ON_PAGE(PageTransHuge(page) && !PageHuge(page), page);
+	}
+
+	/*
+	 * At this point page must be either written or cleaned by truncate.
+	 * Dirty page here signals a bug and loss of unwritten data.
+	 *
+	 * This fixes dirty accounting after removing the page entirely but
+	 * leaves PageDirty set: it has no effect for truncated page and
+	 * anyway will be cleared before returning page into buddy allocator.
+	 */
+	if (WARN_ON_ONCE(PageDirty(page)))
+		account_page_cleaned(page, mapping, inode_to_wb(mapping->host));
+}
+
+/**
+ * delete_from_page_cache - delete page from page cache
+ * @page: the page which the kernel is trying to remove from page cache
+ *
+ * This must be called only on pages that have been verified to be in the page
+ * cache and locked.  It will never put the page into the free list, the caller
+ * has a reference on the page.
+ */
+void delete_from_page_cache(struct page *page)
+{
+	struct address_space *mapping = page_mapping(page);
+	unsigned long flags;
+	void (*freepage)(struct page *);
+
+	BUG_ON(!PageLocked(page));
+
+	freepage = mapping->a_ops->freepage;
+
+	spin_lock_irqsave(&mapping->tree_lock, flags);
+	__delete_from_page_cache(page, NULL);
+	spin_unlock_irqrestore(&mapping->tree_lock, flags);
+
+	if (freepage)
+		freepage(page);
+
+	if (PageTransHuge(page) && !PageHuge(page)) {
+		page_ref_sub(page, HPAGE_PMD_NR);
+		VM_BUG_ON_PAGE(page_count(page) <= 0, page);
+	} else {
+		put_page(page);
+	}
+}
+EXPORT_SYMBOL(delete_from_page_cache);
+
+int filemap_check_errors(struct address_space *mapping)
+{
+	int ret = 0;
+	/* Check for outstanding write errors */
+	if (test_bit(AS_ENOSPC, &mapping->flags) &&
+	    test_and_clear_bit(AS_ENOSPC, &mapping->flags))
+		ret = -ENOSPC;
+	if (test_bit(AS_EIO, &mapping->flags) &&
+	    test_and_clear_bit(AS_EIO, &mapping->flags))
+		ret = -EIO;
+	return ret;
+}
+EXPORT_SYMBOL(filemap_check_errors);
+
+/**
+ * __filemap_fdatawrite_range - start writeback on mapping dirty pages in range
+ * @mapping:	address space structure to write
+ * @start:	offset in bytes where the range starts
+ * @end:	offset in bytes where the range ends (inclusive)
+ * @sync_mode:	enable synchronous operation
+ *
+ * Start writeback against all of a mapping's dirty pages that lie
+ * within the byte offsets <start, end> inclusive.
+ *
+ * If sync_mode is WB_SYNC_ALL then this is a "data integrity" operation, as
+ * opposed to a regular memory cleansing writeback.  The difference between
+ * these two operations is that if a dirty page/buffer is encountered, it must
+ * be waited upon, and not just skipped over.
+ */
+int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
+				loff_t end, int sync_mode)
+{
+	int ret;
+	struct writeback_control wbc = {
+		.sync_mode = sync_mode,
+		.nr_to_write = LONG_MAX,
+		.range_start = start,
+		.range_end = end,
+	};
+
+	if (!mapping_cap_writeback_dirty(mapping))
+		return 0;
+
+	wbc_attach_fdatawrite_inode(&wbc, mapping->host);
+	ret = do_writepages(mapping, &wbc);
+	wbc_detach_inode(&wbc);
+	return ret;
+}
+
+static inline int __filemap_fdatawrite(struct address_space *mapping,
+	int sync_mode)
+{
+	return __filemap_fdatawrite_range(mapping, 0, LLONG_MAX, sync_mode);
+}
+
+int filemap_fdatawrite(struct address_space *mapping)
+{
+	return __filemap_fdatawrite(mapping, WB_SYNC_ALL);
+}
+EXPORT_SYMBOL(filemap_fdatawrite);
+
+int filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
+				loff_t end)
+{
+	return __filemap_fdatawrite_range(mapping, start, end, WB_SYNC_ALL);
+}
+EXPORT_SYMBOL(filemap_fdatawrite_range);
+
+/**
+ * filemap_flush - mostly a non-blocking flush
+ * @mapping:	target address_space
+ *
+ * This is a mostly non-blocking flush.  Not suitable for data-integrity
+ * purposes - I/O may not be started against all dirty pages.
+ */
+int filemap_flush(struct address_space *mapping)
+{
+	return __filemap_fdatawrite(mapping, WB_SYNC_NONE);
+}
+EXPORT_SYMBOL(filemap_flush);
+
+static int __filemap_fdatawait_range(struct address_space *mapping,
+				     loff_t start_byte, loff_t end_byte)
+{
+	pgoff_t index = start_byte >> PAGE_SHIFT;
+	pgoff_t end = end_byte >> PAGE_SHIFT;
+	struct pagevec pvec;
+	int nr_pages;
+	int ret = 0;
+
+	if (end_byte < start_byte)
+		goto out;
+
+	pagevec_init(&pvec, 0);
+	while ((index <= end) &&
+			(nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
+			PAGECACHE_TAG_WRITEBACK,
+			min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)) != 0) {
+		unsigned i;
+
+		for (i = 0; i < nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+
+			/* until radix tree lookup accepts end_index */
+			if (page->index > end)
+				continue;
+
+			wait_on_page_writeback(page);
+			if (TestClearPageError(page))
+				ret = -EIO;
+		}
+		pagevec_release(&pvec);
+		cond_resched();
+	}
+out:
+	return ret;
+}
+
+/**
+ * filemap_fdatawait_range - wait for writeback to complete
+ * @mapping:		address space structure to wait for
+ * @start_byte:		offset in bytes where the range starts
+ * @end_byte:		offset in bytes where the range ends (inclusive)
+ *
+ * Walk the list of under-writeback pages of the given address space
+ * in the given range and wait for all of them.  Check error status of
+ * the address space and return it.
+ *
+ * Since the error status of the address space is cleared by this function,
+ * callers are responsible for checking the return value and handling and/or
+ * reporting the error.
+ */
+int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,
+			    loff_t end_byte)
+{
+	int ret, ret2;
+
+	ret = __filemap_fdatawait_range(mapping, start_byte, end_byte);
+	ret2 = filemap_check_errors(mapping);
+	if (!ret)
+		ret = ret2;
+
+	return ret;
+}
+EXPORT_SYMBOL(filemap_fdatawait_range);
+
+/**
+ * filemap_fdatawait_keep_errors - wait for writeback without clearing errors
+ * @mapping: address space structure to wait for
+ *
+ * Walk the list of under-writeback pages of the given address space
+ * and wait for all of them.  Unlike filemap_fdatawait(), this function
+ * does not clear error status of the address space.
+ *
+ * Use this function if callers don't handle errors themselves.  Expected
+ * call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
+ * fsfreeze(8)
+ */
+void filemap_fdatawait_keep_errors(struct address_space *mapping)
+{
+	loff_t i_size = i_size_read(mapping->host);
+
+	if (i_size == 0)
+		return;
+
+	__filemap_fdatawait_range(mapping, 0, i_size - 1);
+}
+
+/**
+ * filemap_fdatawait - wait for all under-writeback pages to complete
+ * @mapping: address space structure to wait for
+ *
+ * Walk the list of under-writeback pages of the given address space
+ * and wait for all of them.  Check error status of the address space
+ * and return it.
+ *
+ * Since the error status of the address space is cleared by this function,
+ * callers are responsible for checking the return value and handling and/or
+ * reporting the error.
+ */
+int filemap_fdatawait(struct address_space *mapping)
+{
+	loff_t i_size = i_size_read(mapping->host);
+
+	if (i_size == 0)
+		return 0;
+
+	return filemap_fdatawait_range(mapping, 0, i_size - 1);
+}
+EXPORT_SYMBOL(filemap_fdatawait);
+
+int filemap_write_and_wait(struct address_space *mapping)
+{
+	int err = 0;
+
+	if ((!dax_mapping(mapping) && mapping->nrpages) ||
+	    (dax_mapping(mapping) && mapping->nrexceptional)) {
+		err = filemap_fdatawrite(mapping);
+		/*
+		 * Even if the above returned error, the pages may be
+		 * written partially (e.g. -ENOSPC), so we wait for it.
+		 * But the -EIO is special case, it may indicate the worst
+		 * thing (e.g. bug) happened, so we avoid waiting for it.
+		 */
+		if (err != -EIO) {
+			int err2 = filemap_fdatawait(mapping);
+			if (!err)
+				err = err2;
+		}
+	} else {
+		err = filemap_check_errors(mapping);
+	}
+	return err;
+}
+EXPORT_SYMBOL(filemap_write_and_wait);
+
+/**
+ * filemap_write_and_wait_range - write out & wait on a file range
+ * @mapping:	the address_space for the pages
+ * @lstart:	offset in bytes where the range starts
+ * @lend:	offset in bytes where the range ends (inclusive)
+ *
+ * Write out and wait upon file offsets lstart->lend, inclusive.
+ *
+ * Note that @lend is inclusive (describes the last byte to be written) so
+ * that this function can be used to write to the very end-of-file (end = -1).
+ */
+int filemap_write_and_wait_range(struct address_space *mapping,
+				 loff_t lstart, loff_t lend)
+{
+	int err = 0;
+
+	if ((!dax_mapping(mapping) && mapping->nrpages) ||
+	    (dax_mapping(mapping) && mapping->nrexceptional)) {
+		err = __filemap_fdatawrite_range(mapping, lstart, lend,
+						 WB_SYNC_ALL);
+		/* See comment of filemap_write_and_wait() */
+		if (err != -EIO) {
+			int err2 = filemap_fdatawait_range(mapping,
+						lstart, lend);
+			if (!err)
+				err = err2;
+		}
+	} else {
+		err = filemap_check_errors(mapping);
+	}
+	return err;
+}
+EXPORT_SYMBOL(filemap_write_and_wait_range);
+
+/**
+ * replace_page_cache_page - replace a pagecache page with a new one
+ * @old:	page to be replaced
+ * @new:	page to replace with
+ * @gfp_mask:	allocation mode
+ *
+ * This function replaces a page in the pagecache with a new one.  On
+ * success it acquires the pagecache reference for the new page and
+ * drops it for the old page.  Both the old and new pages must be
+ * locked.  This function does not add the new page to the LRU, the
+ * caller must do that.
+ *
+ * The remove + add is atomic.  The only way this function can fail is
+ * memory allocation failure.
+ */
+int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)
+{
+	int error;
+
+	VM_BUG_ON_PAGE(!PageLocked(old), old);
+	VM_BUG_ON_PAGE(!PageLocked(new), new);
+	VM_BUG_ON_PAGE(new->mapping, new);
+
+	error = radix_tree_preload(gfp_mask & ~__GFP_HIGHMEM);
+	if (!error) {
+		struct address_space *mapping = old->mapping;
+		void (*freepage)(struct page *);
+		unsigned long flags;
+
+		pgoff_t offset = old->index;
+		freepage = mapping->a_ops->freepage;
+
+		get_page(new);
+		new->mapping = mapping;
+		new->index = offset;
+
+		spin_lock_irqsave(&mapping->tree_lock, flags);
+		__delete_from_page_cache(old, NULL);
+		error = page_cache_tree_insert(mapping, new, NULL);
+		BUG_ON(error);
+
+		/*
+		 * hugetlb pages do not participate in page cache accounting.
+		 */
+		if (!PageHuge(new))
+			__inc_node_page_state(new, NR_FILE_PAGES);
+		if (PageSwapBacked(new))
+			__inc_node_page_state(new, NR_SHMEM);
+		spin_unlock_irqrestore(&mapping->tree_lock, flags);
+		mem_cgroup_migrate(old, new);
+		radix_tree_preload_end();
+		if (freepage)
+			freepage(old);
+		put_page(old);
+	}
+
+	return error;
+}
+EXPORT_SYMBOL_GPL(replace_page_cache_page);
+
+static int __add_to_page_cache_locked(struct page *page,
+				      struct address_space *mapping,
+				      pgoff_t offset, gfp_t gfp_mask,
+				      void **shadowp)
+{
+	int huge = PageHuge(page);
+	struct mem_cgroup *memcg;
+	int error;
+
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	VM_BUG_ON_PAGE(PageSwapBacked(page), page);
+
+	if (!huge) {
+		error = mem_cgroup_try_charge(page, current->mm,
+					      gfp_mask, &memcg, false);
+		if (error)
+			return error;
+	}
+
+	error = radix_tree_maybe_preload(gfp_mask & ~__GFP_HIGHMEM);
+	if (error) {
+		if (!huge)
+			mem_cgroup_cancel_charge(page, memcg, false);
+		return error;
+	}
+
+	get_page(page);
+	page->mapping = mapping;
+	page->index = offset;
+
+	spin_lock_irq(&mapping->tree_lock);
+	error = page_cache_tree_insert(mapping, page, shadowp);
+	radix_tree_preload_end();
+	if (unlikely(error))
+		goto err_insert;
+
+	/* hugetlb pages do not participate in page cache accounting. */
+	if (!huge)
+		__inc_node_page_state(page, NR_FILE_PAGES);
+	spin_unlock_irq(&mapping->tree_lock);
+	if (!huge)
+		mem_cgroup_commit_charge(page, memcg, false, false);
+	trace_mm_filemap_add_to_page_cache(page);
+	return 0;
+err_insert:
+	page->mapping = NULL;
+	/* Leave page->index set: truncation relies upon it */
+	spin_unlock_irq(&mapping->tree_lock);
+	if (!huge)
+		mem_cgroup_cancel_charge(page, memcg, false);
+	put_page(page);
+	return error;
+}
+
+/**
+ * add_to_page_cache_locked - add a locked page to the pagecache
+ * @page:	page to add
+ * @mapping:	the page's address_space
+ * @offset:	page index
+ * @gfp_mask:	page allocation mode
+ *
+ * This function is used to add a page to the pagecache. It must be locked.
+ * This function does not add the page to the LRU.  The caller must do that.
+ */
+int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
+		pgoff_t offset, gfp_t gfp_mask)
+{
+	return __add_to_page_cache_locked(page, mapping, offset,
+					  gfp_mask, NULL);
+}
+EXPORT_SYMBOL(add_to_page_cache_locked);
+
+int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
+				pgoff_t offset, gfp_t gfp_mask)
+{
+	void *shadow = NULL;
+	int ret;
+
+	__SetPageLocked(page);
+	ret = __add_to_page_cache_locked(page, mapping, offset,
+					 gfp_mask, &shadow);
+	if (unlikely(ret))
+		__ClearPageLocked(page);
+	else {
+		/*
+		 * The page might have been evicted from cache only
+		 * recently, in which case it should be activated like
+		 * any other repeatedly accessed page.
+		 * The exception is pages getting rewritten; evicting other
+		 * data from the working set, only to cache data that will
+		 * get overwritten with something else, is a waste of memory.
+		 */
+		if (!(gfp_mask & __GFP_WRITE) &&
+		    shadow && workingset_refault(shadow)) {
+			SetPageActive(page);
+			workingset_activation(page);
+		} else
+			ClearPageActive(page);
+		lru_cache_add(page);
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(add_to_page_cache_lru);
+
+#ifdef CONFIG_NUMA
+struct page *__page_cache_alloc(gfp_t gfp)
+{
+	int n;
+	struct page *page;
+
+	if (cpuset_do_page_mem_spread()) {
+		unsigned int cpuset_mems_cookie;
+		do {
+			cpuset_mems_cookie = read_mems_allowed_begin();
+			n = cpuset_mem_spread_node();
+			page = __alloc_pages_node(n, gfp, 0);
+		} while (!page && read_mems_allowed_retry(cpuset_mems_cookie));
+
+		return page;
+	}
+	return alloc_pages(gfp, 0);
+}
+EXPORT_SYMBOL(__page_cache_alloc);
+#endif
+
+/*
+ * In order to wait for pages to become available there must be
+ * waitqueues associated with pages. By using a hash table of
+ * waitqueues where the bucket discipline is to maintain all
+ * waiters on the same queue and wake all when any of the pages
+ * become available, and for the woken contexts to check to be
+ * sure the appropriate page became available, this saves space
+ * at a cost of "thundering herd" phenomena during rare hash
+ * collisions.
+ */
+#define PAGE_WAIT_TABLE_BITS 8
+#define PAGE_WAIT_TABLE_SIZE (1 << PAGE_WAIT_TABLE_BITS)
+static wait_queue_head_t page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;
+
+static wait_queue_head_t *page_waitqueue(struct page *page)
+{
+	return &page_wait_table[hash_ptr(page, PAGE_WAIT_TABLE_BITS)];
+}
+
+void __init pagecache_init(void)
+{
+	int i;
+
+	for (i = 0; i < PAGE_WAIT_TABLE_SIZE; i++)
+		init_waitqueue_head(&page_wait_table[i]);
+
+	page_writeback_init();
+}
+
+struct wait_page_key {
+	struct page *page;
+	int bit_nr;
+	int page_match;
+};
+
+struct wait_page_queue {
+	struct page *page;
+	int bit_nr;
+	wait_queue_t wait;
+};
+
+static int wake_page_function(wait_queue_t *wait, unsigned mode, int sync, void *arg)
+{
+	struct wait_page_key *key = arg;
+	struct wait_page_queue *wait_page
+		= container_of(wait, struct wait_page_queue, wait);
+
+	if (wait_page->page != key->page)
+	       return 0;
+	key->page_match = 1;
+
+	if (wait_page->bit_nr != key->bit_nr)
+		return 0;
+	if (test_bit(key->bit_nr, &key->page->flags))
+		return 0;
+
+	return autoremove_wake_function(wait, mode, sync, key);
+}
+
+static void wake_up_page_bit(struct page *page, int bit_nr)
+{
+	wait_queue_head_t *q = page_waitqueue(page);
+	struct wait_page_key key;
+	unsigned long flags;
+
+	key.page = page;
+	key.bit_nr = bit_nr;
+	key.page_match = 0;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__wake_up_locked_key(q, TASK_NORMAL, &key);
+	/*
+	 * It is possible for other pages to have collided on the waitqueue
+	 * hash, so in that case check for a page match. That prevents a long-
+	 * term waiter
+	 *
+	 * It is still possible to miss a case here, when we woke page waiters
+	 * and removed them from the waitqueue, but there are still other
+	 * page waiters.
+	 */
+	if (!waitqueue_active(q) || !key.page_match) {
+		ClearPageWaiters(page);
+		/*
+		 * It's possible to miss clearing Waiters here, when we woke
+		 * our page waiters, but the hashed waitqueue has waiters for
+		 * other pages on it.
+		 *
+		 * That's okay, it's a rare case. The next waker will clear it.
+		 */
+	}
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+
+static void wake_up_page(struct page *page, int bit)
+{
+	if (!PageWaiters(page))
+		return;
+	wake_up_page_bit(page, bit);
+}
+
+static inline int wait_on_page_bit_common(wait_queue_head_t *q,
+		struct page *page, int bit_nr, int state, bool lock)
+{
+	struct wait_page_queue wait_page;
+	wait_queue_t *wait = &wait_page.wait;
+	int ret = 0;
+
+	init_wait(wait);
+	wait->func = wake_page_function;
+	wait_page.page = page;
+	wait_page.bit_nr = bit_nr;
+
+	for (;;) {
+		spin_lock_irq(&q->lock);
+
+		if (likely(list_empty(&wait->task_list))) {
+			if (lock)
+				__add_wait_queue_tail_exclusive(q, wait);
+			else
+				__add_wait_queue(q, wait);
+			SetPageWaiters(page);
+		}
+
+		set_current_state(state);
+
+		spin_unlock_irq(&q->lock);
+
+		if (likely(test_bit(bit_nr, &page->flags))) {
+			io_schedule();
+			if (unlikely(signal_pending_state(state, current))) {
+				ret = -EINTR;
+				break;
+			}
+		}
+
+		if (lock) {
+			if (!test_and_set_bit_lock(bit_nr, &page->flags))
+				break;
+		} else {
+			if (!test_bit(bit_nr, &page->flags))
+				break;
+		}
+	}
+
+	finish_wait(q, wait);
+
+	/*
+	 * A signal could leave PageWaiters set. Clearing it here if
+	 * !waitqueue_active would be possible (by open-coding finish_wait),
+	 * but still fail to catch it in the case of wait hash collision. We
+	 * already can fail to clear wait hash collision cases, so don't
+	 * bother with signals either.
+	 */
+
+	return ret;
+}
+
+void wait_on_page_bit(struct page *page, int bit_nr)
+{
+	wait_queue_head_t *q = page_waitqueue(page);
+	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, false);
+}
+EXPORT_SYMBOL(wait_on_page_bit);
+
+int wait_on_page_bit_killable(struct page *page, int bit_nr)
+{
+	wait_queue_head_t *q = page_waitqueue(page);
+	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, false);
+}
+
+/**
+ * add_page_wait_queue - Add an arbitrary waiter to a page's wait queue
+ * @page: Page defining the wait queue of interest
+ * @waiter: Waiter to add to the queue
+ *
+ * Add an arbitrary @waiter to the wait queue for the nominated @page.
+ */
+void add_page_wait_queue(struct page *page, wait_queue_t *waiter)
+{
+	wait_queue_head_t *q = page_waitqueue(page);
+	unsigned long flags;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__add_wait_queue(q, waiter);
+	SetPageWaiters(page);
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+EXPORT_SYMBOL_GPL(add_page_wait_queue);
+
+#ifndef clear_bit_unlock_is_negative_byte
+
+/*
+ * PG_waiters is the high bit in the same byte as PG_lock.
+ *
+ * On x86 (and on many other architectures), we can clear PG_lock and
+ * test the sign bit at the same time. But if the architecture does
+ * not support that special operation, we just do this all by hand
+ * instead.
+ *
+ * The read of PG_waiters has to be after (or concurrently with) PG_locked
+ * being cleared, but a memory barrier should be unneccssary since it is
+ * in the same byte as PG_locked.
+ */
+static inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem)
+{
+	clear_bit_unlock(nr, mem);
+	/* smp_mb__after_atomic(); */
+	return test_bit(PG_waiters, mem);
+}
+
+#endif
+
+/**
+ * unlock_page - unlock a locked page
+ * @page: the page
+ *
+ * Unlocks the page and wakes up sleepers in ___wait_on_page_locked().
+ * Also wakes sleepers in wait_on_page_writeback() because the wakeup
+ * mechanism between PageLocked pages and PageWriteback pages is shared.
+ * But that's OK - sleepers in wait_on_page_writeback() just go back to sleep.
+ *
+ * Note that this depends on PG_waiters being the sign bit in the byte
+ * that contains PG_locked - thus the BUILD_BUG_ON(). That allows us to
+ * clear the PG_locked bit and test PG_waiters at the same time fairly
+ * portably (architectures that do LL/SC can test any bit, while x86 can
+ * test the sign bit).
+ */
+void unlock_page(struct page *page)
+{
+	BUILD_BUG_ON(PG_waiters != 7);
+	page = compound_head(page);
+	VM_BUG_ON_PAGE(!PageLocked(page), page);
+	if (clear_bit_unlock_is_negative_byte(PG_locked, &page->flags))
+		wake_up_page_bit(page, PG_locked);
+}
+EXPORT_SYMBOL(unlock_page);
+
+/**
+ * end_page_writeback - end writeback against a page
+ * @page: the page
+ */
+void end_page_writeback(struct page *page)
+{
+	/*
+	 * TestClearPageReclaim could be used here but it is an atomic
+	 * operation and overkill in this particular case. Failing to
+	 * shuffle a page marked for immediate reclaim is too mild to
+	 * justify taking an atomic operation penalty at the end of
+	 * ever page writeback.
+	 */
+	if (PageReclaim(page)) {
+		ClearPageReclaim(page);
+		rotate_reclaimable_page(page);
+	}
+
+	if (!test_clear_page_writeback(page))
+		BUG();
+
+	smp_mb__after_atomic();
+	wake_up_page(page, PG_writeback);
+}
+EXPORT_SYMBOL(end_page_writeback);
+
+/*
+ * After completing I/O on a page, call this routine to update the page
+ * flags appropriately
+ */
+void page_endio(struct page *page, bool is_write, int err)
+{
+	if (!is_write) {
+		if (!err) {
+			SetPageUptodate(page);
+		} else {
+			ClearPageUptodate(page);
+			SetPageError(page);
+		}
+		unlock_page(page);
+	} else {
+		if (err) {
+			struct address_space *mapping;
+
+			SetPageError(page);
+			mapping = page_mapping(page);
+			if (mapping)
+				mapping_set_error(mapping, err);
+		}
+		end_page_writeback(page);
+	}
+}
+EXPORT_SYMBOL_GPL(page_endio);
+
+/**
+ * __lock_page - get a lock on the page, assuming we need to sleep to get it
+ * @__page: the page to lock
+ */
+void __lock_page(struct page *__page)
+{
+	struct page *page = compound_head(__page);
+	wait_queue_head_t *q = page_waitqueue(page);
+	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, true);
+}
+EXPORT_SYMBOL(__lock_page);
+
+int __lock_page_killable(struct page *__page)
+{
+	struct page *page = compound_head(__page);
+	wait_queue_head_t *q = page_waitqueue(page);
+	return wait_on_page_bit_common(q, page, PG_locked, TASK_KILLABLE, true);
+}
+EXPORT_SYMBOL_GPL(__lock_page_killable);
+
+/*
+ * Return values:
+ * 1 - page is locked; mmap_sem is still held.
+ * 0 - page is not locked.
+ *     mmap_sem has been released (up_read()), unless flags had both
+ *     FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_RETRY_NOWAIT set, in
+ *     which case mmap_sem is still held.
+ *
+ * If neither ALLOW_RETRY nor KILLABLE are set, will always return 1
+ * with the page locked and the mmap_sem unperturbed.
+ */
+int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
+			 unsigned int flags)
+{
+	if (flags & FAULT_FLAG_ALLOW_RETRY) {
+		/*
+		 * CAUTION! In this case, mmap_sem is not released
+		 * even though return 0.
+		 */
+		if (flags & FAULT_FLAG_RETRY_NOWAIT)
+			return 0;
+
+		up_read(&mm->mmap_sem);
+		if (flags & FAULT_FLAG_KILLABLE)
+			wait_on_page_locked_killable(page);
+		else
+			wait_on_page_locked(page);
+		return 0;
+	} else {
+		if (flags & FAULT_FLAG_KILLABLE) {
+			int ret;
+
+			ret = __lock_page_killable(page);
+			if (ret) {
+				up_read(&mm->mmap_sem);
+				return 0;
+			}
+		} else
+			__lock_page(page);
+		return 1;
+	}
+}
+
+/**
+ * page_cache_next_hole - find the next hole (not-present entry)
+ * @mapping: mapping
+ * @index: index
+ * @max_scan: maximum range to search
+ *
+ * Search the set [index, min(index+max_scan-1, MAX_INDEX)] for the
+ * lowest indexed hole.
+ *
+ * Returns: the index of the hole if found, otherwise returns an index
+ * outside of the set specified (in which case 'return - index >=
+ * max_scan' will be true). In rare cases of index wrap-around, 0 will
+ * be returned.
+ *
+ * page_cache_next_hole may be called under rcu_read_lock. However,
+ * like radix_tree_gang_lookup, this will not atomically search a
+ * snapshot of the tree at a single point in time. For example, if a
+ * hole is created at index 5, then subsequently a hole is created at
+ * index 10, page_cache_next_hole covering both indexes may return 10
+ * if called under rcu_read_lock.
+ */
+pgoff_t page_cache_next_hole(struct address_space *mapping,
+			     pgoff_t index, unsigned long max_scan)
+{
+	unsigned long i;
+
+	for (i = 0; i < max_scan; i++) {
+		struct page *page;
+
+		page = radix_tree_lookup(&mapping->page_tree, index);
+		if (!page || radix_tree_exceptional_entry(page))
+			break;
+		index++;
+		if (index == 0)
+			break;
+	}
+
+	return index;
+}
+EXPORT_SYMBOL(page_cache_next_hole);
+
+/**
+ * page_cache_prev_hole - find the prev hole (not-present entry)
+ * @mapping: mapping
+ * @index: index
+ * @max_scan: maximum range to search
+ *
+ * Search backwards in the range [max(index-max_scan+1, 0), index] for
+ * the first hole.
+ *
+ * Returns: the index of the hole if found, otherwise returns an index
+ * outside of the set specified (in which case 'index - return >=
+ * max_scan' will be true). In rare cases of wrap-around, ULONG_MAX
+ * will be returned.
+ *
+ * page_cache_prev_hole may be called under rcu_read_lock. However,
+ * like radix_tree_gang_lookup, this will not atomically search a
+ * snapshot of the tree at a single point in time. For example, if a
+ * hole is created at index 10, then subsequently a hole is created at
+ * index 5, page_cache_prev_hole covering both indexes may return 5 if
+ * called under rcu_read_lock.
+ */
+pgoff_t page_cache_prev_hole(struct address_space *mapping,
+			     pgoff_t index, unsigned long max_scan)
+{
+	unsigned long i;
+
+	for (i = 0; i < max_scan; i++) {
+		struct page *page;
+
+		page = radix_tree_lookup(&mapping->page_tree, index);
+		if (!page || radix_tree_exceptional_entry(page))
+			break;
+		index--;
+		if (index == ULONG_MAX)
+			break;
+	}
+
+	return index;
+}
+EXPORT_SYMBOL(page_cache_prev_hole);
+
+/**
+ * find_get_entry - find and get a page cache entry
+ * @mapping: the address_space to search
+ * @offset: the page cache index
+ *
+ * Looks up the page cache slot at @mapping & @offset.  If there is a
+ * page cache page, it is returned with an increased refcount.
+ *
+ * If the slot holds a shadow entry of a previously evicted page, or a
+ * swap entry from shmem/tmpfs, it is returned.
+ *
+ * Otherwise, %NULL is returned.
+ */
+struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
+{
+	void **pagep;
+	struct page *head, *page;
+
+	rcu_read_lock();
+repeat:
+	page = NULL;
+	pagep = radix_tree_lookup_slot(&mapping->page_tree, offset);
+	if (pagep) {
+		page = radix_tree_deref_slot(pagep);
+		if (unlikely(!page))
+			goto out;
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page))
+				goto repeat;
+			/*
+			 * A shadow entry of a recently evicted page,
+			 * or a swap entry from shmem/tmpfs.  Return
+			 * it without attempting to raise page count.
+			 */
+			goto out;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/*
+		 * Has the page moved?
+		 * This is part of the lockless pagecache protocol. See
+		 * include/linux/pagemap.h for details.
+		 */
+		if (unlikely(page != *pagep)) {
+			put_page(head);
+			goto repeat;
+		}
+	}
+out:
+	rcu_read_unlock();
+
+	return page;
+}
+EXPORT_SYMBOL(find_get_entry);
+
+/**
+ * find_lock_entry - locate, pin and lock a page cache entry
+ * @mapping: the address_space to search
+ * @offset: the page cache index
+ *
+ * Looks up the page cache slot at @mapping & @offset.  If there is a
+ * page cache page, it is returned locked and with an increased
+ * refcount.
+ *
+ * If the slot holds a shadow entry of a previously evicted page, or a
+ * swap entry from shmem/tmpfs, it is returned.
+ *
+ * Otherwise, %NULL is returned.
+ *
+ * find_lock_entry() may sleep.
+ */
+struct page *find_lock_entry(struct address_space *mapping, pgoff_t offset)
+{
+	struct page *page;
+
+repeat:
+	page = find_get_entry(mapping, offset);
+	if (page && !radix_tree_exception(page)) {
+		lock_page(page);
+		/* Has the page been truncated? */
+		if (unlikely(page_mapping(page) != mapping)) {
+			unlock_page(page);
+			put_page(page);
+			goto repeat;
+		}
+		VM_BUG_ON_PAGE(page_to_pgoff(page) != offset, page);
+	}
+	return page;
+}
+EXPORT_SYMBOL(find_lock_entry);
+
+/**
+ * pagecache_get_page - find and get a page reference
+ * @mapping: the address_space to search
+ * @offset: the page index
+ * @fgp_flags: PCG flags
+ * @gfp_mask: gfp mask to use for the page cache data page allocation
+ *
+ * Looks up the page cache slot at @mapping & @offset.
+ *
+ * PCG flags modify how the page is returned.
+ *
+ * @fgp_flags can be:
+ *
+ * - FGP_ACCESSED: the page will be marked accessed
+ * - FGP_LOCK: Page is return locked
+ * - FGP_CREAT: If page is not present then a new page is allocated using
+ *   @gfp_mask and added to the page cache and the VM's LRU
+ *   list. The page is returned locked and with an increased
+ *   refcount. Otherwise, NULL is returned.
+ *
+ * If FGP_LOCK or FGP_CREAT are specified then the function may sleep even
+ * if the GFP flags specified for FGP_CREAT are atomic.
+ *
+ * If there is a page cache page, it is returned with an increased refcount.
+ */
+struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
+	int fgp_flags, gfp_t gfp_mask)
+{
+	struct page *page;
+
+repeat:
+	page = find_get_entry(mapping, offset);
+	if (radix_tree_exceptional_entry(page))
+		page = NULL;
+	if (!page)
+		goto no_page;
+
+	if (fgp_flags & FGP_LOCK) {
+		if (fgp_flags & FGP_NOWAIT) {
+			if (!trylock_page(page)) {
+				put_page(page);
+				return NULL;
+			}
+		} else {
+			lock_page(page);
+		}
+
+		/* Has the page been truncated? */
+		if (unlikely(page->mapping != mapping)) {
+			unlock_page(page);
+			put_page(page);
+			goto repeat;
+		}
+		VM_BUG_ON_PAGE(page->index != offset, page);
+	}
+
+	if (page && (fgp_flags & FGP_ACCESSED))
+		mark_page_accessed(page);
+
+no_page:
+	if (!page && (fgp_flags & FGP_CREAT)) {
+		int err;
+		if ((fgp_flags & FGP_WRITE) && mapping_cap_account_dirty(mapping))
+			gfp_mask |= __GFP_WRITE;
+		if (fgp_flags & FGP_NOFS)
+			gfp_mask &= ~__GFP_FS;
+
+		page = __page_cache_alloc(gfp_mask);
+		if (!page)
+			return NULL;
+
+		if (WARN_ON_ONCE(!(fgp_flags & FGP_LOCK)))
+			fgp_flags |= FGP_LOCK;
+
+		/* Init accessed so avoid atomic mark_page_accessed later */
+		if (fgp_flags & FGP_ACCESSED)
+			__SetPageReferenced(page);
+
+		err = add_to_page_cache_lru(page, mapping, offset,
+				gfp_mask & GFP_RECLAIM_MASK);
+		if (unlikely(err)) {
+			put_page(page);
+			page = NULL;
+			if (err == -EEXIST)
+				goto repeat;
+		}
+	}
+
+	return page;
+}
+EXPORT_SYMBOL(pagecache_get_page);
+
+/**
+ * find_get_entries - gang pagecache lookup
+ * @mapping:	The address_space to search
+ * @start:	The starting page cache index
+ * @nr_entries:	The maximum number of entries
+ * @entries:	Where the resulting entries are placed
+ * @indices:	The cache indices corresponding to the entries in @entries
+ *
+ * find_get_entries() will search for and return a group of up to
+ * @nr_entries entries in the mapping.  The entries are placed at
+ * @entries.  find_get_entries() takes a reference against any actual
+ * pages it returns.
+ *
+ * The search returns a group of mapping-contiguous page cache entries
+ * with ascending indexes.  There may be holes in the indices due to
+ * not-present pages.
+ *
+ * Any shadow entries of evicted pages, or swap entries from
+ * shmem/tmpfs, are included in the returned array.
+ *
+ * find_get_entries() returns the number of pages and shadow entries
+ * which were found.
+ */
+unsigned find_get_entries(struct address_space *mapping,
+			  pgoff_t start, unsigned int nr_entries,
+			  struct page **entries, pgoff_t *indices)
+{
+	void **slot;
+	unsigned int ret = 0;
+	struct radix_tree_iter iter;
+
+	if (!nr_entries)
+		return 0;
+
+	rcu_read_lock();
+	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
+		struct page *head, *page;
+repeat:
+		page = radix_tree_deref_slot(slot);
+		if (unlikely(!page))
+			continue;
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+			/*
+			 * A shadow entry of a recently evicted page, a swap
+			 * entry from shmem/tmpfs or a DAX entry.  Return it
+			 * without attempting to raise page count.
+			 */
+			goto export;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+export:
+		indices[ret] = iter.index;
+		entries[ret] = page;
+		if (++ret == nr_entries)
+			break;
+	}
+	rcu_read_unlock();
+	return ret;
+}
+
+/**
+ * find_get_pages - gang pagecache lookup
+ * @mapping:	The address_space to search
+ * @start:	The starting page index
+ * @nr_pages:	The maximum number of pages
+ * @pages:	Where the resulting pages are placed
+ *
+ * find_get_pages() will search for and return a group of up to
+ * @nr_pages pages in the mapping.  The pages are placed at @pages.
+ * find_get_pages() takes a reference against the returned pages.
+ *
+ * The search returns a group of mapping-contiguous pages with ascending
+ * indexes.  There may be holes in the indices due to not-present pages.
+ *
+ * find_get_pages() returns the number of pages which were found.
+ */
+unsigned find_get_pages(struct address_space *mapping, pgoff_t start,
+			    unsigned int nr_pages, struct page **pages)
+{
+	struct radix_tree_iter iter;
+	void **slot;
+	unsigned ret = 0;
+
+	if (unlikely(!nr_pages))
+		return 0;
+
+	rcu_read_lock();
+	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
+		struct page *head, *page;
+repeat:
+		page = radix_tree_deref_slot(slot);
+		if (unlikely(!page))
+			continue;
+
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+			/*
+			 * A shadow entry of a recently evicted page,
+			 * or a swap entry from shmem/tmpfs.  Skip
+			 * over it.
+			 */
+			continue;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+
+		pages[ret] = page;
+		if (++ret == nr_pages)
+			break;
+	}
+
+	rcu_read_unlock();
+	return ret;
+}
+
+/**
+ * find_get_pages_contig - gang contiguous pagecache lookup
+ * @mapping:	The address_space to search
+ * @index:	The starting page index
+ * @nr_pages:	The maximum number of pages
+ * @pages:	Where the resulting pages are placed
+ *
+ * find_get_pages_contig() works exactly like find_get_pages(), except
+ * that the returned number of pages are guaranteed to be contiguous.
+ *
+ * find_get_pages_contig() returns the number of pages which were found.
+ */
+unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
+			       unsigned int nr_pages, struct page **pages)
+{
+	struct radix_tree_iter iter;
+	void **slot;
+	unsigned int ret = 0;
+
+	if (unlikely(!nr_pages))
+		return 0;
+
+	rcu_read_lock();
+	radix_tree_for_each_contig(slot, &mapping->page_tree, &iter, index) {
+		struct page *head, *page;
+repeat:
+		page = radix_tree_deref_slot(slot);
+		/* The hole, there no reason to continue */
+		if (unlikely(!page))
+			break;
+
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+			/*
+			 * A shadow entry of a recently evicted page,
+			 * or a swap entry from shmem/tmpfs.  Stop
+			 * looking for contiguous pages.
+			 */
+			break;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/*
+		 * must check mapping and index after taking the ref.
+		 * otherwise we can get both false positives and false
+		 * negatives, which is just confusing to the caller.
+		 */
+		if (page->mapping == NULL || page_to_pgoff(page) != iter.index) {
+			put_page(page);
+			break;
+		}
+
+		pages[ret] = page;
+		if (++ret == nr_pages)
+			break;
+	}
+	rcu_read_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(find_get_pages_contig);
+
+/**
+ * find_get_pages_tag - find and return pages that match @tag
+ * @mapping:	the address_space to search
+ * @index:	the starting page index
+ * @tag:	the tag index
+ * @nr_pages:	the maximum number of pages
+ * @pages:	where the resulting pages are placed
+ *
+ * Like find_get_pages, except we only return pages which are tagged with
+ * @tag.   We update @index to index the next page for the traversal.
+ */
+unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
+			int tag, unsigned int nr_pages, struct page **pages)
+{
+	struct radix_tree_iter iter;
+	void **slot;
+	unsigned ret = 0;
+
+	if (unlikely(!nr_pages))
+		return 0;
+
+	rcu_read_lock();
+	radix_tree_for_each_tagged(slot, &mapping->page_tree,
+				   &iter, *index, tag) {
+		struct page *head, *page;
+repeat:
+		page = radix_tree_deref_slot(slot);
+		if (unlikely(!page))
+			continue;
+
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+			/*
+			 * A shadow entry of a recently evicted page.
+			 *
+			 * Those entries should never be tagged, but
+			 * this tree walk is lockless and the tags are
+			 * looked up in bulk, one radix tree node at a
+			 * time, so there is a sizable window for page
+			 * reclaim to evict a page we saw tagged.
+			 *
+			 * Skip over it.
+			 */
+			continue;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+
+		pages[ret] = page;
+		if (++ret == nr_pages)
+			break;
+	}
+
+	rcu_read_unlock();
+
+	if (ret)
+		*index = pages[ret - 1]->index + 1;
+
+	return ret;
+}
+EXPORT_SYMBOL(find_get_pages_tag);
+
+/**
+ * find_get_entries_tag - find and return entries that match @tag
+ * @mapping:	the address_space to search
+ * @start:	the starting page cache index
+ * @tag:	the tag index
+ * @nr_entries:	the maximum number of entries
+ * @entries:	where the resulting entries are placed
+ * @indices:	the cache indices corresponding to the entries in @entries
+ *
+ * Like find_get_entries, except we only return entries which are tagged with
+ * @tag.
+ */
+unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
+			int tag, unsigned int nr_entries,
+			struct page **entries, pgoff_t *indices)
+{
+	void **slot;
+	unsigned int ret = 0;
+	struct radix_tree_iter iter;
+
+	if (!nr_entries)
+		return 0;
+
+	rcu_read_lock();
+	radix_tree_for_each_tagged(slot, &mapping->page_tree,
+				   &iter, start, tag) {
+		struct page *head, *page;
+repeat:
+		page = radix_tree_deref_slot(slot);
+		if (unlikely(!page))
+			continue;
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+
+			/*
+			 * A shadow entry of a recently evicted page, a swap
+			 * entry from shmem/tmpfs or a DAX entry.  Return it
+			 * without attempting to raise page count.
+			 */
+			goto export;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+export:
+		indices[ret] = iter.index;
+		entries[ret] = page;
+		if (++ret == nr_entries)
+			break;
+	}
+	rcu_read_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(find_get_entries_tag);
+
+/*
+ * CD/DVDs are error prone. When a medium error occurs, the driver may fail
+ * a _large_ part of the i/o request. Imagine the worst scenario:
+ *
+ *      ---R__________________________________________B__________
+ *         ^ reading here                             ^ bad block(assume 4k)
+ *
+ * read(R) => miss => readahead(R...B) => media error => frustrating retries
+ * => failing the whole request => read(R) => read(R+1) =>
+ * readahead(R+1...B+1) => bang => read(R+2) => read(R+3) =>
+ * readahead(R+3...B+2) => bang => read(R+3) => read(R+4) =>
+ * readahead(R+4...B+3) => bang => read(R+4) => read(R+5) => ......
+ *
+ * It is going insane. Fix it by quickly scaling down the readahead size.
+ */
+static void shrink_readahead_size_eio(struct file *filp,
+					struct file_ra_state *ra)
+{
+	ra->ra_pages /= 4;
+}
+
+//PARALFETCH
+extern unsigned long long int flashfetch_readahead_hit;
+extern unsigned long long int flashfetch_prefetch_hit;
+
+/**
+ * do_generic_file_read - generic file read routine
+ * @filp:	the file to read
+ * @ppos:	current file position
+ * @iter:	data destination
+ * @written:	already copied
+ *
+ * This is a generic file read routine, and uses the
+ * mapping->a_ops->readpage() function for the actual low-level stuff.
+ *
+ * This is really ugly. But the goto's actually try to clarify some
+ * of the logic when it comes to error handling etc.
+ */
+static ssize_t do_generic_file_read(struct file *filp, loff_t *ppos,
+		struct iov_iter *iter, ssize_t written)
+{
+	struct address_space *mapping = filp->f_mapping;
+	struct inode *inode = mapping->host;
+	struct file_ra_state *ra = &filp->f_ra;
+	pgoff_t index;
+	pgoff_t last_index;
+	pgoff_t prev_index;
+	unsigned long offset;      /* offset into pagecache page */
+	unsigned int prev_offset;
+	int error = 0;
+
+	if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
+		return 0;
+	iov_iter_truncate(iter, inode->i_sb->s_maxbytes);
+
+	index = *ppos >> PAGE_SHIFT;
+	prev_index = ra->prev_pos >> PAGE_SHIFT;
+	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
+	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
+	offset = *ppos & ~PAGE_MASK;
+
+	for (;;) {
+		struct page *page;
+		pgoff_t end_index;
+		loff_t isize;
+		unsigned long nr, ret;
+
+		cond_resched();
+find_page:
+		if (fatal_signal_pending(current)) {
+			error = -EINTR;
+			goto out;
+		}
+
+		page = find_get_page(mapping, index);
+		if (!page) {
+			page_cache_sync_readahead(mapping,
+					ra, filp,
+					index, last_index - index);
+			page = find_get_page(mapping, index);
+			if (unlikely(page == NULL))
+				goto no_cached_page;
+		}
+#ifdef CONFIG_PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+#endif
+		if (PageReadahead(page)) {
+			page_cache_async_readahead(mapping,
+					ra, filp, page,
+					index, last_index - index);
+		}
+		if (!PageUptodate(page)) {
+#ifdef CONFIG_PARALFETCH
+			if (TestClearPagePrefetch(page)) 
+				flashfetch_readahead_hit++;
+#endif
+			/*
+			 * See comment in do_read_cache_page on why
+			 * wait_on_page_locked is used to avoid unnecessarily
+			 * serialisations and why it's safe.
+			 */
+			error = wait_on_page_locked_killable(page);
+			if (unlikely(error))
+				goto readpage_error;
+			if (PageUptodate(page))
+				goto page_ok;
+
+			if (inode->i_blkbits == PAGE_SHIFT ||
+					!mapping->a_ops->is_partially_uptodate)
+				goto page_not_up_to_date;
+			/* pipes can't handle partially uptodate pages */
+			if (unlikely(iter->type & ITER_PIPE))
+				goto page_not_up_to_date;
+			if (!trylock_page(page))
+				goto page_not_up_to_date;
+			/* Did it get truncated before we got the lock? */
+			if (!page->mapping)
+				goto page_not_up_to_date_locked;
+			if (!mapping->a_ops->is_partially_uptodate(page,
+							offset, iter->count))
+				goto page_not_up_to_date_locked;
+			unlock_page(page);
+		}
+#ifdef CONFIG_PARALFETCH
+		//PARALFECH
+		else if (TestClearPagePrefetch(page)) 
+				flashfetch_prefetch_hit++;
+#endif
+page_ok:
+		/*
+		 * i_size must be checked after we know the page is Uptodate.
+		 *
+		 * Checking i_size after the check allows us to calculate
+		 * the correct value for "nr", which means the zero-filled
+		 * part of the page is not copied back to userspace (unless
+		 * another truncate extends the file - this is desired though).
+		 */
+
+		isize = i_size_read(inode);
+		end_index = (isize - 1) >> PAGE_SHIFT;
+		if (unlikely(!isize || index > end_index)) {
+			put_page(page);
+			goto out;
+		}
+
+		/* nr is the maximum number of bytes to copy from this page */
+		nr = PAGE_SIZE;
+		if (index == end_index) {
+			nr = ((isize - 1) & ~PAGE_MASK) + 1;
+			if (nr <= offset) {
+				put_page(page);
+				goto out;
+			}
+		}
+		nr = nr - offset;
+
+		/* If users can be writing to this page using arbitrary
+		 * virtual addresses, take care about potential aliasing
+		 * before reading the page on the kernel side.
+		 */
+		if (mapping_writably_mapped(mapping))
+			flush_dcache_page(page);
+
+		/*
+		 * When a sequential read accesses a page several times,
+		 * only mark it as accessed the first time.
+		 */
+		if (prev_index != index || offset != prev_offset)
+			mark_page_accessed(page);
+		prev_index = index;
+
+		/*
+		 * Ok, we have the page, and it's up-to-date, so
+		 * now we can copy it to user space...
+		 */
+
+		ret = copy_page_to_iter(page, offset, nr, iter);
+		offset += ret;
+		index += offset >> PAGE_SHIFT;
+		offset &= ~PAGE_MASK;
+		prev_offset = offset;
+
+		put_page(page);
+		written += ret;
+		if (!iov_iter_count(iter))
+			goto out;
+		if (ret < nr) {
+			error = -EFAULT;
+			goto out;
+		}
+		continue;
+
+page_not_up_to_date:
+		/* Get exclusive access to the page ... */
+		error = lock_page_killable(page);
+		if (unlikely(error))
+			goto readpage_error;
+
+page_not_up_to_date_locked:
+		/* Did it get truncated before we got the lock? */
+		if (!page->mapping) {
+			unlock_page(page);
+			put_page(page);
+			continue;
+		}
+
+		/* Did somebody else fill it already? */
+		if (PageUptodate(page)) {
+			unlock_page(page);
+			goto page_ok;
+		}
+
+readpage:
+		/*
+		 * A previous I/O error may have been due to temporary
+		 * failures, eg. multipath errors.
+		 * PG_error will be set again if readpage fails.
+		 */
+		ClearPageError(page);
+		/* Start the actual read. The read will unlock the page. */
+		error = mapping->a_ops->readpage(filp, page);
+
+		if (unlikely(error)) {
+			if (error == AOP_TRUNCATED_PAGE) {
+				put_page(page);
+				error = 0;
+				goto find_page;
+			}
+			goto readpage_error;
+		}
+
+		if (!PageUptodate(page)) {
+			error = lock_page_killable(page);
+			if (unlikely(error))
+				goto readpage_error;
+			if (!PageUptodate(page)) {
+				if (page->mapping == NULL) {
+					/*
+					 * invalidate_mapping_pages got it
+					 */
+					unlock_page(page);
+					put_page(page);
+					goto find_page;
+				}
+				unlock_page(page);
+				shrink_readahead_size_eio(filp, ra);
+				error = -EIO;
+				goto readpage_error;
+			}
+			unlock_page(page);
+		}
+
+		goto page_ok;
+
+readpage_error:
+		/* UHHUH! A synchronous read error occurred. Report it */
+		put_page(page);
+		goto out;
+
+no_cached_page:
+		/*
+		 * Ok, it wasn't cached, so we need to create a new
+		 * page..
+		 */
+		page = page_cache_alloc_cold(mapping);
+		if (!page) {
+			error = -ENOMEM;
+			goto out;
+		}
+		error = add_to_page_cache_lru(page, mapping, index,
+				mapping_gfp_constraint(mapping, GFP_KERNEL));
+		if (error) {
+			put_page(page);
+			if (error == -EEXIST) {
+				error = 0;
+				goto find_page;
+			}
+			goto out;
+		}
+		goto readpage;
+	}
+
+out:
+	ra->prev_pos = prev_index;
+	ra->prev_pos <<= PAGE_SHIFT;
+	ra->prev_pos |= prev_offset;
+
+	*ppos = ((loff_t)index << PAGE_SHIFT) + offset;
+	file_accessed(filp);
+	return written ? written : error;
+}
+
+/**
+ * generic_file_read_iter - generic filesystem read routine
+ * @iocb:	kernel I/O control block
+ * @iter:	destination for the data read
+ *
+ * This is the "read_iter()" routine for all filesystems
+ * that can use the page cache directly.
+ */
+ssize_t
+generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	struct file *file = iocb->ki_filp;
+	ssize_t retval = 0;
+	size_t count = iov_iter_count(iter);
+
+	if (!count)
+		goto out; /* skip atime */
+
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		struct address_space *mapping = file->f_mapping;
+		struct inode *inode = mapping->host;
+		loff_t size;
+
+		size = i_size_read(inode);
+		retval = filemap_write_and_wait_range(mapping, iocb->ki_pos,
+					iocb->ki_pos + count - 1);
+		if (retval < 0)
+			goto out;
+
+		file_accessed(file);
+
+		retval = mapping->a_ops->direct_IO(iocb, iter);
+		if (retval >= 0) {
+			iocb->ki_pos += retval;
+			count -= retval;
+		}
+		iov_iter_revert(iter, count - iov_iter_count(iter));
+
+		/*
+		 * Btrfs can have a short DIO read if we encounter
+		 * compressed extents, so if there was an error, or if
+		 * we've already read everything we wanted to, or if
+		 * there was a short read because we hit EOF, go ahead
+		 * and return.  Otherwise fallthrough to buffered io for
+		 * the rest of the read.  Buffered reads will not work for
+		 * DAX files, so don't bother trying.
+		 */
+		if (retval < 0 || !count || iocb->ki_pos >= size ||
+		    IS_DAX(inode))
+			goto out;
+	}
+
+	retval = do_generic_file_read(file, &iocb->ki_pos, iter, retval);
+out:
+	return retval;
+}
+EXPORT_SYMBOL(generic_file_read_iter);
+
+#ifdef CONFIG_MMU
+/**
+ * page_cache_read - adds requested page to the page cache if not already there
+ * @file:	file to read
+ * @offset:	page index
+ * @gfp_mask:	memory allocation flags
+ *
+ * This adds the requested page to the page cache if it isn't already there,
+ * and schedules an I/O to read in its contents from disk.
+ */
+static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
+{
+	struct address_space *mapping = file->f_mapping;
+	struct page *page;
+	int ret;
+
+	do {
+		page = __page_cache_alloc(gfp_mask|__GFP_COLD);
+		if (!page)
+			return -ENOMEM;
+
+		ret = add_to_page_cache_lru(page, mapping, offset, gfp_mask & GFP_KERNEL);
+		if (ret == 0)
+			ret = mapping->a_ops->readpage(file, page);
+		else if (ret == -EEXIST)
+			ret = 0; /* losing race to add is OK */
+
+		put_page(page);
+
+	} while (ret == AOP_TRUNCATED_PAGE);
+
+	return ret;
+}
+
+#define MMAP_LOTSAMISS  (100)
+
+/*
+ * Synchronous readahead happens when we don't even find
+ * a page in the page cache at all.
+ */
+static void do_sync_mmap_readahead(struct vm_area_struct *vma,
+				   struct file_ra_state *ra,
+				   struct file *file,
+				   pgoff_t offset)
+{
+	struct address_space *mapping = file->f_mapping;
+
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
+	/* If we don't want any read-ahead, don't bother */
+	if (vma->vm_flags & VM_RAND_READ)
+		return;
+	if (!ra->ra_pages)
+		return;
+
+	if (vma->vm_flags & VM_SEQ_READ) {
+		//PARALFETCH
+		/*
+                printk("PF:SMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
+		page_cache_sync_readahead(mapping, ra, file, offset,
+					  ra->ra_pages);
+		return;
+	}
+
+	/* Avoid banging the cache line if not needed */
+	if (ra->mmap_miss < MMAP_LOTSAMISS * 10)
+		ra->mmap_miss++;
+
+	/*
+	 * Do we miss much more than hit in this file? If so,
+	 * stop bothering with read-ahead. It will only hurt.
+	 */
+	if (ra->mmap_miss > MMAP_LOTSAMISS)
+		return;
+
+	/*
+	 * mmap read-around
+	 */
+	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
+	ra->size = ra->ra_pages;
+	ra->async_size = ra->ra_pages / 4;
+
+	//PARALFETCH
+	/*
+        printk("PF:SMRAR:dev:0x%x ino:%lu offs:%lu, size:%u",
+                        inode->i_sb->s_dev, inode->i_ino,
+                        ra->start, ra->size);
+			*/
+
+	ra_submit(ra, mapping, file);
+}
+
+/*
+ * Asynchronous readahead happens when we find the page and PG_readahead,
+ * so we want to possibly extend the readahead further..
+ */
+static void do_async_mmap_readahead(struct vm_area_struct *vma,
+				    struct file_ra_state *ra,
+				    struct file *file,
+				    struct page *page,
+				    pgoff_t offset)
+{
+	struct address_space *mapping = file->f_mapping;
+
+	//PARALFETCH
+        //struct inode *inode = mapping->host;
+	
+	/* If we don't want any read-ahead, don't bother */
+	if (vma->vm_flags & VM_RAND_READ)
+		return;
+	if (ra->mmap_miss > 0)
+		ra->mmap_miss--;
+	if (PageReadahead(page)) {
+		//PARALFETCH
+		/*
+                printk("PF:AMRA:dev:0x%x ino:%lu offs:%lu, size:%u",
+                                inode->i_sb->s_dev, inode->i_ino,
+                                offset, ra->ra_pages);
+				*/
+
+		page_cache_async_readahead(mapping, ra, file,
+					   page, offset, ra->ra_pages);
+	}
+}
+
+/**
+ * filemap_fault - read in file data for page fault handling
+ * @vmf:	struct vm_fault containing details of the fault
+ *
+ * filemap_fault() is invoked via the vma operations vector for a
+ * mapped memory region to read in file data during a page fault.
+ *
+ * The goto's are kind of ugly, but this streamlines the normal case of having
+ * it in the page cache, and handles the special cases reasonably without
+ * having a lot of duplicated code.
+ *
+ * vma->vm_mm->mmap_sem must be held on entry.
+ *
+ * If our return value has VM_FAULT_RETRY set, it's because
+ * lock_page_or_retry() returned 0.
+ * The mmap_sem has usually been released in this case.
+ * See __lock_page_or_retry() for the exception.
+ *
+ * If our return value does not have VM_FAULT_RETRY set, the mmap_sem
+ * has not been released.
+ *
+ * We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set.
+ */
+int filemap_fault(struct vm_fault *vmf)
+{
+	int error;
+	struct file *file = vmf->vma->vm_file;
+	struct address_space *mapping = file->f_mapping;
+	struct file_ra_state *ra = &file->f_ra;
+	struct inode *inode = mapping->host;
+	pgoff_t offset = vmf->pgoff;
+	pgoff_t max_off;
+	struct page *page;
+	int ret = 0;
+
+	max_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	if (unlikely(offset >= max_off))
+		return VM_FAULT_SIGBUS;
+
+	/*
+	 * Do we have something in the page cache already?
+	 */
+	page = find_get_page(mapping, offset);
+	if (likely(page) && !(vmf->flags & FAULT_FLAG_TRIED)) {
+		/*
+		 * We found the page, so try async readahead before
+		 * waiting for the lock.
+		 */
+		do_async_mmap_readahead(vmf->vma, ra, file, page, offset);
+	} else if (!page) {
+		/* No page in the page cache at all */
+		do_sync_mmap_readahead(vmf->vma, ra, file, offset);
+		count_vm_event(PGMAJFAULT);
+		mem_cgroup_count_vm_event(vmf->vma->vm_mm, PGMAJFAULT);
+		ret = VM_FAULT_MAJOR;
+retry_find:
+		page = find_get_page(mapping, offset);
+		if (!page)
+			goto no_cached_page;
+	}
+
+	if (!lock_page_or_retry(page, vmf->vma->vm_mm, vmf->flags)) {
+		put_page(page);
+		return ret | VM_FAULT_RETRY;
+	}
+
+	/* Did it get truncated? */
+	if (unlikely(page->mapping != mapping)) {
+		unlock_page(page);
+		put_page(page);
+		goto retry_find;
+	}
+	VM_BUG_ON_PAGE(page->index != offset, page);
+
+	/*
+	 * We have a locked page in the page cache, now we need to check
+	 * that it's up-to-date. If not, it is going to be due to an error.
+	 */
+	if (unlikely(!PageUptodate(page)))
+		goto page_not_uptodate;
+
+	/*
+	 * Found the page and have a reference on it.
+	 * We must recheck i_size under page lock.
+	 */
+	max_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	if (unlikely(offset >= max_off)) {
+		unlock_page(page);
+		put_page(page);
+		return VM_FAULT_SIGBUS;
+	}
+
+	vmf->page = page;
+	return ret | VM_FAULT_LOCKED;
+
+no_cached_page:
+	/*
+	 * We're only likely to ever get here if MADV_RANDOM is in
+	 * effect.
+	 */
+	error = page_cache_read(file, offset, vmf->gfp_mask);
+
+	/*
+	 * The page we want has now been added to the page cache.
+	 * In the unlikely event that someone removed it in the
+	 * meantime, we'll just come back here and read it again.
+	 */
+	if (error >= 0)
+		goto retry_find;
+
+	/*
+	 * An error return from page_cache_read can result if the
+	 * system is low on memory, or a problem occurs while trying
+	 * to schedule I/O.
+	 */
+	if (error == -ENOMEM)
+		return VM_FAULT_OOM;
+	return VM_FAULT_SIGBUS;
+
+page_not_uptodate:
+	/*
+	 * Umm, take care of errors if the page isn't up-to-date.
+	 * Try to re-read it _once_. We do this synchronously,
+	 * because there really aren't any performance issues here
+	 * and we need to check for errors.
+	 */
+	ClearPageError(page);
+	error = mapping->a_ops->readpage(file, page);
+	if (!error) {
+		wait_on_page_locked(page);
+		if (!PageUptodate(page))
+			error = -EIO;
+	}
+	put_page(page);
+
+	if (!error || error == AOP_TRUNCATED_PAGE)
+		goto retry_find;
+
+	/* Things didn't work out. Return zero to tell the mm layer so. */
+	shrink_readahead_size_eio(file, ra);
+	return VM_FAULT_SIGBUS;
+}
+EXPORT_SYMBOL(filemap_fault);
+
+//PARALFETCH
+extern pid_t launch_pid;
+extern unsigned int flashfetch_monitor_blkio;
+extern struct disk_trace *flashfetch_trace;
+extern int pfault_trace;
+extern int pfault_debug;
+
+void filemap_map_pages(struct vm_fault *vmf,
+		pgoff_t start_pgoff, pgoff_t end_pgoff)
+{
+	struct radix_tree_iter iter;
+	void **slot;
+	struct file *file = vmf->vma->vm_file;
+	struct address_space *mapping = file->f_mapping;
+	pgoff_t last_pgoff = start_pgoff;
+	unsigned long max_idx;
+	struct page *head, *page;
+
+	//PARALFETCH
+        struct inode *inode = mapping->host;
+	int idx;
+        struct disk_log_entry *ff_log;
+	int should_log = 0;
+	/* previously commented
+        if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid))
+	       	printk("PF:filemap_map_pages:dev:0x%x ino:%lu start_pgoff:%lu, end_pgoff:%lu",
+		inode->i_sb->s_dev, inode->i_ino,
+		start_pgoff, end_pgoff);
+		*/
+	if(flashfetch_monitor_blkio && flashfetch_trace && pfault_trace) {
+		if(launch_pid != 0 && (launch_pid == current->pid || launch_pid == current->parent->pid)) {
+			if(inode) {
+				if(inode->i_sb && inode->i_sb->s_dev) {
+					idx = get_log_entry(flashfetch_trace);
+					if(idx >= 0) {
+						ff_log = &(flashfetch_trace->log[idx]);
+						ff_log->ts = ktime_to_ns(ktime_get());
+						ff_log->dev = inode->i_sb->s_dev;
+						ff_log->ino = inode->i_ino;
+						ff_log->blk_num = (u64)start_pgoff;
+						ff_log->blk_len = (u32)(end_pgoff - start_pgoff + 1);
+
+						if(pfault_debug)
+							printk("PFAULT: %llu 0x%x %lu %llu %d\n", ff_log->ts, ff_log->dev, ff_log->ino, ff_log->blk_num, ff_log->blk_len);
+					}
+				}
+			}
+		}
+        }
+
+	rcu_read_lock();
+	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter,
+			start_pgoff) {
+		if (iter.index > end_pgoff) {
+			// make log;
+			break;
+		}
+repeat:
+		page = radix_tree_deref_slot(slot);
+		if (unlikely(!page))
+			goto next;
+		//PARALFETCH
+		else if (TestClearPagePrefetch(page))
+			flashfetch_prefetch_hit++;
+		
+		if (radix_tree_exception(page)) {
+			if (radix_tree_deref_retry(page)) {
+				slot = radix_tree_iter_retry(&iter);
+				continue;
+			}
+			goto next;
+		}
+
+		head = compound_head(page);
+		if (!page_cache_get_speculative(head))
+			goto repeat;
+
+		/* The page was split under us? */
+		if (compound_head(page) != head) {
+			put_page(head);
+			goto repeat;
+		}
+
+		/* Has the page moved? */
+		if (unlikely(page != *slot)) {
+			put_page(head);
+			goto repeat;
+		}
+
+		if (!PageUptodate(page) ||
+				PageReadahead(page) ||
+				PageHWPoison(page))
+			goto skip;
+		if (!trylock_page(page))
+			goto skip;
+
+		if (page->mapping != mapping || !PageUptodate(page))
+			goto unlock;
+
+		max_idx = DIV_ROUND_UP(i_size_read(mapping->host), PAGE_SIZE);
+		if (page->index >= max_idx)
+			goto unlock;
+
+		if (file->f_ra.mmap_miss > 0)
+			file->f_ra.mmap_miss--;
+
+		vmf->address += (iter.index - last_pgoff) << PAGE_SHIFT;
+		if (vmf->pte)
+			vmf->pte += iter.index - last_pgoff;
+		last_pgoff = iter.index;
+		if (alloc_set_pte(vmf, NULL, page))
+			goto unlock;
+		unlock_page(page);
+		goto next;
+unlock:
+		unlock_page(page);
+skip:
+		put_page(page);
+next:
+		/* Huge page is mapped? No need to proceed. */
+		if (pmd_trans_huge(*vmf->pmd))
+			break;
+
+		// make log and reset log start index
+
+		if (iter.index == end_pgoff)
+			break;
+	}
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(filemap_map_pages);
+
+int filemap_page_mkwrite(struct vm_fault *vmf)
+{
+	struct page *page = vmf->page;
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	int ret = VM_FAULT_LOCKED;
+
+	sb_start_pagefault(inode->i_sb);
+	file_update_time(vmf->vma->vm_file);
+	lock_page(page);
+	if (page->mapping != inode->i_mapping) {
+	unlock_page(page);
+		ret = VM_FAULT_NOPAGE;
+		goto out;
+	}
+	/*
+	 * We mark the page dirty already here so that when freeze is in
+	 * progress, we are guaranteed that writeback during freezing will
+	 * see the dirty page and writeprotect it again.
+	 */
+	set_page_dirty(page);
+	wait_for_stable_page(page);
+out:
+	sb_end_pagefault(inode->i_sb);
+	return ret;
+}
+EXPORT_SYMBOL(filemap_page_mkwrite);
+
+const struct vm_operations_struct generic_file_vm_ops = {
+	.fault		= filemap_fault,
+	.map_pages	= filemap_map_pages,
+	.page_mkwrite	= filemap_page_mkwrite,
+};
+
+/* This is used for a general mmap of a disk file */
+
+int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
+{
+	struct address_space *mapping = file->f_mapping;
+
+	if (!mapping->a_ops->readpage)
+		return -ENOEXEC;
+	file_accessed(file);
+	vma->vm_ops = &generic_file_vm_ops;
+	return 0;
+}
+
+/*
+ * This is for filesystems which do not implement ->writepage.
+ */
+int generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	if ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))
+		return -EINVAL;
+	return generic_file_mmap(file, vma);
+}
+#else
+int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
+{
+	return -ENOSYS;
+}
+int generic_file_readonly_mmap(struct file * file, struct vm_area_struct * vma)
+{
+	return -ENOSYS;
+}
+#endif /* CONFIG_MMU */
+
+EXPORT_SYMBOL(generic_file_mmap);
+EXPORT_SYMBOL(generic_file_readonly_mmap);
+
+static struct page *wait_on_page_read(struct page *page)
+{
+	if (!IS_ERR(page)) {
+		wait_on_page_locked(page);
+		if (!PageUptodate(page)) {
+			put_page(page);
+			page = ERR_PTR(-EIO);
+		}
+	}
+	return page;
+}
+
+static struct page *do_read_cache_page(struct address_space *mapping,
+				pgoff_t index,
+				int (*filler)(void *, struct page *),
+				void *data,
+				gfp_t gfp)
+{
+	struct page *page;
+	int err;
+repeat:
+	page = find_get_page(mapping, index);
+	if (!page) {
+		page = __page_cache_alloc(gfp | __GFP_COLD);
+		if (!page)
+			return ERR_PTR(-ENOMEM);
+		err = add_to_page_cache_lru(page, mapping, index, gfp);
+		if (unlikely(err)) {
+			put_page(page);
+			if (err == -EEXIST)
+				goto repeat;
+			/* Presumably ENOMEM for radix tree node */
+			return ERR_PTR(err);
+		}
+
+filler:
+		err = filler(data, page);
+		if (err < 0) {
+			put_page(page);
+			return ERR_PTR(err);
+		}
+
+		page = wait_on_page_read(page);
+		if (IS_ERR(page))
+			return page;
+		goto out;
+	}
+	if (PageUptodate(page))
+		goto out;
+
+	/*
+	 * Page is not up to date and may be locked due one of the following
+	 * case a: Page is being filled and the page lock is held
+	 * case b: Read/write error clearing the page uptodate status
+	 * case c: Truncation in progress (page locked)
+	 * case d: Reclaim in progress
+	 *
+	 * Case a, the page will be up to date when the page is unlocked.
+	 *    There is no need to serialise on the page lock here as the page
+	 *    is pinned so the lock gives no additional protection. Even if the
+	 *    the page is truncated, the data is still valid if PageUptodate as
+	 *    it's a race vs truncate race.
+	 * Case b, the page will not be up to date
+	 * Case c, the page may be truncated but in itself, the data may still
+	 *    be valid after IO completes as it's a read vs truncate race. The
+	 *    operation must restart if the page is not uptodate on unlock but
+	 *    otherwise serialising on page lock to stabilise the mapping gives
+	 *    no additional guarantees to the caller as the page lock is
+	 *    released before return.
+	 * Case d, similar to truncation. If reclaim holds the page lock, it
+	 *    will be a race with remove_mapping that determines if the mapping
+	 *    is valid on unlock but otherwise the data is valid and there is
+	 *    no need to serialise with page lock.
+	 *
+	 * As the page lock gives no additional guarantee, we optimistically
+	 * wait on the page to be unlocked and check if it's up to date and
+	 * use the page if it is. Otherwise, the page lock is required to
+	 * distinguish between the different cases. The motivation is that we
+	 * avoid spurious serialisations and wakeups when multiple processes
+	 * wait on the same page for IO to complete.
+	 */
+	wait_on_page_locked(page);
+	if (PageUptodate(page))
+		goto out;
+
+	/* Distinguish between all the cases under the safety of the lock */
+	lock_page(page);
+
+	/* Case c or d, restart the operation */
+	if (!page->mapping) {
+		unlock_page(page);
+		put_page(page);
+		goto repeat;
+	}
+
+	/* Someone else locked and filled the page in a very small window */
+	if (PageUptodate(page)) {
+		unlock_page(page);
+		goto out;
+	}
+	goto filler;
+
+out:
+	mark_page_accessed(page);
+	return page;
+}
+
+/**
+ * read_cache_page - read into page cache, fill it if needed
+ * @mapping:	the page's address_space
+ * @index:	the page index
+ * @filler:	function to perform the read
+ * @data:	first arg to filler(data, page) function, often left as NULL
+ *
+ * Read into the page cache. If a page already exists, and PageUptodate() is
+ * not set, try to fill the page and wait for it to become unlocked.
+ *
+ * If the page does not get brought uptodate, return -EIO.
+ */
+struct page *read_cache_page(struct address_space *mapping,
+				pgoff_t index,
+				int (*filler)(void *, struct page *),
+				void *data)
+{
+	return do_read_cache_page(mapping, index, filler, data, mapping_gfp_mask(mapping));
+}
+EXPORT_SYMBOL(read_cache_page);
+
+/**
+ * read_cache_page_gfp - read into page cache, using specified page allocation flags.
+ * @mapping:	the page's address_space
+ * @index:	the page index
+ * @gfp:	the page allocator flags to use if allocating
+ *
+ * This is the same as "read_mapping_page(mapping, index, NULL)", but with
+ * any new page allocations done using the specified allocation flags.
+ *
+ * If the page does not get brought uptodate, return -EIO.
+ */
+struct page *read_cache_page_gfp(struct address_space *mapping,
+				pgoff_t index,
+				gfp_t gfp)
+{
+	filler_t *filler = (filler_t *)mapping->a_ops->readpage;
+
+	return do_read_cache_page(mapping, index, filler, NULL, gfp);
+}
+EXPORT_SYMBOL(read_cache_page_gfp);
+
+/*
+ * Performs necessary checks before doing a write
+ *
+ * Can adjust writing position or amount of bytes to write.
+ * Returns appropriate error code that caller should return or
+ * zero in case that write should be allowed.
+ */
+inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = file->f_mapping->host;
+	unsigned long limit = rlimit(RLIMIT_FSIZE);
+	loff_t pos;
+
+	if (!iov_iter_count(from))
+		return 0;
+
+	/* FIXME: this is for backwards compatibility with 2.4 */
+	if (iocb->ki_flags & IOCB_APPEND)
+		iocb->ki_pos = i_size_read(inode);
+
+	pos = iocb->ki_pos;
+
+	if (limit != RLIM_INFINITY) {
+		if (iocb->ki_pos >= limit) {
+			send_sig(SIGXFSZ, current, 0);
+			return -EFBIG;
+		}
+		iov_iter_truncate(from, limit - (unsigned long)pos);
+	}
+
+	/*
+	 * LFS rule
+	 */
+	if (unlikely(pos + iov_iter_count(from) > MAX_NON_LFS &&
+				!(file->f_flags & O_LARGEFILE))) {
+		if (pos >= MAX_NON_LFS)
+			return -EFBIG;
+		iov_iter_truncate(from, MAX_NON_LFS - (unsigned long)pos);
+	}
+
+	/*
+	 * Are we about to exceed the fs block limit ?
+	 *
+	 * If we have written data it becomes a short write.  If we have
+	 * exceeded without writing data we send a signal and return EFBIG.
+	 * Linus frestrict idea will clean these up nicely..
+	 */
+	if (unlikely(pos >= inode->i_sb->s_maxbytes))
+		return -EFBIG;
+
+	iov_iter_truncate(from, inode->i_sb->s_maxbytes - pos);
+	return iov_iter_count(from);
+}
+EXPORT_SYMBOL(generic_write_checks);
+
+int pagecache_write_begin(struct file *file, struct address_space *mapping,
+				loff_t pos, unsigned len, unsigned flags,
+				struct page **pagep, void **fsdata)
+{
+	const struct address_space_operations *aops = mapping->a_ops;
+
+	return aops->write_begin(file, mapping, pos, len, flags,
+							pagep, fsdata);
+}
+EXPORT_SYMBOL(pagecache_write_begin);
+
+int pagecache_write_end(struct file *file, struct address_space *mapping,
+				loff_t pos, unsigned len, unsigned copied,
+				struct page *page, void *fsdata)
+{
+	const struct address_space_operations *aops = mapping->a_ops;
+
+	return aops->write_end(file, mapping, pos, len, copied, page, fsdata);
+}
+EXPORT_SYMBOL(pagecache_write_end);
+
+ssize_t
+generic_file_direct_write(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file	*file = iocb->ki_filp;
+	struct address_space *mapping = file->f_mapping;
+	struct inode	*inode = mapping->host;
+	loff_t		pos = iocb->ki_pos;
+	ssize_t		written;
+	size_t		write_len;
+	pgoff_t		end;
+
+	write_len = iov_iter_count(from);
+	end = (pos + write_len - 1) >> PAGE_SHIFT;
+
+	written = filemap_write_and_wait_range(mapping, pos, pos + write_len - 1);
+	if (written)
+		goto out;
+
+	/*
+	 * After a write we want buffered reads to be sure to go to disk to get
+	 * the new data.  We invalidate clean cached page from the region we're
+	 * about to write.  We do this *before* the write so that we can return
+	 * without clobbering -EIOCBQUEUED from ->direct_IO().
+	 */
+	written = invalidate_inode_pages2_range(mapping,
+					pos >> PAGE_SHIFT, end);
+	/*
+	 * If a page can not be invalidated, return 0 to fall back
+	 * to buffered write.
+	 */
+	if (written) {
+		if (written == -EBUSY)
+			return 0;
+		goto out;
+	}
+
+	written = mapping->a_ops->direct_IO(iocb, from);
+
+	/*
+	 * Finally, try again to invalidate clean pages which might have been
+	 * cached by non-direct readahead, or faulted in by get_user_pages()
+	 * if the source of the write was an mmap'ed region of the file
+	 * we're writing.  Either one is a pretty crazy thing to do,
+	 * so we don't support it 100%.  If this invalidation
+	 * fails, tough, the write still worked...
+	 */
+	invalidate_inode_pages2_range(mapping,
+				pos >> PAGE_SHIFT, end);
+
+	if (written > 0) {
+		pos += written;
+		write_len -= written;
+		if (pos > i_size_read(inode) && !S_ISBLK(inode->i_mode)) {
+			i_size_write(inode, pos);
+			mark_inode_dirty(inode);
+		}
+		iocb->ki_pos = pos;
+	}
+	iov_iter_revert(from, write_len - iov_iter_count(from));
+out:
+	return written;
+}
+EXPORT_SYMBOL(generic_file_direct_write);
+
+/*
+ * Find or create a page at the given pagecache position. Return the locked
+ * page. This function is specifically for buffered writes.
+ */
+struct page *grab_cache_page_write_begin(struct address_space *mapping,
+					pgoff_t index, unsigned flags)
+{
+	struct page *page;
+	int fgp_flags = FGP_LOCK|FGP_WRITE|FGP_CREAT;
+
+	if (flags & AOP_FLAG_NOFS)
+		fgp_flags |= FGP_NOFS;
+
+	page = pagecache_get_page(mapping, index, fgp_flags,
+			mapping_gfp_mask(mapping));
+	if (page)
+		wait_for_stable_page(page);
+
+	return page;
+}
+EXPORT_SYMBOL(grab_cache_page_write_begin);
+
+ssize_t generic_perform_write(struct file *file,
+				struct iov_iter *i, loff_t pos)
+{
+	struct address_space *mapping = file->f_mapping;
+	const struct address_space_operations *a_ops = mapping->a_ops;
+	long status = 0;
+	ssize_t written = 0;
+	unsigned int flags = 0;
+
+	do {
+		struct page *page;
+		unsigned long offset;	/* Offset into pagecache page */
+		unsigned long bytes;	/* Bytes to write to page */
+		size_t copied;		/* Bytes copied from user */
+		void *fsdata;
+
+		offset = (pos & (PAGE_SIZE - 1));
+		bytes = min_t(unsigned long, PAGE_SIZE - offset,
+						iov_iter_count(i));
+
+again:
+		/*
+		 * Bring in the user page that we will copy from _first_.
+		 * Otherwise there's a nasty deadlock on copying from the
+		 * same page as we're writing to, without it being marked
+		 * up-to-date.
+		 *
+		 * Not only is this an optimisation, but it is also required
+		 * to check that the address is actually valid, when atomic
+		 * usercopies are used, below.
+		 */
+		if (unlikely(iov_iter_fault_in_readable(i, bytes))) {
+			status = -EFAULT;
+			break;
+		}
+
+		if (fatal_signal_pending(current)) {
+			status = -EINTR;
+			break;
+		}
+
+		status = a_ops->write_begin(file, mapping, pos, bytes, flags,
+						&page, &fsdata);
+		if (unlikely(status < 0))
+			break;
+
+		if (mapping_writably_mapped(mapping))
+			flush_dcache_page(page);
+
+		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
+		flush_dcache_page(page);
+
+		status = a_ops->write_end(file, mapping, pos, bytes, copied,
+						page, fsdata);
+		if (unlikely(status < 0))
+			break;
+		copied = status;
+
+		cond_resched();
+
+		iov_iter_advance(i, copied);
+		if (unlikely(copied == 0)) {
+			/*
+			 * If we were unable to copy any data at all, we must
+			 * fall back to a single segment length write.
+			 *
+			 * If we didn't fallback here, we could livelock
+			 * because not all segments in the iov can be copied at
+			 * once without a pagefault.
+			 */
+			bytes = min_t(unsigned long, PAGE_SIZE - offset,
+						iov_iter_single_seg_count(i));
+			goto again;
+		}
+		pos += copied;
+		written += copied;
+
+		balance_dirty_pages_ratelimited(mapping);
+	} while (iov_iter_count(i));
+
+	return written ? written : status;
+}
+EXPORT_SYMBOL(generic_perform_write);
+
+/**
+ * __generic_file_write_iter - write data to a file
+ * @iocb:	IO state structure (file, offset, etc.)
+ * @from:	iov_iter with data to write
+ *
+ * This function does all the work needed for actually writing data to a
+ * file. It does all basic checks, removes SUID from the file, updates
+ * modification times and calls proper subroutines depending on whether we
+ * do direct IO or a standard buffered write.
+ *
+ * It expects i_mutex to be grabbed unless we work on a block device or similar
+ * object which does not need locking at all.
+ *
+ * This function does *not* take care of syncing data in case of O_SYNC write.
+ * A caller has to handle it. This is mainly due to the fact that we want to
+ * avoid syncing under i_mutex.
+ */
+ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file *file = iocb->ki_filp;
+	struct address_space * mapping = file->f_mapping;
+	struct inode 	*inode = mapping->host;
+	ssize_t		written = 0;
+	ssize_t		err;
+	ssize_t		status;
+
+	/* We can write back this queue in page reclaim */
+	current->backing_dev_info = inode_to_bdi(inode);
+	err = file_remove_privs(file);
+	if (err)
+		goto out;
+
+	err = file_update_time(file);
+	if (err)
+		goto out;
+
+	if (iocb->ki_flags & IOCB_DIRECT) {
+		loff_t pos, endbyte;
+
+		written = generic_file_direct_write(iocb, from);
+		/*
+		 * If the write stopped short of completing, fall back to
+		 * buffered writes.  Some filesystems do this for writes to
+		 * holes, for example.  For DAX files, a buffered write will
+		 * not succeed (even if it did, DAX does not handle dirty
+		 * page-cache pages correctly).
+		 */
+		if (written < 0 || !iov_iter_count(from) || IS_DAX(inode))
+			goto out;
+
+		status = generic_perform_write(file, from, pos = iocb->ki_pos);
+		/*
+		 * If generic_perform_write() returned a synchronous error
+		 * then we want to return the number of bytes which were
+		 * direct-written, or the error code if that was zero.  Note
+		 * that this differs from normal direct-io semantics, which
+		 * will return -EFOO even if some bytes were written.
+		 */
+		if (unlikely(status < 0)) {
+			err = status;
+			goto out;
+		}
+		/*
+		 * We need to ensure that the page cache pages are written to
+		 * disk and invalidated to preserve the expected O_DIRECT
+		 * semantics.
+		 */
+		endbyte = pos + status - 1;
+		err = filemap_write_and_wait_range(mapping, pos, endbyte);
+		if (err == 0) {
+			iocb->ki_pos = endbyte + 1;
+			written += status;
+			invalidate_mapping_pages(mapping,
+						 pos >> PAGE_SHIFT,
+						 endbyte >> PAGE_SHIFT);
+		} else {
+			/*
+			 * We don't know how much we wrote, so just return
+			 * the number of bytes which were direct-written
+			 */
+		}
+	} else {
+		written = generic_perform_write(file, from, iocb->ki_pos);
+		if (likely(written > 0))
+			iocb->ki_pos += written;
+	}
+out:
+	current->backing_dev_info = NULL;
+	return written ? written : err;
+}
+EXPORT_SYMBOL(__generic_file_write_iter);
+
+/**
+ * generic_file_write_iter - write data to a file
+ * @iocb:	IO state structure
+ * @from:	iov_iter with data to write
+ *
+ * This is a wrapper around __generic_file_write_iter() to be used by most
+ * filesystems. It takes care of syncing the file in case of O_SYNC file
+ * and acquires i_mutex as needed.
+ */
+ssize_t generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file *file = iocb->ki_filp;
+	struct inode *inode = file->f_mapping->host;
+	ssize_t ret;
+
+	inode_lock(inode);
+	ret = generic_write_checks(iocb, from);
+	if (ret > 0)
+		ret = __generic_file_write_iter(iocb, from);
+	inode_unlock(inode);
+
+	if (ret > 0)
+		ret = generic_write_sync(iocb, ret);
+	return ret;
+}
+EXPORT_SYMBOL(generic_file_write_iter);
+
+/**
+ * try_to_release_page() - release old fs-specific metadata on a page
+ *
+ * @page: the page which the kernel is trying to free
+ * @gfp_mask: memory allocation flags (and I/O mode)
+ *
+ * The address_space is to try to release any data against the page
+ * (presumably at page->private).  If the release was successful, return '1'.
+ * Otherwise return zero.
+ *
+ * This may also be called if PG_fscache is set on a page, indicating that the
+ * page is known to the local caching routines.
+ *
+ * The @gfp_mask argument specifies whether I/O may be performed to release
+ * this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS).
+ *
+ */
+int try_to_release_page(struct page *page, gfp_t gfp_mask)
+{
+	struct address_space * const mapping = page->mapping;
+
+	BUG_ON(!PageLocked(page));
+	if (PageWriteback(page))
+		return 0;
+
+	if (mapping && mapping->a_ops->releasepage)
+		return mapping->a_ops->releasepage(page, gfp_mask);
+	return try_to_free_buffers(page);
+}
+
+EXPORT_SYMBOL(try_to_release_page);
diff -uNr linux-4.12.9/mm/internal.h linux-4.12.9-pf/mm/internal.h
--- linux-4.12.9/mm/internal.h	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/internal.h	2019-01-02 21:24:57.460609746 +0900
@@ -53,9 +53,10 @@
 			     unsigned long addr, unsigned long end,
 			     struct zap_details *details);
 
+//PARALFETCH
 extern int __do_page_cache_readahead(struct address_space *mapping,
 		struct file *filp, pgoff_t offset, unsigned long nr_to_read,
-		unsigned long lookahead_size);
+		unsigned long lookahead_size, int prefetch);
 
 /*
  * Submit IO for the read-ahead request in file_ra_state.
@@ -63,8 +64,9 @@
 static inline unsigned long ra_submit(struct file_ra_state *ra,
 		struct address_space *mapping, struct file *filp)
 {
+	//PARALFETCH
 	return __do_page_cache_readahead(mapping, filp,
-					ra->start, ra->size, ra->async_size);
+				ra->start, ra->size, ra->async_size, 0);
 }
 
 /*
diff -uNr linux-4.12.9/mm/madvise.c linux-4.12.9-pf/mm/madvise.c
--- linux-4.12.9/mm/madvise.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/madvise.c	2019-01-02 21:24:57.452609701 +0900
@@ -293,7 +293,8 @@
 		end = vma->vm_end;
 	end = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 
-	force_page_cache_readahead(file->f_mapping, file, start, end - start);
+	//PARALFETCH
+	force_page_cache_readahead(file->f_mapping, file, start, end - start, 0);
 	return 0;
 }
 
diff -uNr linux-4.12.9/mm/memory.c linux-4.12.9-pf/mm/memory.c
--- linux-4.12.9/mm/memory.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/memory.c	2019-01-02 21:24:57.457609729 +0900
@@ -3368,11 +3368,17 @@
 	 * something).
 	 */
 	if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
+		//PARALFETCH
+		//printk("1:DFA\n");
+
 		ret = do_fault_around(vmf);
 		if (ret)
 			return ret;
 	}
 
+	//PARALFETCH
+	//printk("2:DF\n");
+
 	ret = __do_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
diff -uNr linux-4.12.9/mm/readahead.c linux-4.12.9-pf/mm/readahead.c
--- linux-4.12.9/mm/readahead.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/mm/readahead.c	2019-01-02 21:24:57.456609723 +0900
@@ -147,9 +147,10 @@
  *
  * Returns the number of pages requested, or the maximum amount of I/O allowed.
  */
+//PARALFETCH
 int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 			pgoff_t offset, unsigned long nr_to_read,
-			unsigned long lookahead_size)
+			unsigned long lookahead_size, int prefetch)
 {
 	struct inode *inode = mapping->host;
 	struct page *page;
@@ -165,6 +166,13 @@
 
 	end_index = ((isize - 1) >> PAGE_SHIFT);
 
+	//PARALFETCH
+	/*
+	if (prefetch == 1) {
+		printk("P-Page\n");
+	}
+	*/
+
 	/*
 	 * Preallocate as many pages as we will need.
 	 */
@@ -188,6 +196,12 @@
 		if (page_idx == nr_to_read - lookahead_size)
 			SetPageReadahead(page);
 		ret++;
+
+		//PARALFETCH
+		if ((prefetch == 1) && (page_offset == offset)) {
+			///printk("P-Page\n");
+			SetPagePrefetch(page);
+		}
 	}
 
 	/*
@@ -206,8 +220,9 @@
  * Chunk the readahead into 2 megabyte units, so that we don't pin too much
  * memory at once.
  */
+//PARALFETCH
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
-			       pgoff_t offset, unsigned long nr_to_read)
+			       pgoff_t offset, unsigned long nr_to_read, int prefetch)
 {
 	struct backing_dev_info *bdi = inode_to_bdi(mapping->host);
 	struct file_ra_state *ra = &filp->f_ra;
@@ -229,8 +244,9 @@
 
 		if (this_chunk > nr_to_read)
 			this_chunk = nr_to_read;
+		//PARALFETCH
 		err = __do_page_cache_readahead(mapping, filp,
-						offset, this_chunk, 0);
+					offset, this_chunk, 0, prefetch);
 		if (err < 0)
 			return err;
 
@@ -457,7 +473,8 @@
 	 * standalone, small random read
 	 * Read as is, and do not pollute the readahead state.
 	 */
-	return __do_page_cache_readahead(mapping, filp, offset, req_size, 0);
+	//PARALFETCH
+	return __do_page_cache_readahead(mapping, filp, offset, req_size, 0, 0);
 
 initial_readahead:
 	ra->start = offset;
@@ -502,7 +519,8 @@
 
 	/* be dumb */
 	if (filp && (filp->f_mode & FMODE_RANDOM)) {
-		force_page_cache_readahead(mapping, filp, offset, req_size);
+		//PARALFETCH
+		force_page_cache_readahead(mapping, filp, offset, req_size, 0);
 		return;
 	}
 
@@ -570,7 +588,8 @@
 	if (dax_mapping(mapping))
 		return 0;
 
-	return force_page_cache_readahead(mapping, filp, index, nr);
+	//PARALFETCH
+	return force_page_cache_readahead(mapping, filp, index, nr, 0);
 }
 
 SYSCALL_DEFINE3(readahead, int, fd, loff_t, offset, size_t, count)
diff -uNr linux-4.12.9/tools/objtool/arch/x86/insn/inat-tables.c linux-4.12.9-pf/tools/objtool/arch/x86/insn/inat-tables.c
--- linux-4.12.9/tools/objtool/arch/x86/insn/inat-tables.c	1970-01-01 09:00:00.000000000 +0900
+++ linux-4.12.9-pf/tools/objtool/arch/x86/insn/inat-tables.c	2019-01-02 21:25:05.736657084 +0900
@@ -0,0 +1,1390 @@
+/* x86 opcode map generated from x86-opcode-map.txt */
+/* Do not change this code. */
+
+/* Table: one byte opcode */
+const insn_attr_t inat_primary_table[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM,
+	[0x01] = INAT_MODRM,
+	[0x02] = INAT_MODRM,
+	[0x03] = INAT_MODRM,
+	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x05] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x08] = INAT_MODRM,
+	[0x09] = INAT_MODRM,
+	[0x0a] = INAT_MODRM,
+	[0x0b] = INAT_MODRM,
+	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x0d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x0f] = INAT_MAKE_ESCAPE(1),
+	[0x10] = INAT_MODRM,
+	[0x11] = INAT_MODRM,
+	[0x12] = INAT_MODRM,
+	[0x13] = INAT_MODRM,
+	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x15] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x18] = INAT_MODRM,
+	[0x19] = INAT_MODRM,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x1c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x1d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x20] = INAT_MODRM,
+	[0x21] = INAT_MODRM,
+	[0x22] = INAT_MODRM,
+	[0x23] = INAT_MODRM,
+	[0x24] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x25] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x26] = INAT_MAKE_PREFIX(INAT_PFX_ES),
+	[0x28] = INAT_MODRM,
+	[0x29] = INAT_MODRM,
+	[0x2a] = INAT_MODRM,
+	[0x2b] = INAT_MODRM,
+	[0x2c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x2d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x2e] = INAT_MAKE_PREFIX(INAT_PFX_CS),
+	[0x30] = INAT_MODRM,
+	[0x31] = INAT_MODRM,
+	[0x32] = INAT_MODRM,
+	[0x33] = INAT_MODRM,
+	[0x34] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x35] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x36] = INAT_MAKE_PREFIX(INAT_PFX_SS),
+	[0x38] = INAT_MODRM,
+	[0x39] = INAT_MODRM,
+	[0x3a] = INAT_MODRM,
+	[0x3b] = INAT_MODRM,
+	[0x3c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x3d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0x3e] = INAT_MAKE_PREFIX(INAT_PFX_DS),
+	[0x40] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x41] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x42] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x43] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x44] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x45] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x46] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x47] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x48] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x49] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4a] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4b] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4c] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4d] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4e] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x4f] = INAT_MAKE_PREFIX(INAT_PFX_REX),
+	[0x50] = INAT_FORCE64,
+	[0x51] = INAT_FORCE64,
+	[0x52] = INAT_FORCE64,
+	[0x53] = INAT_FORCE64,
+	[0x54] = INAT_FORCE64,
+	[0x55] = INAT_FORCE64,
+	[0x56] = INAT_FORCE64,
+	[0x57] = INAT_FORCE64,
+	[0x58] = INAT_FORCE64,
+	[0x59] = INAT_FORCE64,
+	[0x5a] = INAT_FORCE64,
+	[0x5b] = INAT_FORCE64,
+	[0x5c] = INAT_FORCE64,
+	[0x5d] = INAT_FORCE64,
+	[0x5e] = INAT_FORCE64,
+	[0x5f] = INAT_FORCE64,
+	[0x62] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_EVEX),
+	[0x63] = INAT_MODRM | INAT_MODRM,
+	[0x64] = INAT_MAKE_PREFIX(INAT_PFX_FS),
+	[0x65] = INAT_MAKE_PREFIX(INAT_PFX_GS),
+	[0x66] = INAT_MAKE_PREFIX(INAT_PFX_OPNDSZ),
+	[0x67] = INAT_MAKE_PREFIX(INAT_PFX_ADDRSZ),
+	[0x68] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x69] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x6a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0x6b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x71] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x72] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x73] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x74] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x75] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x76] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x77] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x78] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x79] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7a] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7b] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7d] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7e] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x7f] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0x80] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x82] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x83] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
+	[0x84] = INAT_MODRM,
+	[0x85] = INAT_MODRM,
+	[0x86] = INAT_MODRM,
+	[0x87] = INAT_MODRM,
+	[0x88] = INAT_MODRM,
+	[0x89] = INAT_MODRM,
+	[0x8a] = INAT_MODRM,
+	[0x8b] = INAT_MODRM,
+	[0x8c] = INAT_MODRM,
+	[0x8d] = INAT_MODRM,
+	[0x8e] = INAT_MODRM,
+	[0x8f] = INAT_MAKE_GROUP(2) | INAT_MODRM | INAT_FORCE64,
+	[0x9a] = INAT_MAKE_IMM(INAT_IMM_PTR),
+	[0x9c] = INAT_FORCE64,
+	[0x9d] = INAT_FORCE64,
+	[0xa0] = INAT_MOFFSET,
+	[0xa1] = INAT_MOFFSET,
+	[0xa2] = INAT_MOFFSET,
+	[0xa3] = INAT_MOFFSET,
+	[0xa8] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xa9] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+	[0xb0] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb1] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb2] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb3] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb6] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xb8] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xb9] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xba] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbb] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbc] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbd] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbe] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xbf] = INAT_MAKE_IMM(INAT_IMM_VWORD),
+	[0xc0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xc1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_FORCE64,
+	[0xc4] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX3),
+	[0xc5] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX2),
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(4),
+	[0xc7] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(5),
+	[0xc8] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_SCNDIMM,
+	[0xc9] = INAT_FORCE64,
+	[0xca] = INAT_MAKE_IMM(INAT_IMM_WORD),
+	[0xcd] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd0] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd1] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd2] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd3] = INAT_MODRM | INAT_MAKE_GROUP(3),
+	[0xd4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xd8] = INAT_MODRM,
+	[0xd9] = INAT_MODRM,
+	[0xda] = INAT_MODRM,
+	[0xdb] = INAT_MODRM,
+	[0xdc] = INAT_MODRM,
+	[0xdd] = INAT_MODRM,
+	[0xde] = INAT_MODRM,
+	[0xdf] = INAT_MODRM,
+	[0xe0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xe4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe6] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+	[0xe8] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0xe9] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0xea] = INAT_MAKE_IMM(INAT_IMM_PTR),
+	[0xeb] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
+	[0xf0] = INAT_MAKE_PREFIX(INAT_PFX_LOCK),
+	[0xf2] = INAT_MAKE_PREFIX(INAT_PFX_REPNE) | INAT_MAKE_PREFIX(INAT_PFX_REPNE),
+	[0xf3] = INAT_MAKE_PREFIX(INAT_PFX_REPE) | INAT_MAKE_PREFIX(INAT_PFX_REPE),
+	[0xf6] = INAT_MODRM | INAT_MAKE_GROUP(6),
+	[0xf7] = INAT_MODRM | INAT_MAKE_GROUP(7),
+	[0xfe] = INAT_MAKE_GROUP(8),
+	[0xff] = INAT_MAKE_GROUP(9),
+};
+
+/* Table: 2-byte opcode (0x0f) */
+const insn_attr_t inat_escape_table_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MAKE_GROUP(10),
+	[0x01] = INAT_MAKE_GROUP(11),
+	[0x02] = INAT_MODRM,
+	[0x03] = INAT_MODRM,
+	[0x0d] = INAT_MAKE_GROUP(12),
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x17] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x18] = INAT_MAKE_GROUP(13),
+	[0x1a] = INAT_MODRM | INAT_VARIANT,
+	[0x1b] = INAT_MODRM | INAT_VARIANT,
+	[0x1f] = INAT_MODRM,
+	[0x20] = INAT_MODRM,
+	[0x21] = INAT_MODRM,
+	[0x22] = INAT_MODRM,
+	[0x23] = INAT_MODRM,
+	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2a] = INAT_MODRM | INAT_VARIANT,
+	[0x2b] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2c] = INAT_MODRM | INAT_VARIANT,
+	[0x2d] = INAT_MODRM | INAT_VARIANT,
+	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x38] = INAT_MAKE_ESCAPE(2),
+	[0x3a] = INAT_MAKE_ESCAPE(3),
+	[0x40] = INAT_MODRM,
+	[0x41] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x42] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x43] = INAT_MODRM,
+	[0x44] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x45] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x46] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x47] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x48] = INAT_MODRM,
+	[0x49] = INAT_MODRM,
+	[0x4a] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x4b] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x4c] = INAT_MODRM,
+	[0x4d] = INAT_MODRM,
+	[0x4e] = INAT_MODRM,
+	[0x4f] = INAT_MODRM,
+	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x52] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x53] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x54] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x55] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x56] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x57] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5c] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5d] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x5f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x60] = INAT_MODRM | INAT_VARIANT,
+	[0x61] = INAT_MODRM | INAT_VARIANT,
+	[0x62] = INAT_MODRM | INAT_VARIANT,
+	[0x63] = INAT_MODRM | INAT_VARIANT,
+	[0x64] = INAT_MODRM | INAT_VARIANT,
+	[0x65] = INAT_MODRM | INAT_VARIANT,
+	[0x66] = INAT_MODRM | INAT_VARIANT,
+	[0x67] = INAT_MODRM | INAT_VARIANT,
+	[0x68] = INAT_MODRM | INAT_VARIANT,
+	[0x69] = INAT_MODRM | INAT_VARIANT,
+	[0x6a] = INAT_MODRM | INAT_VARIANT,
+	[0x6b] = INAT_MODRM | INAT_VARIANT,
+	[0x6c] = INAT_VARIANT,
+	[0x6d] = INAT_VARIANT,
+	[0x6e] = INAT_MODRM | INAT_VARIANT,
+	[0x6f] = INAT_MODRM | INAT_VARIANT,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x71] = INAT_MAKE_GROUP(14),
+	[0x72] = INAT_MAKE_GROUP(15),
+	[0x73] = INAT_MAKE_GROUP(16),
+	[0x74] = INAT_MODRM | INAT_VARIANT,
+	[0x75] = INAT_MODRM | INAT_VARIANT,
+	[0x76] = INAT_MODRM | INAT_VARIANT,
+	[0x77] = INAT_VEXOK | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x79] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x7a] = INAT_VARIANT,
+	[0x7b] = INAT_VARIANT,
+	[0x7c] = INAT_VARIANT,
+	[0x7d] = INAT_VARIANT,
+	[0x7e] = INAT_MODRM | INAT_VARIANT,
+	[0x7f] = INAT_MODRM | INAT_VARIANT,
+	[0x80] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x82] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x83] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x84] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x85] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x86] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x87] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x88] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x89] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8a] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8b] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8c] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8d] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8e] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x8f] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64,
+	[0x90] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x91] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x92] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x93] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x94] = INAT_MODRM,
+	[0x95] = INAT_MODRM,
+	[0x96] = INAT_MODRM,
+	[0x97] = INAT_MODRM,
+	[0x98] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x99] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x9a] = INAT_MODRM,
+	[0x9b] = INAT_MODRM,
+	[0x9c] = INAT_MODRM,
+	[0x9d] = INAT_MODRM,
+	[0x9e] = INAT_MODRM,
+	[0x9f] = INAT_MODRM,
+	[0xa0] = INAT_FORCE64,
+	[0xa1] = INAT_FORCE64,
+	[0xa3] = INAT_MODRM,
+	[0xa4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xa5] = INAT_MODRM,
+	[0xa6] = INAT_MAKE_GROUP(17),
+	[0xa7] = INAT_MAKE_GROUP(18),
+	[0xa8] = INAT_FORCE64,
+	[0xa9] = INAT_FORCE64,
+	[0xab] = INAT_MODRM,
+	[0xac] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xad] = INAT_MODRM,
+	[0xae] = INAT_MAKE_GROUP(19),
+	[0xaf] = INAT_MODRM,
+	[0xb0] = INAT_MODRM,
+	[0xb1] = INAT_MODRM,
+	[0xb2] = INAT_MODRM,
+	[0xb3] = INAT_MODRM,
+	[0xb4] = INAT_MODRM,
+	[0xb5] = INAT_MODRM,
+	[0xb6] = INAT_MODRM,
+	[0xb7] = INAT_MODRM,
+	[0xb8] = INAT_VARIANT,
+	[0xb9] = INAT_MAKE_GROUP(20),
+	[0xba] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(21),
+	[0xbb] = INAT_MODRM,
+	[0xbc] = INAT_MODRM | INAT_VARIANT,
+	[0xbd] = INAT_MODRM | INAT_VARIANT,
+	[0xbe] = INAT_MODRM,
+	[0xbf] = INAT_MODRM,
+	[0xc0] = INAT_MODRM,
+	[0xc1] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0xc3] = INAT_MODRM,
+	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0xc7] = INAT_MAKE_GROUP(22),
+	[0xd0] = INAT_VARIANT,
+	[0xd1] = INAT_MODRM | INAT_VARIANT,
+	[0xd2] = INAT_MODRM | INAT_VARIANT,
+	[0xd3] = INAT_MODRM | INAT_VARIANT,
+	[0xd4] = INAT_MODRM | INAT_VARIANT,
+	[0xd5] = INAT_MODRM | INAT_VARIANT,
+	[0xd6] = INAT_VARIANT,
+	[0xd7] = INAT_MODRM | INAT_VARIANT,
+	[0xd8] = INAT_MODRM | INAT_VARIANT,
+	[0xd9] = INAT_MODRM | INAT_VARIANT,
+	[0xda] = INAT_MODRM | INAT_VARIANT,
+	[0xdb] = INAT_MODRM | INAT_VARIANT,
+	[0xdc] = INAT_MODRM | INAT_VARIANT,
+	[0xdd] = INAT_MODRM | INAT_VARIANT,
+	[0xde] = INAT_MODRM | INAT_VARIANT,
+	[0xdf] = INAT_MODRM | INAT_VARIANT,
+	[0xe0] = INAT_MODRM | INAT_VARIANT,
+	[0xe1] = INAT_MODRM | INAT_VARIANT,
+	[0xe2] = INAT_MODRM | INAT_VARIANT,
+	[0xe3] = INAT_MODRM | INAT_VARIANT,
+	[0xe4] = INAT_MODRM | INAT_VARIANT,
+	[0xe5] = INAT_MODRM | INAT_VARIANT,
+	[0xe6] = INAT_VARIANT,
+	[0xe7] = INAT_MODRM | INAT_VARIANT,
+	[0xe8] = INAT_MODRM | INAT_VARIANT,
+	[0xe9] = INAT_MODRM | INAT_VARIANT,
+	[0xea] = INAT_MODRM | INAT_VARIANT,
+	[0xeb] = INAT_MODRM | INAT_VARIANT,
+	[0xec] = INAT_MODRM | INAT_VARIANT,
+	[0xed] = INAT_MODRM | INAT_VARIANT,
+	[0xee] = INAT_MODRM | INAT_VARIANT,
+	[0xef] = INAT_MODRM | INAT_VARIANT,
+	[0xf0] = INAT_VARIANT,
+	[0xf1] = INAT_MODRM | INAT_VARIANT,
+	[0xf2] = INAT_MODRM | INAT_VARIANT,
+	[0xf3] = INAT_MODRM | INAT_VARIANT,
+	[0xf4] = INAT_MODRM | INAT_VARIANT,
+	[0xf5] = INAT_MODRM | INAT_VARIANT,
+	[0xf6] = INAT_MODRM | INAT_VARIANT,
+	[0xf7] = INAT_MODRM | INAT_VARIANT,
+	[0xf8] = INAT_MODRM | INAT_VARIANT,
+	[0xf9] = INAT_MODRM | INAT_VARIANT,
+	[0xfa] = INAT_MODRM | INAT_VARIANT,
+	[0xfb] = INAT_MODRM | INAT_VARIANT,
+	[0xfc] = INAT_MODRM | INAT_VARIANT,
+	[0xfd] = INAT_MODRM | INAT_VARIANT,
+	[0xfe] = INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_1_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x13] = INAT_MODRM | INAT_VEXOK,
+	[0x14] = INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x28] = INAT_MODRM | INAT_VEXOK,
+	[0x29] = INAT_MODRM | INAT_VEXOK,
+	[0x2a] = INAT_MODRM,
+	[0x2b] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM,
+	[0x2d] = INAT_MODRM,
+	[0x2e] = INAT_MODRM | INAT_VEXOK,
+	[0x2f] = INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MODRM | INAT_VEXOK,
+	[0x44] = INAT_MODRM | INAT_VEXOK,
+	[0x45] = INAT_MODRM | INAT_VEXOK,
+	[0x46] = INAT_MODRM | INAT_VEXOK,
+	[0x47] = INAT_MODRM | INAT_VEXOK,
+	[0x4a] = INAT_MODRM | INAT_VEXOK,
+	[0x4b] = INAT_MODRM | INAT_VEXOK,
+	[0x50] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x54] = INAT_MODRM | INAT_VEXOK,
+	[0x55] = INAT_MODRM | INAT_VEXOK,
+	[0x56] = INAT_MODRM | INAT_VEXOK,
+	[0x57] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x60] = INAT_MODRM | INAT_VEXOK,
+	[0x61] = INAT_MODRM | INAT_VEXOK,
+	[0x62] = INAT_MODRM | INAT_VEXOK,
+	[0x63] = INAT_MODRM | INAT_VEXOK,
+	[0x64] = INAT_MODRM | INAT_VEXOK,
+	[0x65] = INAT_MODRM | INAT_VEXOK,
+	[0x66] = INAT_MODRM | INAT_VEXOK,
+	[0x67] = INAT_MODRM | INAT_VEXOK,
+	[0x68] = INAT_MODRM | INAT_VEXOK,
+	[0x69] = INAT_MODRM | INAT_VEXOK,
+	[0x6a] = INAT_MODRM | INAT_VEXOK,
+	[0x6b] = INAT_MODRM | INAT_VEXOK,
+	[0x6c] = INAT_MODRM | INAT_VEXOK,
+	[0x6d] = INAT_MODRM | INAT_VEXOK,
+	[0x6e] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x74] = INAT_MODRM | INAT_VEXOK,
+	[0x75] = INAT_MODRM | INAT_VEXOK,
+	[0x76] = INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK,
+	[0x7d] = INAT_MODRM | INAT_VEXOK,
+	[0x7e] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x90] = INAT_MODRM | INAT_VEXOK,
+	[0x91] = INAT_MODRM | INAT_VEXOK,
+	[0x92] = INAT_MODRM | INAT_VEXOK,
+	[0x93] = INAT_MODRM | INAT_VEXOK,
+	[0x98] = INAT_MODRM | INAT_VEXOK,
+	[0x99] = INAT_MODRM | INAT_VEXOK,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd0] = INAT_MODRM | INAT_VEXOK,
+	[0xd1] = INAT_MODRM | INAT_VEXOK,
+	[0xd2] = INAT_MODRM | INAT_VEXOK,
+	[0xd3] = INAT_MODRM | INAT_VEXOK,
+	[0xd4] = INAT_MODRM | INAT_VEXOK,
+	[0xd5] = INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM | INAT_VEXOK,
+	[0xd7] = INAT_MODRM | INAT_VEXOK,
+	[0xd8] = INAT_MODRM | INAT_VEXOK,
+	[0xd9] = INAT_MODRM | INAT_VEXOK,
+	[0xda] = INAT_MODRM | INAT_VEXOK,
+	[0xdb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xdc] = INAT_MODRM | INAT_VEXOK,
+	[0xdd] = INAT_MODRM | INAT_VEXOK,
+	[0xde] = INAT_MODRM | INAT_VEXOK,
+	[0xdf] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xe0] = INAT_MODRM | INAT_VEXOK,
+	[0xe1] = INAT_MODRM | INAT_VEXOK,
+	[0xe2] = INAT_MODRM | INAT_VEXOK,
+	[0xe3] = INAT_MODRM | INAT_VEXOK,
+	[0xe4] = INAT_MODRM | INAT_VEXOK,
+	[0xe5] = INAT_MODRM | INAT_VEXOK,
+	[0xe6] = INAT_MODRM | INAT_VEXOK,
+	[0xe7] = INAT_MODRM | INAT_VEXOK,
+	[0xe8] = INAT_MODRM | INAT_VEXOK,
+	[0xe9] = INAT_MODRM | INAT_VEXOK,
+	[0xea] = INAT_MODRM | INAT_VEXOK,
+	[0xeb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xec] = INAT_MODRM | INAT_VEXOK,
+	[0xed] = INAT_MODRM | INAT_VEXOK,
+	[0xee] = INAT_MODRM | INAT_VEXOK,
+	[0xef] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xf1] = INAT_MODRM | INAT_VEXOK,
+	[0xf2] = INAT_MODRM | INAT_VEXOK,
+	[0xf3] = INAT_MODRM | INAT_VEXOK,
+	[0xf4] = INAT_MODRM | INAT_VEXOK,
+	[0xf5] = INAT_MODRM | INAT_VEXOK,
+	[0xf6] = INAT_MODRM | INAT_VEXOK,
+	[0xf7] = INAT_MODRM | INAT_VEXOK,
+	[0xf8] = INAT_MODRM | INAT_VEXOK,
+	[0xf9] = INAT_MODRM | INAT_VEXOK,
+	[0xfa] = INAT_MODRM | INAT_VEXOK,
+	[0xfb] = INAT_MODRM | INAT_VEXOK,
+	[0xfc] = INAT_MODRM | INAT_VEXOK,
+	[0xfd] = INAT_MODRM | INAT_VEXOK,
+	[0xfe] = INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_1_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x52] = INAT_MODRM | INAT_VEXOK,
+	[0x53] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7e] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0xb8] = INAT_MODRM,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM,
+	[0xe6] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_1_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK,
+	[0x12] = INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM,
+	[0x1b] = INAT_MODRM,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK,
+	[0x51] = INAT_MODRM | INAT_VEXOK,
+	[0x58] = INAT_MODRM | INAT_VEXOK,
+	[0x59] = INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK,
+	[0x5c] = INAT_MODRM | INAT_VEXOK,
+	[0x5d] = INAT_MODRM | INAT_VEXOK,
+	[0x5e] = INAT_MODRM | INAT_VEXOK,
+	[0x5f] = INAT_MODRM | INAT_VEXOK,
+	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK,
+	[0x7d] = INAT_MODRM | INAT_VEXOK,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x92] = INAT_MODRM | INAT_VEXOK,
+	[0x93] = INAT_MODRM | INAT_VEXOK,
+	[0xbc] = INAT_MODRM,
+	[0xbd] = INAT_MODRM,
+	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0xd0] = INAT_MODRM | INAT_VEXOK,
+	[0xd6] = INAT_MODRM,
+	[0xe6] = INAT_MODRM | INAT_VEXOK,
+	[0xf0] = INAT_MODRM | INAT_VEXOK,
+};
+
+/* Table: 3-byte opcode 1 (0x0f 0x38) */
+const insn_attr_t inat_escape_table_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM | INAT_VARIANT,
+	[0x01] = INAT_MODRM | INAT_VARIANT,
+	[0x02] = INAT_MODRM | INAT_VARIANT,
+	[0x03] = INAT_MODRM | INAT_VARIANT,
+	[0x04] = INAT_MODRM | INAT_VARIANT,
+	[0x05] = INAT_MODRM | INAT_VARIANT,
+	[0x06] = INAT_MODRM | INAT_VARIANT,
+	[0x07] = INAT_MODRM | INAT_VARIANT,
+	[0x08] = INAT_MODRM | INAT_VARIANT,
+	[0x09] = INAT_MODRM | INAT_VARIANT,
+	[0x0a] = INAT_MODRM | INAT_VARIANT,
+	[0x0b] = INAT_MODRM | INAT_VARIANT,
+	[0x0c] = INAT_VARIANT,
+	[0x0d] = INAT_VARIANT,
+	[0x0e] = INAT_VARIANT,
+	[0x0f] = INAT_VARIANT,
+	[0x10] = INAT_VARIANT,
+	[0x11] = INAT_VARIANT,
+	[0x12] = INAT_VARIANT,
+	[0x13] = INAT_VARIANT,
+	[0x14] = INAT_VARIANT,
+	[0x15] = INAT_VARIANT,
+	[0x16] = INAT_VARIANT,
+	[0x17] = INAT_VARIANT,
+	[0x18] = INAT_VARIANT,
+	[0x19] = INAT_VARIANT,
+	[0x1a] = INAT_VARIANT,
+	[0x1b] = INAT_VARIANT,
+	[0x1c] = INAT_MODRM | INAT_VARIANT,
+	[0x1d] = INAT_MODRM | INAT_VARIANT,
+	[0x1e] = INAT_MODRM | INAT_VARIANT,
+	[0x1f] = INAT_VARIANT,
+	[0x20] = INAT_VARIANT,
+	[0x21] = INAT_VARIANT,
+	[0x22] = INAT_VARIANT,
+	[0x23] = INAT_VARIANT,
+	[0x24] = INAT_VARIANT,
+	[0x25] = INAT_VARIANT,
+	[0x26] = INAT_VARIANT,
+	[0x27] = INAT_VARIANT,
+	[0x28] = INAT_VARIANT,
+	[0x29] = INAT_VARIANT,
+	[0x2a] = INAT_VARIANT,
+	[0x2b] = INAT_VARIANT,
+	[0x2c] = INAT_VARIANT,
+	[0x2d] = INAT_VARIANT,
+	[0x2e] = INAT_VARIANT,
+	[0x2f] = INAT_VARIANT,
+	[0x30] = INAT_VARIANT,
+	[0x31] = INAT_VARIANT,
+	[0x32] = INAT_VARIANT,
+	[0x33] = INAT_VARIANT,
+	[0x34] = INAT_VARIANT,
+	[0x35] = INAT_VARIANT,
+	[0x36] = INAT_VARIANT,
+	[0x37] = INAT_VARIANT,
+	[0x38] = INAT_VARIANT,
+	[0x39] = INAT_VARIANT,
+	[0x3a] = INAT_VARIANT,
+	[0x3b] = INAT_VARIANT,
+	[0x3c] = INAT_VARIANT,
+	[0x3d] = INAT_VARIANT,
+	[0x3e] = INAT_VARIANT,
+	[0x3f] = INAT_VARIANT,
+	[0x40] = INAT_VARIANT,
+	[0x41] = INAT_VARIANT,
+	[0x42] = INAT_VARIANT,
+	[0x43] = INAT_VARIANT,
+	[0x44] = INAT_VARIANT,
+	[0x45] = INAT_VARIANT,
+	[0x46] = INAT_VARIANT,
+	[0x47] = INAT_VARIANT,
+	[0x4c] = INAT_VARIANT,
+	[0x4d] = INAT_VARIANT,
+	[0x4e] = INAT_VARIANT,
+	[0x4f] = INAT_VARIANT,
+	[0x58] = INAT_VARIANT,
+	[0x59] = INAT_VARIANT,
+	[0x5a] = INAT_VARIANT,
+	[0x5b] = INAT_VARIANT,
+	[0x64] = INAT_VARIANT,
+	[0x65] = INAT_VARIANT,
+	[0x66] = INAT_VARIANT,
+	[0x75] = INAT_VARIANT,
+	[0x76] = INAT_VARIANT,
+	[0x77] = INAT_VARIANT,
+	[0x78] = INAT_VARIANT,
+	[0x79] = INAT_VARIANT,
+	[0x7a] = INAT_VARIANT,
+	[0x7b] = INAT_VARIANT,
+	[0x7c] = INAT_VARIANT,
+	[0x7d] = INAT_VARIANT,
+	[0x7e] = INAT_VARIANT,
+	[0x7f] = INAT_VARIANT,
+	[0x80] = INAT_VARIANT,
+	[0x81] = INAT_VARIANT,
+	[0x82] = INAT_VARIANT,
+	[0x83] = INAT_VARIANT,
+	[0x88] = INAT_VARIANT,
+	[0x89] = INAT_VARIANT,
+	[0x8a] = INAT_VARIANT,
+	[0x8b] = INAT_VARIANT,
+	[0x8c] = INAT_VARIANT,
+	[0x8d] = INAT_VARIANT,
+	[0x8e] = INAT_VARIANT,
+	[0x90] = INAT_VARIANT,
+	[0x91] = INAT_VARIANT,
+	[0x92] = INAT_VARIANT,
+	[0x93] = INAT_VARIANT,
+	[0x96] = INAT_VARIANT,
+	[0x97] = INAT_VARIANT,
+	[0x98] = INAT_VARIANT,
+	[0x99] = INAT_VARIANT,
+	[0x9a] = INAT_VARIANT,
+	[0x9b] = INAT_VARIANT,
+	[0x9c] = INAT_VARIANT,
+	[0x9d] = INAT_VARIANT,
+	[0x9e] = INAT_VARIANT,
+	[0x9f] = INAT_VARIANT,
+	[0xa0] = INAT_VARIANT,
+	[0xa1] = INAT_VARIANT,
+	[0xa2] = INAT_VARIANT,
+	[0xa3] = INAT_VARIANT,
+	[0xa6] = INAT_VARIANT,
+	[0xa7] = INAT_VARIANT,
+	[0xa8] = INAT_VARIANT,
+	[0xa9] = INAT_VARIANT,
+	[0xaa] = INAT_VARIANT,
+	[0xab] = INAT_VARIANT,
+	[0xac] = INAT_VARIANT,
+	[0xad] = INAT_VARIANT,
+	[0xae] = INAT_VARIANT,
+	[0xaf] = INAT_VARIANT,
+	[0xb4] = INAT_VARIANT,
+	[0xb5] = INAT_VARIANT,
+	[0xb6] = INAT_VARIANT,
+	[0xb7] = INAT_VARIANT,
+	[0xb8] = INAT_VARIANT,
+	[0xb9] = INAT_VARIANT,
+	[0xba] = INAT_VARIANT,
+	[0xbb] = INAT_VARIANT,
+	[0xbc] = INAT_VARIANT,
+	[0xbd] = INAT_VARIANT,
+	[0xbe] = INAT_VARIANT,
+	[0xbf] = INAT_VARIANT,
+	[0xc4] = INAT_VARIANT,
+	[0xc6] = INAT_MAKE_GROUP(23),
+	[0xc7] = INAT_MAKE_GROUP(24),
+	[0xc8] = INAT_MODRM | INAT_VARIANT,
+	[0xc9] = INAT_MODRM,
+	[0xca] = INAT_MODRM | INAT_VARIANT,
+	[0xcb] = INAT_MODRM | INAT_VARIANT,
+	[0xcc] = INAT_MODRM | INAT_VARIANT,
+	[0xcd] = INAT_MODRM | INAT_VARIANT,
+	[0xdb] = INAT_VARIANT,
+	[0xdc] = INAT_VARIANT,
+	[0xdd] = INAT_VARIANT,
+	[0xde] = INAT_VARIANT,
+	[0xdf] = INAT_VARIANT,
+	[0xf0] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0xf1] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0xf2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf3] = INAT_MAKE_GROUP(25),
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
+	[0xf6] = INAT_VARIANT,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_2_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MODRM | INAT_VEXOK,
+	[0x01] = INAT_MODRM | INAT_VEXOK,
+	[0x02] = INAT_MODRM | INAT_VEXOK,
+	[0x03] = INAT_MODRM | INAT_VEXOK,
+	[0x04] = INAT_MODRM | INAT_VEXOK,
+	[0x05] = INAT_MODRM | INAT_VEXOK,
+	[0x06] = INAT_MODRM | INAT_VEXOK,
+	[0x07] = INAT_MODRM | INAT_VEXOK,
+	[0x08] = INAT_MODRM | INAT_VEXOK,
+	[0x09] = INAT_MODRM | INAT_VEXOK,
+	[0x0a] = INAT_MODRM | INAT_VEXOK,
+	[0x0b] = INAT_MODRM | INAT_VEXOK,
+	[0x0c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x0f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x10] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x14] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MODRM | INAT_VEXOK,
+	[0x18] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x19] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x1b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1c] = INAT_MODRM | INAT_VEXOK,
+	[0x1d] = INAT_MODRM | INAT_VEXOK,
+	[0x1e] = INAT_MODRM | INAT_VEXOK,
+	[0x1f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MODRM | INAT_VEXOK,
+	[0x21] = INAT_MODRM | INAT_VEXOK,
+	[0x22] = INAT_MODRM | INAT_VEXOK,
+	[0x23] = INAT_MODRM | INAT_VEXOK,
+	[0x24] = INAT_MODRM | INAT_VEXOK,
+	[0x25] = INAT_MODRM | INAT_VEXOK,
+	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x28] = INAT_MODRM | INAT_VEXOK,
+	[0x29] = INAT_MODRM | INAT_VEXOK,
+	[0x2a] = INAT_MODRM | INAT_VEXOK,
+	[0x2b] = INAT_MODRM | INAT_VEXOK,
+	[0x2c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x2d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x30] = INAT_MODRM | INAT_VEXOK,
+	[0x31] = INAT_MODRM | INAT_VEXOK,
+	[0x32] = INAT_MODRM | INAT_VEXOK,
+	[0x33] = INAT_MODRM | INAT_VEXOK,
+	[0x34] = INAT_MODRM | INAT_VEXOK,
+	[0x35] = INAT_MODRM | INAT_VEXOK,
+	[0x36] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x37] = INAT_MODRM | INAT_VEXOK,
+	[0x38] = INAT_MODRM | INAT_VEXOK,
+	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3a] = INAT_MODRM | INAT_VEXOK,
+	[0x3b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3c] = INAT_MODRM | INAT_VEXOK,
+	[0x3d] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x3e] = INAT_MODRM | INAT_VEXOK,
+	[0x3f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x64] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x65] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x75] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x76] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x77] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x80] = INAT_MODRM,
+	[0x81] = INAT_MODRM,
+	[0x82] = INAT_MODRM,
+	[0x83] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x88] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x89] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x8d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x8e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x90] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x91] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
+	[0x92] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x93] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x96] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x97] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x98] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x99] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9b] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x9f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa0] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa3] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xa6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xa9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xaa] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xab] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xac] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xae] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xb5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xb6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xb9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xba] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbb] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbc] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbd] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbe] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xbf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xc4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xc8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xca] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcb] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcc] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xcd] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xdb] = INAT_MODRM | INAT_VEXOK,
+	[0xdc] = INAT_MODRM | INAT_VEXOK,
+	[0xdd] = INAT_MODRM | INAT_VEXOK,
+	[0xde] = INAT_MODRM | INAT_VEXOK,
+	[0xdf] = INAT_MODRM | INAT_VEXOK,
+	[0xf0] = INAT_MODRM,
+	[0xf1] = INAT_MODRM,
+	[0xf6] = INAT_MODRM,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+const insn_attr_t inat_escape_table_2_2[INAT_OPCODE_TABLE_SIZE] = {
+	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x21] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x22] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x23] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x24] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x25] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x30] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x31] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x32] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x33] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x34] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x35] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x38] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf6] = INAT_MODRM,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+const insn_attr_t inat_escape_table_2_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0xf0] = INAT_MODRM | INAT_MODRM,
+	[0xf1] = INAT_MODRM | INAT_MODRM,
+	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* Table: 3-byte opcode 2 (0x0f 0x3a) */
+const insn_attr_t inat_escape_table_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_VARIANT,
+	[0x01] = INAT_VARIANT,
+	[0x02] = INAT_VARIANT,
+	[0x03] = INAT_VARIANT,
+	[0x04] = INAT_VARIANT,
+	[0x05] = INAT_VARIANT,
+	[0x06] = INAT_VARIANT,
+	[0x08] = INAT_VARIANT,
+	[0x09] = INAT_VARIANT,
+	[0x0a] = INAT_VARIANT,
+	[0x0b] = INAT_VARIANT,
+	[0x0c] = INAT_VARIANT,
+	[0x0d] = INAT_VARIANT,
+	[0x0e] = INAT_VARIANT,
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x14] = INAT_VARIANT,
+	[0x15] = INAT_VARIANT,
+	[0x16] = INAT_VARIANT,
+	[0x17] = INAT_VARIANT,
+	[0x18] = INAT_VARIANT,
+	[0x19] = INAT_VARIANT,
+	[0x1a] = INAT_VARIANT,
+	[0x1b] = INAT_VARIANT,
+	[0x1d] = INAT_VARIANT,
+	[0x1e] = INAT_VARIANT,
+	[0x1f] = INAT_VARIANT,
+	[0x20] = INAT_VARIANT,
+	[0x21] = INAT_VARIANT,
+	[0x22] = INAT_VARIANT,
+	[0x23] = INAT_VARIANT,
+	[0x25] = INAT_VARIANT,
+	[0x26] = INAT_VARIANT,
+	[0x27] = INAT_VARIANT,
+	[0x30] = INAT_VARIANT,
+	[0x31] = INAT_VARIANT,
+	[0x32] = INAT_VARIANT,
+	[0x33] = INAT_VARIANT,
+	[0x38] = INAT_VARIANT,
+	[0x39] = INAT_VARIANT,
+	[0x3a] = INAT_VARIANT,
+	[0x3b] = INAT_VARIANT,
+	[0x3e] = INAT_VARIANT,
+	[0x3f] = INAT_VARIANT,
+	[0x40] = INAT_VARIANT,
+	[0x41] = INAT_VARIANT,
+	[0x42] = INAT_VARIANT,
+	[0x43] = INAT_VARIANT,
+	[0x44] = INAT_VARIANT,
+	[0x46] = INAT_VARIANT,
+	[0x4a] = INAT_VARIANT,
+	[0x4b] = INAT_VARIANT,
+	[0x4c] = INAT_VARIANT,
+	[0x50] = INAT_VARIANT,
+	[0x51] = INAT_VARIANT,
+	[0x54] = INAT_VARIANT,
+	[0x55] = INAT_VARIANT,
+	[0x56] = INAT_VARIANT,
+	[0x57] = INAT_VARIANT,
+	[0x60] = INAT_VARIANT,
+	[0x61] = INAT_VARIANT,
+	[0x62] = INAT_VARIANT,
+	[0x63] = INAT_VARIANT,
+	[0x66] = INAT_VARIANT,
+	[0x67] = INAT_VARIANT,
+	[0xcc] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0xdf] = INAT_VARIANT,
+	[0xf0] = INAT_VARIANT,
+};
+const insn_attr_t inat_escape_table_3_1[INAT_OPCODE_TABLE_SIZE] = {
+	[0x00] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x01] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x02] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x03] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x05] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x06] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x08] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x09] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x15] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x16] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x17] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x18] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x19] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x1a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x1e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x20] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x21] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x22] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x23] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x25] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x26] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x27] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x30] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x31] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x32] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x33] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x38] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x39] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x3a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x3f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x40] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x41] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x42] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x43] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x44] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x46] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x4c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x50] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x51] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x54] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x55] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x56] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x57] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x60] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x61] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x62] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x63] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x66] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x67] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0xdf] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+const insn_attr_t inat_escape_table_3_3[INAT_OPCODE_TABLE_SIZE] = {
+	[0xf0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* GrpTable: Grp1 */
+
+/* GrpTable: Grp1A */
+
+/* GrpTable: Grp2 */
+
+/* GrpTable: Grp3_1 */
+const insn_attr_t inat_group_table_6[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp3_2 */
+const insn_attr_t inat_group_table_7[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp4 */
+const insn_attr_t inat_group_table_8[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+};
+
+/* GrpTable: Grp5 */
+const insn_attr_t inat_group_table_9[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM | INAT_FORCE64,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM | INAT_FORCE64,
+	[0x5] = INAT_MODRM,
+	[0x6] = INAT_MODRM | INAT_FORCE64,
+};
+
+/* GrpTable: Grp6 */
+const insn_attr_t inat_group_table_10[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x5] = INAT_MODRM,
+};
+
+/* GrpTable: Grp7 */
+const insn_attr_t inat_group_table_11[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+	[0x4] = INAT_MODRM,
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp8 */
+
+/* GrpTable: Grp9 */
+const insn_attr_t inat_group_table_22[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM,
+	[0x6] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+	[0x7] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_22_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x6] = INAT_MODRM,
+};
+const insn_attr_t inat_group_table_22_2[INAT_GROUP_TABLE_SIZE] = {
+	[0x6] = INAT_MODRM,
+	[0x7] = INAT_MODRM,
+};
+
+/* GrpTable: Grp10 */
+
+/* GrpTable: Grp11A */
+const insn_attr_t inat_group_table_4[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
+};
+
+/* GrpTable: Grp11B */
+const insn_attr_t inat_group_table_5[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
+};
+
+/* GrpTable: Grp12 */
+const insn_attr_t inat_group_table_14[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_14_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp13 */
+const insn_attr_t inat_group_table_15[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_VARIANT,
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_15_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp14 */
+const insn_attr_t inat_group_table_16[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x3] = INAT_VARIANT,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
+	[0x7] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_16_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
+};
+
+/* GrpTable: Grp15 */
+const insn_attr_t inat_group_table_19[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_VARIANT,
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_19_2[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+};
+
+/* GrpTable: Grp16 */
+const insn_attr_t inat_group_table_13[INAT_GROUP_TABLE_SIZE] = {
+	[0x0] = INAT_MODRM,
+	[0x1] = INAT_MODRM,
+	[0x2] = INAT_MODRM,
+	[0x3] = INAT_MODRM,
+};
+
+/* GrpTable: Grp17 */
+const insn_attr_t inat_group_table_25[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
+};
+
+/* GrpTable: Grp18 */
+const insn_attr_t inat_group_table_23[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_VARIANT,
+	[0x5] = INAT_VARIANT,
+	[0x6] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_23_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+};
+
+/* GrpTable: Grp19 */
+const insn_attr_t inat_group_table_24[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_VARIANT,
+	[0x2] = INAT_VARIANT,
+	[0x5] = INAT_VARIANT,
+	[0x6] = INAT_VARIANT,
+};
+const insn_attr_t inat_group_table_24_1[INAT_GROUP_TABLE_SIZE] = {
+	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
+};
+
+/* GrpTable: GrpP */
+
+/* GrpTable: GrpPDLK */
+
+/* GrpTable: GrpRNG */
+
+/* Escape opcode map array */
+const insn_attr_t * const inat_escape_tables[INAT_ESC_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[1][0] = inat_escape_table_1,
+	[1][1] = inat_escape_table_1_1,
+	[1][2] = inat_escape_table_1_2,
+	[1][3] = inat_escape_table_1_3,
+	[2][0] = inat_escape_table_2,
+	[2][1] = inat_escape_table_2_1,
+	[2][2] = inat_escape_table_2_2,
+	[2][3] = inat_escape_table_2_3,
+	[3][0] = inat_escape_table_3,
+	[3][1] = inat_escape_table_3_1,
+	[3][3] = inat_escape_table_3_3,
+};
+
+/* Group opcode map array */
+const insn_attr_t * const inat_group_tables[INAT_GRP_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[4][0] = inat_group_table_4,
+	[5][0] = inat_group_table_5,
+	[6][0] = inat_group_table_6,
+	[7][0] = inat_group_table_7,
+	[8][0] = inat_group_table_8,
+	[9][0] = inat_group_table_9,
+	[10][0] = inat_group_table_10,
+	[11][0] = inat_group_table_11,
+	[13][0] = inat_group_table_13,
+	[14][0] = inat_group_table_14,
+	[14][1] = inat_group_table_14_1,
+	[15][0] = inat_group_table_15,
+	[15][1] = inat_group_table_15_1,
+	[16][0] = inat_group_table_16,
+	[16][1] = inat_group_table_16_1,
+	[19][0] = inat_group_table_19,
+	[19][2] = inat_group_table_19_2,
+	[22][0] = inat_group_table_22,
+	[22][1] = inat_group_table_22_1,
+	[22][2] = inat_group_table_22_2,
+	[23][0] = inat_group_table_23,
+	[23][1] = inat_group_table_23_1,
+	[24][0] = inat_group_table_24,
+	[24][1] = inat_group_table_24_1,
+	[25][0] = inat_group_table_25,
+};
+
+/* AVX opcode map array */
+const insn_attr_t * const inat_avx_tables[X86_VEX_M_MAX + 1][INAT_LSTPFX_MAX + 1] = {
+	[1][0] = inat_escape_table_1,
+	[1][1] = inat_escape_table_1_1,
+	[1][2] = inat_escape_table_1_2,
+	[1][3] = inat_escape_table_1_3,
+	[2][0] = inat_escape_table_2,
+	[2][1] = inat_escape_table_2_1,
+	[2][2] = inat_escape_table_2_2,
+	[2][3] = inat_escape_table_2_3,
+	[3][0] = inat_escape_table_3,
+	[3][1] = inat_escape_table_3_1,
+	[3][3] = inat_escape_table_3_3,
+};
Binary files linux-4.12.9/tools/objtool/fixdep and linux-4.12.9-pf/tools/objtool/fixdep differ
Binary files linux-4.12.9/tools/objtool/objtool and linux-4.12.9-pf/tools/objtool/objtool differ
diff -uNr linux-4.12.9/tools/vm/page-types.c linux-4.12.9-pf/tools/vm/page-types.c
--- linux-4.12.9/tools/vm/page-types.c	2017-08-25 09:15:18.000000000 +0900
+++ linux-4.12.9-pf/tools/vm/page-types.c	2019-01-02 21:25:05.887657968 +0900
@@ -99,6 +99,9 @@
 #define KPF_SWAP		62
 #define KPF_MMAP_EXCLUSIVE	63
 
+//PARALFETCH
+#define KPF_PREFETCH		64
+
 #define KPF_ALL_BITS		((uint64_t)~0ULL)
 #define KPF_HACKERS_BITS	(0xffffULL << 32)
 #define KPF_OVERLOADED_BITS	(0xffffULL << 48)
@@ -152,6 +155,8 @@
 	[KPF_FILE]		= "F:file",
 	[KPF_SWAP]		= "w:swap",
 	[KPF_MMAP_EXCLUSIVE]	= "1:mmap_exclusive",
+//PARALFETCH
+	[KPF_PREFETCH]		= "I:prefetch",
 };
 
 
